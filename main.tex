\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}

\usepackage{amsmath, amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}

\usepackage{url}
\usepackage{hyperref}
\hypersetup{colorlinks=true, citecolor=blue, linkcolor=blue, urlcolor=blue}

\usepackage[numbers,sort&compress]{natbib}
\usepackage{enumitem}
\setlist[itemize]{leftmargin=*}

\graphicspath{{figures/}}

\title{Marin: Fully Open LLM Training at 8B and 32B with Mid-Flight Adaptation}
\author{marin team}
\date{February 2026}

\begin{document}
\maketitle

\begin{abstract}
Fully open language-model releases enable reproduction and scientific study beyond what is possible from open weights alone.
We present a retrospective of \textsc{Marin}, a community-driven effort that releases not only checkpoints, but also training code, data mixtures, intermediate artifacts, and an issue-driven development history.
We document two base-model runs: Marin 8B, trained through multiple cooldown/reheat phases with data-mixture evolution and deep-cooldown stability interventions; and Marin 32B, which required a mid-run switch to QK-norm attention to eliminate loss spikes and a shuffling redesign (Feistel permutation) to avoid late-training phase-shift pathologies.
Beyond final benchmark numbers, we emphasize actionable lessons about stability tooling, data-mixture iteration (microannealing), shuffle correctness, and contamination hygiene.
\end{abstract}

\input{sections/intro}
\input{sections/related}
\input{sections/methods}
\input{sections/experiments}
\input{sections/conclusion}

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
