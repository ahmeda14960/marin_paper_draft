
<!doctype html>
<html lang="en" class="no-js">
  <head>
    

      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Documentation for the Marin project">
      
      
      
        <link rel="canonical" href="https://marin.community/reports/marin-32b-retro/">
      
      
        <link rel="prev" href="../marin-8b-retro/">
      
      
        <link rel="next" href="../markdownified-datasets/">
      
      
        
      
      
      <link rel="icon" href="../../static/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
<meta name="readthedocs-addons-api-version" content="1" />

    
      
        <title>Marin 32B Retro - Marin Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../css/mkdocstrings.css">
    
      <link rel="stylesheet" href="../../css/custom.css">
    
      <link rel="stylesheet" href="../../css/friendly.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  <script async type="text/javascript" src="/_/static/javascript/readthedocs-addons.js"></script><meta name="readthedocs-project-slug" content="marin" /><meta name="readthedocs-version-slug" content="latest" /><meta name="readthedocs-resolver-filename" content="/reports/marin-32b-retro/" /><meta name="readthedocs-http-status" content="200" /></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="white">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#marin-32b-retrospective" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Marin Documentation" class="md-header__button md-logo" aria-label="Marin Documentation" data-md-component="logo">
      
  <img src="../../static/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Marin Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Marin 32B Retro
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="white"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="white"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/marin-community/marin" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    marin-community/marin
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Marin Documentation" class="md-nav__button md-logo" aria-label="Marin Documentation" data-md-component="logo">
      
  <img src="../../static/logo.png" alt="logo">

    </a>
    Marin Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/marin-community/marin" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    marin-community/marin
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Tutorials
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Tutorials
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Installation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/first-experiment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    First Experiment
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/local-gpu/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Setting up a Local GPU Environment
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/storage-bucket/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Prepare a Storage Bucket
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/tpu-cluster-setup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Setting up a TPU Cluster
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/executor-101/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Executor 101
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/data-browser/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Data Browser
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/train-an-lm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Training an LM
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/submitting-speedrun/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Submitting a Speedrun
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tutorials/add-optimizer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding an Optimizer for Speedrun
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Explanations
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Explanations
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../explanations/lm-pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    The Language Modeling Pipeline
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../explanations/evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Evaluation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../explanations/experiments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Experiments
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../explanations/executor/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Executor
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../explanations/guidelines/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Guidelines
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Experiments
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Experiments
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Report Index
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../summary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Auto-Generated Summary
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../marin-8b-retro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Marin 8B Retro
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Marin 32B Retro
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Marin 32B Retro
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-phase-overview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Phase Overview
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#baseline-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Baseline Configuration
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Baseline Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hardware" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hardware
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        Architecture
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llama-3-style-32b-phase-12-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama-3-style 32B (Phase 1–2) parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen3-style-32b-phase-3-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen3-style 32B (Phase 3+) parameters
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizer-schedule" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optimizer &amp; Schedule
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mix-batch-schedule" class="md-nav__link">
    <span class="md-ellipsis">
      
        Data Mix &amp; Batch Schedule
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Data Mix &amp; Batch Schedule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batch-schedule" class="md-nav__link">
    <span class="md-ellipsis">
      
        Batch schedule
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-phases" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Phases
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Training Phases">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#phase-1-scaling-up-our-existing-recipe" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 1: Scaling up our existing recipe
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 1: Scaling up our existing recipe">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-tightened-max-grad-norm-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Tightened max grad norm clipping
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-clip-update-norm" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Clip Update Norm
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-skipping-bad-steps" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Skipping Bad Steps
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-2-recovery-without-architecture-changes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 2: Recovery Without Architecture Changes
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 2: Recovery Without Architecture Changes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#necromancy-restart-exp1390_32b_necro" class="md-nav__link">
    <span class="md-ellipsis">
      
        "Necromancy" Restart (exp1390_32b_necro)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alternative-optimizers-exp1388-exp1380" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alternative Optimizers (exp1388, exp1380)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-3-switch-to-qk-norm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 3: Switch to QK-Norm
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-4-midtraining-runs" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 4: Midtraining Runs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 4: Midtraining Runs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attempt-1-bison-cooldown" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attempt 1 — Bison Cooldown
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attempt 1 — Bison Cooldown">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cooldown-mixture-normalized-share" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cooldown mixture (normalized share):
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Parameters
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#outcome" class="md-nav__link">
    <span class="md-ellipsis">
      
        Outcome
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Outcome">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contamination-anomaly-gsm8k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contamination anomaly — GSM8k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shuffling-anomaly" class="md-nav__link">
    <span class="md-ellipsis">
      
        Shuffling anomaly
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attempt-2-mantis-cooldown" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attempt 2 — Mantis Cooldown
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attempt 2 — Mantis Cooldown">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#shuffling-linear-vs-feistel" class="md-nav__link">
    <span class="md-ellipsis">
      
        Shuffling: Linear vs. Feistel
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#base-model-results" class="md-nav__link">
    <span class="md-ellipsis">
      
        Base Model Results
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Base Model Results">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#an-aside-on-terminology" class="md-nav__link">
    <span class="md-ellipsis">
      
        An Aside on Terminology
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#caveats" class="md-nav__link">
    <span class="md-ellipsis">
      
        Caveats
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#limitations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Limitations
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lessons-learned" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lessons Learned
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#acknowledgments" class="md-nav__link">
    <span class="md-ellipsis">
      
        Acknowledgments
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#glossary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Glossary
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../markdownified-datasets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Markdownified Datasets
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Developer Guide
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Developer Guide
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dev-guide/contributing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Contributing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dev-guide/deployment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Deployment
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dev-guide/tutorial-guidelines/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tutorial Guidelines
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dev-guide/guidelines-internal/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Internal Guidelines
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dev-guide/building-docs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Building Docs
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dev-guide/snapshot-tests/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Snapshot Tests
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Technical Reference
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Technical Reference
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references/resource-config/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Specifying Hardware Resources
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references/executor-api/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Executor API
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references/default-steps/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Default Steps
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references/train-config/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Training Configuration
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-phase-overview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Phase Overview
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#baseline-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Baseline Configuration
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Baseline Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hardware" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hardware
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        Architecture
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llama-3-style-32b-phase-12-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Llama-3-style 32B (Phase 1–2) parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen3-style-32b-phase-3-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen3-style 32B (Phase 3+) parameters
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizer-schedule" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optimizer &amp; Schedule
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mix-batch-schedule" class="md-nav__link">
    <span class="md-ellipsis">
      
        Data Mix &amp; Batch Schedule
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Data Mix &amp; Batch Schedule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batch-schedule" class="md-nav__link">
    <span class="md-ellipsis">
      
        Batch schedule
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-phases" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Phases
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Training Phases">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#phase-1-scaling-up-our-existing-recipe" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 1: Scaling up our existing recipe
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 1: Scaling up our existing recipe">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-tightened-max-grad-norm-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Tightened max grad norm clipping
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-clip-update-norm" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Clip Update Norm
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-skipping-bad-steps" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Skipping Bad Steps
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-2-recovery-without-architecture-changes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 2: Recovery Without Architecture Changes
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 2: Recovery Without Architecture Changes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#necromancy-restart-exp1390_32b_necro" class="md-nav__link">
    <span class="md-ellipsis">
      
        "Necromancy" Restart (exp1390_32b_necro)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alternative-optimizers-exp1388-exp1380" class="md-nav__link">
    <span class="md-ellipsis">
      
        Alternative Optimizers (exp1388, exp1380)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-3-switch-to-qk-norm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 3: Switch to QK-Norm
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-4-midtraining-runs" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 4: Midtraining Runs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 4: Midtraining Runs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attempt-1-bison-cooldown" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attempt 1 — Bison Cooldown
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attempt 1 — Bison Cooldown">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cooldown-mixture-normalized-share" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cooldown mixture (normalized share):
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Parameters
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#outcome" class="md-nav__link">
    <span class="md-ellipsis">
      
        Outcome
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Outcome">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contamination-anomaly-gsm8k" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contamination anomaly — GSM8k
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shuffling-anomaly" class="md-nav__link">
    <span class="md-ellipsis">
      
        Shuffling anomaly
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attempt-2-mantis-cooldown" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attempt 2 — Mantis Cooldown
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attempt 2 — Mantis Cooldown">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#shuffling-linear-vs-feistel" class="md-nav__link">
    <span class="md-ellipsis">
      
        Shuffling: Linear vs. Feistel
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#base-model-results" class="md-nav__link">
    <span class="md-ellipsis">
      
        Base Model Results
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Base Model Results">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#an-aside-on-terminology" class="md-nav__link">
    <span class="md-ellipsis">
      
        An Aside on Terminology
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#caveats" class="md-nav__link">
    <span class="md-ellipsis">
      
        Caveats
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#limitations" class="md-nav__link">
    <span class="md-ellipsis">
      
        Limitations
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lessons-learned" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lessons Learned
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#acknowledgments" class="md-nav__link">
    <span class="md-ellipsis">
      
        Acknowledgments
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#glossary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Glossary
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="marin-32b-retrospective">Marin 32B Retrospective<a class="headerlink" href="#marin-32b-retrospective" title="Permanent link">&para;</a></h1>
<!--

- [Introduction](#introduction)
- [Baseline Configuration](#baseline-configuration)
  - [Architecture](#architecture)
  - [Optimizer & Schedule](#optimizer--schedule)
  - [Data Mix & Batch Schedule](#data-mix--batch-schedule)
- [Training Phases](#training-phases)
  - [Phase 1: Loss Spikes in `exp1295_32b`](#phase-1-loss-spikes-in-exp1295_32b)
  - [Phase 2: Recovery Without Architecture Changes](#phase-2-recovery-without-architecture-changes)
    - ["Necromancy" Restart (`exp1390_32b_necro`)](#necromancy-restart-exp1390_32b_necro)
    - [Alternative Optimizers (`exp1388`, `exp1380`)](#alternative-optimizers-exp1388-exp1380)
  - [Phase 3: Switch to QK-Norm (`exp1395_qwen3_32b`)](#phase-3-switch-to-qk-norm-exp1395_qwen3_32b)
  - [Phase 4: Midtraining Runs](#phase-4-midtraining-runs)
    - [Attempt 1 — Bison Cooldown (`exp1529_32b_bison_cooldown`)](#attempt-1--bison-cooldown-exp1529_32b_bison_cooldown)
    - [Attempt 2 — Mantis Cooldown (`exp1529_32b_mantis_cooldown`)](#attempt-2--mantis-cooldown-exp1529_32b_mantis_cooldown)
- [Lessons Learned](#lessons-learned)
- [Glossary](#glossary) -->

<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h2>
<p>This is a retrospective on Marin 32B, which is largely a scale-up of the <a href="../marin-8b-retro/">8B recipe</a>. As with the 8B,
we followed the "Tootsie Roll" playbook: start training, instrument heavily, and make evidence-driven changes mid-flight.
The intent here is to document what worked, what failed, and the mechanics of why and how we made changes so that others can learn from our process beyond the final result.
You can download the base checkpoint on Hugging Face at <a href="https://huggingface.co/marin-community/marin-32b-base">marin-community/marin-32b-base</a>.</p>
<p>We deliberately reused the <a href="https://arxiv.org/abs/2412.02595">Nemotron-CC</a>-centric pretraining mixture and the
AdamW-based schedule that behaved well at 8B. That mostly transferred.
The notable exception was a loss-instability episode around 70k–80k steps in <code>exp1295_32b</code> that we ultimately resolved by introducing QK-Norm via a switch to the Qwen3 32B backbone.
Later, during cooldown, we uncovered GSM8k contamination (from a cached Dolmino bundle) and shuffling pathologies from a linear congruential-driven permutation. Both issues were addressed in the "Mantis" cooldown with a Feistel-based shuffle and a cleaner math mix.</p>
<p>(If you’re not already reading this on ReadTheDocs, we recommend <a href="https://marin.readthedocs.io/en/latest/reports/marin-32b-retro/">viewing it there</a> for the right-hand ToC and better navigation.)</p>
<h2 id="training-phase-overview">Training Phase Overview<a class="headerlink" href="#training-phase-overview" title="Permanent link">&para;</a></h2>
<p>We break the run into four phases:</p>
<table>
<thead>
<tr>
<th>Phase</th>
<th>Steps</th>
<th>Tokens (T)</th>
<th>Key Changes</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0 → 80,000</td>
<td>2.679</td>
<td>Baseline Llama 32B</td>
<td>Very spiky loss; attempted mitigations failed</td>
</tr>
<tr>
<td>2</td>
<td>80,000 → ≈82,000†</td>
<td>≈0.02†</td>
<td>Recovery attempts (necromancy, alt optimizers)</td>
<td>Diagnostic restarts; suspected structural instability</td>
</tr>
<tr>
<td>3</td>
<td>80,000 → 160,000</td>
<td>2.684</td>
<td>Switch to Qwen3 32B (QK-Norm)</td>
<td>Loss stabilized; training recovered</td>
</tr>
<tr>
<td>4</td>
<td>160,000 → 192,000</td>
<td>1.074</td>
<td>Mantis cooldown (Feistel shuffle, better math mix)</td>
<td>Strong results; resolved anomalies from Bison</td>
</tr>
</tbody>
</table>
<p>† Approximate; Phase 2 comprised short diagnostic bursts that were later discarded, so they are excluded from cumulative token totals.</p>
<p>This report covers the following experiments/phases:</p>
<ul>
<li>Phase 1 - Baseline: <a href="https://github.com/marin-community/marin/issues/1295">#1295</a> <a href="https://github.com/marin-community/marin/blob/5e88b5253975ffd13e63a5db0b946883c8660e1b/experiments/tootsie/exp1295_32b.py"><code>exp1295_32b</code></a> <a href="https://marin.community/data-browser/experiment/?path=gs%3A//marin-us-central2/experiments/exp859_big_tootsies-e9092f.json">Data Browser Link</a></li>
<li>Phase 2a - Necromancy Restart: <a href="https://github.com/marin-community/marin/issues/1390">#1390</a> <a href="https://github.com/marin-community/marin/blob/fe373c233ee7288cbf8e7600765c3fc6fb6fa3ac/experiments/tootsie/exp1390_32b_necro.py"><code>exp1390_32b_necro</code></a> <a href="https://marin.community/data-browser/experiment/?path=gs%3A//marin-us-central2/experiments/exp1380_32b_necro-51ba55.json">Data Browser Link</a></li>
<li>Phase 2b - Optimizer Swap (Muon): <a href="https://github.com/marin-community/marin/issues/1380">#1380</a> <a href="https://github.com/marin-community/marin/blob/fe373c233ee7288cbf8e7600765c3fc6fb6fa3ac/experiments/tootsie/exp1380_muon32b.py"><code>exp1380_muon32b</code></a> <a href="https://marin.community/data-browser/experiment/?path=gs%3A//marin-us-central2/experiments/exp1380_muon32b-898f42.json">Data Browser Link</a></li>
<li>Phase 3 - QK-Norm Switch: <a href="https://github.com/marin-community/marin/issues/1395">#1395</a> <a href="https://github.com/marin-community/marin/blob/fe373c233ee7288cbf8e7600765c3fc6fb6fa3ac/experiments/tootsie/exp1395_qwen3_32b.py"><code>exp1395_qwen3_32b</code></a> <a href="https://marin.community/data-browser/experiment/?path=gs%3A//marin-us-central2/experiments/exp1395_qwen3_32b-de6f47.json">Data Browser Link</a></li>
<li>Phase 4a - Bison Cooldown: <a href="https://github.com/marin-community/marin/issues/1529">#1529</a> <a href="https://github.com/marin-community/marin/blob/main/experiments/tootsie/exp1529_32b_bison_cooldown.py"><code>exp1529_32b_bison_cooldown</code></a> <a href="https://marin.community/data-browser/experiment/?path=gs%3A//marin-us-central2/experiments/exp1529_32b_bison_cooldown-48ddfe.json">Data Browser Link</a></li>
<li>Phase 4b - Mantis Cooldown: <a href="https://github.com/marin-community/marin/issues/1681">#1581</a> <a href="https://github.com/marin-community/marin/blob/main/experiments/tootsie/exp1529_32b_mantis_cooldown.py"><code>exp1529_32b_mantis_cooldown</code></a> <a href="https://marin.community/data-browser/experiment/?path=gs%3A//marin-us-central2/experiments/exp1529_32b_mantis_cooldown-c6f4b0.json">Data Browser Link</a></li>
</ul>
<p>Total tokens trained in final artifact: ≈6.437T</p>
<ul>
<li>Phase 1: 2.679T</li>
<li>Phase 3/QK‑Norm: 2.684T</li>
<li>Phase 4b/Mantis cooldown: 1.074T; excludes diagnostic restarts and the abandoned Bison cooldown attempt.</li>
</ul>
<h2 id="baseline-configuration">Baseline Configuration<a class="headerlink" href="#baseline-configuration" title="Permanent link">&para;</a></h2>
<h3 id="hardware">Hardware<a class="headerlink" href="#hardware" title="Permanent link">&para;</a></h3>
<p>We initially started the 32B run on 4 preemptible TPU v5p-512 slices coordinated with multislice, moving to 3 slices
at 18,500 steps as spare capacity dried up, and eventually moving to a reserved v4-2048 once the preemptible v5p were no longer
reliably available (21,010 steps). It remained on the v4-2048 slice for the rest of the run.</p>
<h3 id="architecture">Architecture<a class="headerlink" href="#architecture" title="Permanent link">&para;</a></h3>
<ul>
<li>Initial backbone: Llama 32B (Llama‑3–style settings)</li>
<li>Gradient checkpointing: offload carries to fit large global batches on a v4‑2048 slice</li>
</ul>
<h4 id="llama-3-style-32b-phase-12-parameters">Llama-3-style 32B (Phase 1–2) parameters<a class="headerlink" href="#llama-3-style-32b-phase-12-parameters" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th><strong>Parameter</strong></th>
<th><strong>Value</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>seq_len</code></td>
<td>4096</td>
</tr>
<tr>
<td><code>hidden_dim</code></td>
<td>5120</td>
</tr>
<tr>
<td><code>intermediate_dim</code></td>
<td>27648</td>
</tr>
<tr>
<td><code>num_heads</code></td>
<td>40</td>
</tr>
<tr>
<td><code>num_kv_heads</code></td>
<td>8</td>
</tr>
<tr>
<td><code>num_layers</code></td>
<td>64</td>
</tr>
<tr>
<td><code>activation_function</code></td>
<td><code>silu</code></td>
</tr>
</tbody>
</table>
<p>These dimensions were selected to be the same as those used in OLMo 2 32B.</p>
<h4 id="qwen3-style-32b-phase-3-parameters">Qwen3-style 32B (Phase 3+) parameters<a class="headerlink" href="#qwen3-style-32b-phase-3-parameters" title="Permanent link">&para;</a></h4>
<p>(These are the same except for the addition of QK-Norm in attention.)</p>
<table>
<thead>
<tr>
<th><strong>Parameter</strong></th>
<th><strong>Value</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>seq_len</code></td>
<td>4096</td>
</tr>
<tr>
<td><code>hidden_dim</code></td>
<td>5120</td>
</tr>
<tr>
<td><code>intermediate_dim</code></td>
<td>27648</td>
</tr>
<tr>
<td><code>num_heads</code></td>
<td>40</td>
</tr>
<tr>
<td><code>num_kv_heads</code></td>
<td>8</td>
</tr>
<tr>
<td><code>num_layers</code></td>
<td>64</td>
</tr>
<tr>
<td><code>attention</code></td>
<td>QK‑Norm</td>
</tr>
<tr>
<td><code>activation_function</code></td>
<td><code>silu</code></td>
</tr>
</tbody>
</table>
<h3 id="optimizer-schedule">Optimizer &amp; Schedule<a class="headerlink" href="#optimizer-schedule" title="Permanent link">&para;</a></h3>
<p>Baseline (exp1295_32b) hyperparameters:</p>
<table>
<thead>
<tr>
<th><strong>Hyperparameter</strong></th>
<th><strong>Value</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Optimizer</td>
<td>AdamW</td>
</tr>
<tr>
<td>Peak LR</td>
<td>7e‑4</td>
</tr>
<tr>
<td>LR schedule</td>
<td>linear warmup → hold → decay (WSD‑style)</td>
</tr>
<tr>
<td>Warmup</td>
<td>1% of steps</td>
</tr>
<tr>
<td>Decay</td>
<td>40% of steps</td>
</tr>
<tr>
<td>Weight decay</td>
<td>0.05</td>
</tr>
<tr>
<td>Max grad norm</td>
<td>initially 1.0, then 0.2 from 56.4K steps</td>
</tr>
<tr>
<td>Clip update norm</td>
<td>on (σ=2.0, rolling=128); added at 72233, briefly off ~74k–80k</td>
</tr>
<tr>
<td>EMA beta</td>
<td>0.995</td>
</tr>
<tr>
<td>z-loss</td>
<td>1e-4</td>
</tr>
<tr>
<td>Skip bad steps</td>
<td>true (σ=2.0, rolling=128)</td>
</tr>
</tbody>
</table>
<p>Some of these were activated mid-run due to loss spikes. These include the tightened max grad norm,
the "clip update norm" and the "skip bad steps" flag. We discuss these below when we discuss the loss spikes.</p>
<p>Overall this is the same optimizer and schedule as used in the 8B run, with a scaled LR peak.</p>
<h3 id="data-mix-batch-schedule">Data Mix &amp; Batch Schedule<a class="headerlink" href="#data-mix-batch-schedule" title="Permanent link">&para;</a></h3>
<p>For pretraining, we followed the same recipe as our <a href="../marin-8b-retro/#phase-4-phoenix-reheated">8B Phoenix phase</a>:
<a href="https://arxiv.org/abs/2412.02595">Nemotron-CC</a>, <a href="https://huggingface.co/datasets/bigcode/starcoderdata">StarCoder Data</a>,
and <a href="https://huggingface.co/datasets/EleutherAI/proof-pile-2">Proofpile 2</a>.</p>
<p>Pretraining mixture (normalized share):</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td>nemotron_cc/medium</td>
<td>30.69%</td>
</tr>
<tr>
<td>nemotron_cc/hq_synth</td>
<td>24.70%</td>
</tr>
<tr>
<td>nemotron_cc/medium_low</td>
<td>13.98%</td>
</tr>
<tr>
<td>nemotron_cc/hq_actual</td>
<td>8.30%</td>
</tr>
<tr>
<td>nemotron_cc/medium_high</td>
<td>7.49%</td>
</tr>
<tr>
<td>nemotron_cc/low_actual</td>
<td>6.37%</td>
</tr>
<tr>
<td>nemotron_cc/low_synth</td>
<td>5.70%</td>
</tr>
<tr>
<td>starcoderdata</td>
<td>2.27%</td>
</tr>
<tr>
<td>proofpile_2</td>
<td>0.50%</td>
</tr>
</tbody>
</table>
<h4 id="batch-schedule">Batch schedule<a class="headerlink" href="#batch-schedule" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>Start step</th>
<th>Global batch size</th>
<th>Tokens per batch</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>8192</td>
<td>32Mi</td>
</tr>
<tr>
<td>18,500</td>
<td>7680</td>
<td>30Mi</td>
</tr>
<tr>
<td>21,010</td>
<td>8192</td>
<td>32Mi</td>
</tr>
</tbody>
</table>
<p>The change in global batch size was driven by changing hardware: when we had the 4 v5p-512s we used 8192,
shifting down to 7680 for divisibility by 3 when we had 3 v5p-512s, and finally changing back to 8192
when we moved to the v4 slice.</p>
<p>As with the 8b run, we used a sequence length of 4096 tokens.</p>
<h2 id="training-phases">Training Phases<a class="headerlink" href="#training-phases" title="Permanent link">&para;</a></h2>
<h3 id="phase-1-scaling-up-our-existing-recipe">Phase 1: Scaling up our existing recipe<a class="headerlink" href="#phase-1-scaling-up-our-existing-recipe" title="Permanent link">&para;</a></h3>
<ul>
<li>Issue: <a href="https://github.com/marin-community/marin/issues/1295">#1295</a></li>
<li>Experiment: <a href="https://github.com/marin-community/marin/blob/5e88b5253975ffd13e63a5db0b946883c8660e1b/experiments/tootsie/exp1295_32b.py"><code>exp1295_32b</code></a></li>
<li><a href="https://marin.community/data-browser/experiment/?path=gs%3A//marin-us-central2/experiments/exp859_big_tootsies-e9092f.json">Data Browser Link</a></li>
</ul>
<p>For ~70k steps training behaved as expected though we had many more loss spikes than we had seen at 8b (and in 70b trials).
We got a lot of feedback from the community on these spikes, with some folks telling us we were doomed,
others a bit more hedgey, and some folks telling us they looked acceptable. (Yay for open development!)</p>
<p><img alt="Training loss curve with frequent spikes during the Phase 1 baseline run" src="../32b-spiky-loss.png" /></p>
<p>Ideally, there wouldn't be spikes of course. But many of the people we talked to (and our own experience) suggested that if the model recovered quickly and it didn't really change the trajectory, it was fine.</p>
<p><img alt="Evaluation loss comparison between Marin 32B and peer runs during Phase 1" src="../32b-loss-comparisons.png" /></p>
<p>Nevertheless, we were a bit worried, so we added a bunch of instrumentation and a few interventions.</p>
<p>Specifically, we looked at the following mitigations (documented in <a href="https://github.com/marin-community/marin/issues/1368">#1368</a>)</p>
<h4 id="1-tightened-max-grad-norm-clipping">1. Tightened max grad norm clipping<a class="headerlink" href="#1-tightened-max-grad-norm-clipping" title="Permanent link">&para;</a></h4>
<p>Following standard practice, by default we have clipped our grad norm to a maximum of 1.0.
We observed that loss spikes typically occurred after steps with a large grad norm, but that typically our grad norms were in the range of about 0.2, and norms significantly higher than that often foretold a loss spike.
Therefore, at around 56.4K steps, we tightened the max grad norm to 0.2 to prevent large gradient steps from destabilizing training.</p>
<h4 id="2-clip-update-norm">2. Clip Update Norm<a class="headerlink" href="#2-clip-update-norm" title="Permanent link">&para;</a></h4>
<p>When that didn't work, we added a "clip update norm" filter at around 72.2K steps. Like grad norm spikes, update norm spikes always preceded grad norm spikes, and we thought these would be a more direct way of preventing the model from taking too large of a step. This filter computes a rolling average and standard deviation of the optimizer update norms (i.e. after Adam's scaling is applied), and clips updates that exceed the average by a certain threshold (2 stddevs in our case). We hoped that this would reduce the impact of outlier updates that could lead to loss spikes. Ultimately, it also didn't seem to prevent loss spikes, but it may have reduced their severity. We inadvertently turned this off for a few thousand steps around 74K–80K, which may have contributed to the more significant instability we saw then.</p>
<p><img alt="Update-norm spikes preceding loss spikes in Phase 1 diagnostics" src="../32b-update-spike-precede-loss-spike.png" /></p>
<h4 id="3-skipping-bad-steps">3. Skipping Bad Steps<a class="headerlink" href="#3-skipping-bad-steps" title="Permanent link">&para;</a></h4>
<p>Based on a tip from Luca Soldaini, we <a href="https://github.com/stanford-crfm/levanter/blob/9fde0781a1737e088535c392cf239aba5e1143e2/src/levanter/optim/skipstep.py#L65">added a "skip bad steps" flag</a> (modeled on <a href="https://github.com/allenai/OLMo-core/blob/main/src/olmo_core/optim/skip_step_optimizer.py">the version in OLMo</a>) that skips parameter updates when the update norm exceeds a rolling average by a threshold (2 stddevs in our case).</p>
<blockquote>
<p>Tokens trained: ≈2.679T tokens (80,000 steps; 4096 seq len; batch schedule 0–18,499: 8192, 18,500–21,009: 7680, 21,010–79,999: 8192).</p>
</blockquote>
<p><strong>What we learned:</strong> Gradient clipping and step-skipping heuristics softened the spikes but never removed them; instrumentation alone could not replace an architectural fix at this scale.</p>
<h3 id="phase-2-recovery-without-architecture-changes">Phase 2: Recovery Without Architecture Changes<a class="headerlink" href="#phase-2-recovery-without-architecture-changes" title="Permanent link">&para;</a></h3>
<ul>
<li>Issues: <a href="https://github.com/marin-community/marin/issues/1390">#1390</a>, <a href="https://github.com/marin-community/marin/issues/1380">#1380</a></li>
<li>Experiments: <a href="https://github.com/marin-community/marin/blob/fe373c233ee7288cbf8e7600765c3fc6fb6fa3ac/experiments/tootsie/exp1390_32b_necro.py"><code>exp1390_32b_necro</code></a>, <a href="https://github.com/marin-community/marin/blob/fe373c233ee7288cbf8e7600765c3fc6fb6fa3ac/experiments/tootsie/exp1380_muon32b.py"><code>exp1380_muon32b</code></a></li>
<li>Data browser: <a href="https://marin.community/data-browser/experiment/?path=gs%3A//marin-us-central2/experiments/exp1380_32b_necro-51ba55.json">necro</a>, <a href="https://marin.community/data-browser/experiment/?path=gs%3A//marin-us-central2/experiments/exp1380_muon32b-898f42.json">muon</a></li>
</ul>
<p>Unfortunately, at 80k steps we began to see spikes that were unavoidable even with all the update clipping that we had added. This launched a bit of a fire-drill to see how and whether we could salvage this run!</p>
<p><img alt="Phase 2 restart still exhibiting a severe loss spike despite mitigation attempts" src="../32b-bad-spike.png" /></p>
<p>We treated the 80k checkpoint as salvageable and attempted to coax the run back while keeping the Llama backbone.</p>
<h4 id="necromancy-restart-exp1390_32b_necro">"Necromancy" Restart (<code>exp1390_32b_necro</code>)<a class="headerlink" href="#necromancy-restart-exp1390_32b_necro" title="Permanent link">&para;</a></h4>
<p>Github Issue: <a href="https://github.com/marin-community/marin/issues/1390">#1390</a></p>
<p>We rebuilt optimizer state offline, seeding the warm‑start with update‑history tensors from the last good checkpoint so the clip‑update‑norm filter would have valid rolling statistics. This stabilized gradients for a few thousand steps, but the run relapsed into spikes—a sign the causes of spikes was more core to the model state.</p>
<h4 id="alternative-optimizers-exp1388-exp1380">Alternative Optimizers (<code>exp1388</code>, <code>exp1380</code>)<a class="headerlink" href="#alternative-optimizers-exp1388-exp1380" title="Permanent link">&para;</a></h4>
<p>We next swapped optimizers without touching weights! There's been lots of recent work on better conditioned optimizers, including our own empirical validations in our <a href="https://arxiv.org/abs/2509.02046">"Fantastic Optimizers"</a> paper.
We specifically tried <a href="https://kellerjordan.github.io/posts/muon/">Muon</a>, which is and was all the rage at the time.</p>
<p>In <a href="https://github.com/marin-community/marin/issues/1380"><code>exp1380_muon32b</code></a>, we used Muon with a higher effective LR = 2e‑3, but we retained the Adam‑style LR schedule to avoid a full retune.</p>
<p><img alt="Muon optimizer versus baseline Adam around 80k steps, showing temporary loss relief" src="../32b-muon-vs-adam.png" /></p>
<p>Aside: The Muon run was still warming up its Adam params here so the loss was lower. The Muon run technically spiked a little later, which might be worth investigating.</p>
<p>We actually let it keep going a little longer, but it decided to turn into gradient ascents after a while. So we abandoned it.
We probably need more time to properly tune Muon for this setting.</p>
<p><img alt="Muon optimizer run eventually diverging with repeated spikes and loss blow-up" src="../32b-muon-can-into-space.png" /></p>
<p><strong>Tokens trained:</strong> Diagnostic restarts only (short runs of a few thousand steps); excluded from cumulative phase totals due to restart to 80k Llama checkpoint for Phase 3.</p>
<p><strong>What we learned:</strong> Restart tricks and optimizer swaps stabilized gradients briefly but not sustainably, reinforcing that the instability was rooted in the model’s attention stack rather than in optimizer state.</p>
<h3 id="phase-3-switch-to-qk-norm">Phase 3: Switch to QK-Norm<a class="headerlink" href="#phase-3-switch-to-qk-norm" title="Permanent link">&para;</a></h3>
<ul>
<li>Issue: <a href="https://github.com/marin-community/marin/issues/1395">#1395</a></li>
<li>Experiment: <a href="https://github.com/marin-community/marin/blob/fe373c233ee7288cbf8e7600765c3fc6fb6fa3ac/experiments/tootsie/exp1395_qwen3_32b.py"><code>exp1395_qwen3_32b</code></a></li>
<li><a href="https://marin.community/data-browser/experiment/?path=gs%3A//marin-us-central2/experiments/exp1395_qwen3_32b-de6f47.json">Data Browser Link</a></li>
</ul>
<p>Github Issue: <a href="https://github.com/marin-community/marin/issues/1395">#1395</a></p>
<p>At this point we concluded that stabilizing a 32B Llama without architectural help wasn’t feasible under our constraints. We switched to Qwen3 32B, which adds QK‑Norm in attention, and warm‑started from the 80k Llama weights. This preserved useful signal in embeddings and MLPs while letting the normalized attention heads relearn.</p>
<ul>
<li>Prior reports—from <a href="https://arxiv.org/abs/2501.00656">the OLMo team</a> and <a href="https://arxiv.org/abs/2309.14322">Google DeepMind</a> among others—suggest QK‑Norm provides substantial headroom against loss spikes in large models.
You might reasonably ask "Why not use QK-norm to begin with?" While we thought about it, we had some hubris from our 8B experience (stable without QK‑Norm) as well as an earlier trial 70B run (also stable without QK-norm).
It is worth noting that the Llama 3 team seems to have trained much larger models without QK-Norm, so it seems possible it is not necessary given the right tuning of underlying hyperparameters, which we continue to explore.</li>
</ul>
<p>The switch to QK-norm imposed a one‑time loss penalty, but the training loss recovered in about 10B tokens. What's more, the loss spikes disappeared entirely.</p>
<p><img alt="QK-Norm warm-start loss recovers to the pre-switch trajectory within roughly 10B tokens" src="../qk-recovery.png" /></p>
<p>Warm‑start + rewarm parameters (<code>exp1395_32b</code>):</p>
<table>
<thead>
<tr>
<th><strong>Setting</strong></th>
<th><strong>Value</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Warm start checkpoint</td>
<td>step‑80,000 (Llama 32B)</td>
</tr>
<tr>
<td>Cycles</td>
<td>[80k, 1,000,000,000]</td>
</tr>
<tr>
<td>Re‑warmup</td>
<td>1,000 steps</td>
</tr>
</tbody>
</table>
<p><strong>Tokens trained:</strong> ≈2.684T tokens (80,000 steps from 80k → 160k at 4096 seq len, global batch 8192).</p>
<p><strong>What we learned:</strong> QK-Norm delivered the headroom we needed. Warm-starting preserved earlier progress while eliminating loss spikes after a short re-warmup.</p>
<h3 id="phase-4-midtraining-runs">Phase 4: Midtraining Runs<a class="headerlink" href="#phase-4-midtraining-runs" title="Permanent link">&para;</a></h3>
<ul>
<li>Issues: <a href="https://github.com/marin-community/marin/issues/1529">#1529</a>, <a href="https://github.com/marin-community/marin/issues/1681">#1681</a></li>
<li>Experiments: <a href="https://github.com/marin-community/marin/blob/main/experiments/tootsie/exp1529_32b_bison_cooldown.py"><code>exp1529_32b_bison_cooldown</code></a>, <a href="https://github.com/marin-community/marin/blob/main/experiments/tootsie/exp1529_32b_mantis_cooldown.py"><code>exp1529_32b_mantis_cooldown</code></a></li>
<li>Data browser: <a href="https://marin.community/data-browser/experiment/?path=gs%3A//marin-us-central2/experiments/exp1529_32b_bison_cooldown-48ddfe.json">bison</a>, <a href="https://marin.community/data-browser/experiment/?path=gs%3A//marin-us-central2/experiments/exp1529_32b_mantis_cooldown-c6f4b0.json">mantis</a></li>
</ul>
<p>With stability restored, we trained until we completed 1 epoch over the Nemotron data and then resumed the 8B playbook.
The first attempt (<em>Bison</em>) mirrored our <a href="../marin-8b-retro/#phase-5-starling-second-cooldown">Starling</a>-style cooldown and exposed two issues; the second (<em>Mantis</em>) fixed them.</p>
<h4 id="attempt-1-bison-cooldown">Attempt 1 — Bison Cooldown<a class="headerlink" href="#attempt-1-bison-cooldown" title="Permanent link">&para;</a></h4>
<ul>
<li>Issue: <a href="https://github.com/marin-community/marin/issues/1529">#1529</a></li>
<li>Experiment: <a href="https://github.com/marin-community/marin/blob/main/experiments/tootsie/exp1529_32b_bison_cooldown.py"><code>exp1529_32b_bison_cooldown</code></a></li>
<li><a href="https://marin.community/data-browser/experiment/?path=gs%3A//marin-us-central2/experiments/exp1529_32b_bison_cooldown-48ddfe.json">Data Browser Link</a></li>
</ul>
<p>Based on the success of our Starling cooldown at 8B, we attempted a similar cooldown at 32B.
Starting from the 160k qk-norm-enabled checkpoint, we ran a 32k-step cooldown with a 70/30 Nemotron/Starling mixture.</p>
<h5 id="cooldown-mixture-normalized-share">Cooldown mixture (normalized share):<a class="headerlink" href="#cooldown-mixture-normalized-share" title="Permanent link">&para;</a></h5>
<p>In detail, the cooldown used the same Nemotron-CC mixture as before, supplemented with a mix drawn from Dolmino, Finemath-3-Plus, Arxiv Markdownified, StackExchange Custom, and others.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td>nemotron_cc/medium</td>
<td>21.49%</td>
</tr>
<tr>
<td>nemotron_cc/hq_synth</td>
<td>17.29%</td>
</tr>
<tr>
<td>nemotron_cc/medium_low</td>
<td>9.79%</td>
</tr>
<tr>
<td>nemotron_cc/hq_actual</td>
<td>5.81%</td>
</tr>
<tr>
<td>nemotron_cc/medium_high</td>
<td>5.24%</td>
</tr>
<tr>
<td>nemotron_cc/low_actual</td>
<td>4.46%</td>
</tr>
<tr>
<td>nemotron_cc/low_synth</td>
<td>3.99%</td>
</tr>
<tr>
<td>arxiv_markdownified</td>
<td>7.41%</td>
</tr>
<tr>
<td>dolmino/pes2o</td>
<td>7.41%</td>
</tr>
<tr>
<td>finemath-3-plus</td>
<td>4.33%</td>
</tr>
<tr>
<td>dolmino/flan</td>
<td>4.33%</td>
</tr>
<tr>
<td>stackexchange_custom</td>
<td>2.18%</td>
</tr>
<tr>
<td>dolmino/stackexchange</td>
<td>2.18%</td>
</tr>
<tr>
<td>starcoderdata</td>
<td>1.59%</td>
</tr>
<tr>
<td>all_math</td>
<td>1.08%</td>
</tr>
<tr>
<td>proofpile_2</td>
<td>0.35%</td>
</tr>
<tr>
<td>wikipedia_markdown</td>
<td>0.47%</td>
</tr>
<tr>
<td>dolmino/wiki</td>
<td>0.47%</td>
</tr>
<tr>
<td>medu_science_qa</td>
<td>0.15%</td>
</tr>
</tbody>
</table>
<p>Within the 70/30 split, each data source was weighted approximately equal to its token size, but we upsampled FLAN (10x)
and the math from Dolmino (2x). Again, this is the same as the starling cooldown at 8B.</p>
<h5 id="training-parameters">Training Parameters<a class="headerlink" href="#training-parameters" title="Permanent link">&para;</a></h5>
<p>We used the following optimizer schedule:</p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Warmup</td>
<td>0</td>
</tr>
<tr>
<td>Decay window</td>
<td>160k → 192k (≈16.7% of total)</td>
</tr>
<tr>
<td>LR schedule</td>
<td>linear</td>
</tr>
<tr>
<td>AdamC</td>
<td>enabled during decay</td>
</tr>
</tbody>
</table>
<p><strong>Cooldown specifics (changes vs 8B):</strong></p>
<ol>
<li>Z‑loss throughout. We previously observed late‑decay divergences at low LR in 8B; adding a small z‑loss stopped <code>lm_head</code> norm growth during cooldown.</li>
<li><a href="https://arxiv.org/abs/2506.02285">AdamC</a> during decay. We opted to include this because in the 8b we observed the same gradient growth that it purpoted to fix.
We used the adjusted weight‑decay formulation only in decay phases to reduce gradient growth without retuning the entire schedule.</li>
</ol>
<h4 id="outcome">Outcome<a class="headerlink" href="#outcome" title="Permanent link">&para;</a></h4>
<h5 id="contamination-anomaly-gsm8k">Contamination anomaly — GSM8k<a class="headerlink" href="#contamination-anomaly-gsm8k" title="Permanent link">&para;</a></h5>
<p>The run was broadly strong versus OLMo 2 32B, with one extreme exception: GSM8k.
Under the standard LM Eval Harness prompt, the model was ~22 points worse than the weakest baseline.
Under <a href="https://github.com/allenai/olmes">OLMes</a>‑style prompts, performance looked much more reasonable, but the extreme prompt fragility made us investigate further.</p>
<p>We determined the root cause to be contamination. That is, we (accidentally) cheated but we cheated badly.
Our <a href="https://huggingface.co/datasets/allenai/dolmino-mix-1124/tree/main/data/math">Dolmino</a> math bundle included GSM8k test items in a <code>test.json</code>.
Although we later updated preprocessing to drop <code>test.json</code>, the dataset had already been cached on the cluster, introducing contamination for GSM8K on the Bison cooldown.</p>
<p>Dolmino’s GSM8k uses OLMes formatting, not LM Eval’s default.
Therefore, rather than the expected improvement in results from training on the test,
contamination made our model have dramatically increased surprisal on the original prompts structured tags (e.g., <code>16-8=&lt;&lt;16-7=9&gt;&gt;9</code>) that don’t appear in the contaminated data.
This high surprisal seemed to cause our model to perform much worse than it did on the same questions with those tags removed.</p>
<p>That said, our MATH performance was also quite poor, and we have no reason to believe it was contaminated.
So, our generally poor performance on math may have other causes.</p>
<h5 id="shuffling-anomaly">Shuffling anomaly<a class="headerlink" href="#shuffling-anomaly" title="Permanent link">&para;</a></h5>
<p>Near 190k steps, training loss phase‑shifted while validation remained stable.
This is normally observed when we change the underlying training data mix, but in this case we hadn't!
We had separately begun to wonder whether our pseudo-random shuffle, which finds a co-prime step size across data indices, was leading to a somewhat unlucky shuffle where batches came from correlated data. This phase shift in cooldown increased our confidence this was happening!</p>
<p><img alt="Training loss phase shift during Bison cooldown while evaluation losses remain flat" src="../32b-shuffle-spike.png" /></p>
<p>As you can see, this "spike" ended up being a phase shift in training loss that never recovered.
But, eval losses were unaffected.
This strongly indicated that the underlying data had shifted, rather than any model instability.</p>
<p>Why did we use such a bad shuffle? Because it is cheap and stateless! We can compute the index of any data point on the fly without storing large permutation tables.
In addition, our training stack Levanter samples from each mixture component directly, meaning that we always have a stable proportion of tokens from each dataset.
This has historically mitigated the risk of poor shuffles, but in this case it seems to have failed us.</p>
<p><strong>Tokens trained:</strong> ≈1.074T tokens (32,000 steps from 160k → 192k at 4096 seq len, global batch 8192).</p>
<h4 id="attempt-2-mantis-cooldown">Attempt 2 — Mantis Cooldown<a class="headerlink" href="#attempt-2-mantis-cooldown" title="Permanent link">&para;</a></h4>
<ul>
<li>Issue: <a href="https://github.com/marin-community/marin/issues/1681">#1581</a></li>
<li>Experiment: <a href="https://github.com/marin-community/marin/blob/main/experiments/tootsie/exp1529_32b_mantis_cooldown.py"><code>exp1529_32b_mantis_cooldown</code></a></li>
<li><a href="https://marin.community/data-browser/experiment/?path=gs%3A//marin-us-central2/experiments/exp1529_32b_mantis_cooldown-c6f4b0.json">Data Browser Link</a></li>
</ul>
<p>We restarted from 160k with two major changes:</p>
<ul>
<li>Feistel shuffle. Switched from linear permutation to a more robust Feistel‑based shuffle.</li>
<li>Cleaner math mix. Replaced Dolmino math with <a href="https://arxiv.org/abs/2504.02807">MegaMath</a> splits and later added <a href="https://arxiv.org/abs/2506.05209">Common Pile</a>'s <a href="https://huggingface.co/datasets/common-pile/stackv2_edu_filtered">Stack V2 EDU</a> Python (around 174k), redistributing the HQ budget accordingly.</li>
</ul>
<p>Cooldown mixture (normalized share):</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td>nemotron_cc/medium</td>
<td>21.49%</td>
</tr>
<tr>
<td>nemotron_cc/hq_synth</td>
<td>17.29%</td>
</tr>
<tr>
<td>nemotron_cc/medium_low</td>
<td>9.79%</td>
</tr>
<tr>
<td>nemotron_cc/hq_actual</td>
<td>5.81%</td>
</tr>
<tr>
<td>nemotron_cc/medium_high</td>
<td>5.24%</td>
</tr>
<tr>
<td>nemotron_cc/low_actual</td>
<td>4.46%</td>
</tr>
<tr>
<td>nemotron_cc/low_synth</td>
<td>3.99%</td>
</tr>
<tr>
<td>megamath/web</td>
<td>5.57%</td>
</tr>
<tr>
<td>arxiv_markdownified</td>
<td>4.54%</td>
</tr>
<tr>
<td>megamath/text_code_block</td>
<td>4.24%</td>
</tr>
<tr>
<td>dolmino/pes2o</td>
<td>4.54%</td>
</tr>
<tr>
<td>megamath/web_pro</td>
<td>1.27%</td>
</tr>
<tr>
<td>megamath/translated_code</td>
<td>0.61%</td>
</tr>
<tr>
<td>megamath/qa</td>
<td>0.59%</td>
</tr>
<tr>
<td>finemath-3-plus</td>
<td>2.66%</td>
</tr>
<tr>
<td>dolmino/flan</td>
<td>2.66%</td>
</tr>
<tr>
<td>stackexchange_custom</td>
<td>1.34%</td>
</tr>
<tr>
<td>dolmino/stackexchange</td>
<td>1.34%</td>
</tr>
<tr>
<td>starcoderdata</td>
<td>1.59%</td>
</tr>
<tr>
<td>proofpile_2</td>
<td>0.35%</td>
</tr>
<tr>
<td>wikipedia_markdown</td>
<td>0.29%</td>
</tr>
<tr>
<td>dolmino/wiki</td>
<td>0.29%</td>
</tr>
<tr>
<td>medu_science_qa</td>
<td>0.09%</td>
</tr>
</tbody>
</table>
<p>Notes:
- At ~174k steps, we introduced <code>common_pile_stackv2_edu_filtered_python</code> and re‑normalized the HQ portion accordingly.
- Sampling permutation switched to Feistel.</p>
<p>We kept the optimizer schedule identical to Bison. With better shuffling and clean math, both failure modes disappeared.</p>
<p><strong>Tokens trained:</strong> ≈1.074T tokens (32,000 steps from 160k → 192k at 4096 seq len, global batch 8192).</p>
<h5 id="shuffling-linear-vs-feistel">Shuffling: Linear vs. Feistel<a class="headerlink" href="#shuffling-linear-vs-feistel" title="Permanent link">&para;</a></h5>
<p>Within each batch, we want examples that are as i.i.d. as possible from the full training distribution.
This reduces within‑batch correlation and avoids long, correlated stretches that can bias updates or create non‑stationary “phases” in the loss curve. This also reduces gradient variance from batch to batch, which recent <a href="https://www.lesswrong.com/posts/j3gp8tebQiFJqzBgg/how-the-nanogpt-speedrun-wr-dropped-by-20-in-3-months">NanoGPT speedruns</a> have found beneficial.</p>
<p>To achieve this in a reproducible way at runtime, we compute pseudo-random permutations over training data blocks inside of the data loader. We previously used an affine/LCG permutation, choosing integers <code>a</code> and <code>b</code> with <code>gcd(a, N) = 1</code> for dataset length <code>N</code>, and mapping indices by <code>p(x) = (a * x + b) % N</code>. This is a valid permutation (every index appears exactly once), cheap, and stateless.</p>
<p>The issue is that if our step size is very small (or very large) and the data is not pre-shuffled, there can be clear phases in our training data!</p>
<p>In Mantis, we switched to a Feistel‑network permutation, another pseudo‑random permutation (PRP) over the index domain. Conceptually, Feistel splits the bit representation into halves and applies several mixing rounds with per‑round keys, yielding a bijection with much better mixing properties than an affine map. Empirically, this resolved the phase shift effect we had seen earlier.</p>
<p><img alt="Feistel shuffle removes the cooldown phase shift versus the prior linear permutation" src="../32b-feistel-vs-lcg.png" /></p>
<p>In addition, our validation losses looked better across the board (not just code!)</p>
<p><img alt="Mantis cooldown yields lower Paloma c4_en loss curves compared with Bison cooldown" src="../32b-paloma-c4-en-permutation.png" /></p>
<p><img alt="Overall Paloma average losses improve under Mantis cooldown relative to Bison" src="../32b-paloma-average-permutation.png" /></p>
<p>Needless to say, we'll be using Feistel going forward!</p>
<p><strong>What we learned:</strong> Shuffle quality matters even given perfect per-domain shuffling.</p>
<h2 id="base-model-results">Base Model Results<a class="headerlink" href="#base-model-results" title="Permanent link">&para;</a></h2>
<p>We evaluate with Eleuther's <a href="https://github.com/EleutherAI/lm-evaluation-harness">LM Eval Harness</a> defaults across a standard suite. Numbers may differ from model cards or
other evaluation harnesses (e.g. OLMES) due to prompt/format differences. “Average” is a simple mean over shown tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Average</th>
<th style="text-align: right;">AGI Eval LSAT-AR</th>
<th style="text-align: right;">ARC Easy</th>
<th style="text-align: right;">ARC Challenge</th>
<th style="text-align: right;">BoolQ</th>
<th style="text-align: right;">CommonSense QA</th>
<th style="text-align: right;">COPA</th>
<th style="text-align: right;">HellaSwag</th>
<th style="text-align: right;">lambada_openai</th>
<th style="text-align: right;">OpenBookQA</th>
<th style="text-align: right;">PIQA</th>
<th style="text-align: right;">WinoGrande</th>
<th style="text-align: right;">WSC</th>
<th style="text-align: right;">MMLU</th>
<th style="text-align: right;">GPQA</th>
<th style="text-align: right;">BBH</th>
<th style="text-align: right;">MMLU Pro</th>
<th style="text-align: right;">HumanEval</th>
<th style="text-align: right;">GSM8K</th>
<th style="text-align: right;">MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Marin 32B (Bison)</strong></td>
<td style="text-align: right;">63.0</td>
<td style="text-align: right;">23.4</td>
<td style="text-align: right;">87.8</td>
<td style="text-align: right;"><strong>65.8</strong></td>
<td style="text-align: right;">88.9</td>
<td style="text-align: right;">82.3</td>
<td style="text-align: right;"><strong>94.0</strong></td>
<td style="text-align: right;">86.6</td>
<td style="text-align: right;">77.4</td>
<td style="text-align: right;">46.6</td>
<td style="text-align: right;"><strong>86.1</strong></td>
<td style="text-align: right;">78.61</td>
<td style="text-align: right;">82.42</td>
<td style="text-align: right;">72.9</td>
<td style="text-align: right;">32.13</td>
<td style="text-align: right;">55.2</td>
<td style="text-align: right;">41.9</td>
<td style="text-align: right;">29.27</td>
<td style="text-align: right;">54.71</td>
<td style="text-align: right;">10.35</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Marin 32B (Mantis)</strong></td>
<td style="text-align: right;">65.2</td>
<td style="text-align: right;">24.8</td>
<td style="text-align: right;">88.0</td>
<td style="text-align: right;">65.7</td>
<td style="text-align: right;"><strong>89.4</strong></td>
<td style="text-align: right;">82.8</td>
<td style="text-align: right;">93.0</td>
<td style="text-align: right;"><strong>86.9</strong></td>
<td style="text-align: right;">77.2</td>
<td style="text-align: right;">46.4</td>
<td style="text-align: right;">85.9</td>
<td style="text-align: right;"><strong>79.3</strong></td>
<td style="text-align: right;">79.5</td>
<td style="text-align: right;">74.7</td>
<td style="text-align: right;">34.0</td>
<td style="text-align: right;">59.6</td>
<td style="text-align: right;">45.1</td>
<td style="text-align: right;">42.7</td>
<td style="text-align: right;">69.1</td>
<td style="text-align: right;">15.3</td>
</tr>
<tr>
<td style="text-align: left;"><strong>OLMo 2 32B Base</strong></td>
<td style="text-align: right;">63.2</td>
<td style="text-align: right;">22.6</td>
<td style="text-align: right;">85.9</td>
<td style="text-align: right;">61.86</td>
<td style="text-align: right;">83.0</td>
<td style="text-align: right;">78.6</td>
<td style="text-align: right;">93.0</td>
<td style="text-align: right;">85.9</td>
<td style="text-align: right;"><strong>78.3</strong></td>
<td style="text-align: right;"><strong>47.2</strong></td>
<td style="text-align: right;">83.08</td>
<td style="text-align: right;">78.85</td>
<td style="text-align: right;">86.81</td>
<td style="text-align: right;">71.85</td>
<td style="text-align: right;">32.21</td>
<td style="text-align: right;">56.07</td>
<td style="text-align: right;">42.0</td>
<td style="text-align: right;">23.78</td>
<td style="text-align: right;">76.35</td>
<td style="text-align: right;">12.69</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Qwen 2.5 32B Base</strong></td>
<td style="text-align: right;">68.1</td>
<td style="text-align: right;"><strong>30.43</strong></td>
<td style="text-align: right;">80.81</td>
<td style="text-align: right;">55.89</td>
<td style="text-align: right;">87.65</td>
<td style="text-align: right;"><strong>88.45</strong></td>
<td style="text-align: right;">87.0</td>
<td style="text-align: right;">84.11</td>
<td style="text-align: right;">77.62</td>
<td style="text-align: right;">44.4</td>
<td style="text-align: right;">82.4</td>
<td style="text-align: right;">75.7</td>
<td style="text-align: right;">80.95</td>
<td style="text-align: right;"><strong>80.83</strong></td>
<td style="text-align: right;"><strong>39.01</strong></td>
<td style="text-align: right;"><strong>67.35</strong></td>
<td style="text-align: right;"><strong>57.9</strong></td>
<td style="text-align: right;"><strong>48.78</strong></td>
<td style="text-align: right;"><strong>89.31</strong></td>
<td style="text-align: right;">36.25</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Gemma 3 27B PT</strong></td>
<td style="text-align: right;">65.1</td>
<td style="text-align: right;">22.17</td>
<td style="text-align: right;"><strong>88.17</strong></td>
<td style="text-align: right;">65.44</td>
<td style="text-align: right;">87.09</td>
<td style="text-align: right;">73.38</td>
<td style="text-align: right;">93.0</td>
<td style="text-align: right;">83.02</td>
<td style="text-align: right;">78.07</td>
<td style="text-align: right;">45.0</td>
<td style="text-align: right;">84.06</td>
<td style="text-align: right;">79.01</td>
<td style="text-align: right;"><strong>91.94</strong></td>
<td style="text-align: right;">75.33</td>
<td style="text-align: right;">35.74</td>
<td style="text-align: right;">61.36</td>
<td style="text-align: right;">49.44</td>
<td style="text-align: right;">17.6</td>
<td style="text-align: right;">82.03</td>
<td style="text-align: right;">25.83</td>
</tr>
<tr>
<td style="text-align: left;"><strong>NVIDIA Nemotron Nano 12B v2 Base</strong></td>
<td style="text-align: right;"><strong>68.6</strong></td>
<td style="text-align: right;">28.7</td>
<td style="text-align: right;">83.59</td>
<td style="text-align: right;">60.58</td>
<td style="text-align: right;">84.83</td>
<td style="text-align: right;">76.09</td>
<td style="text-align: right;">85.0</td>
<td style="text-align: right;">81.42</td>
<td style="text-align: right;">72.93</td>
<td style="text-align: right;">45.8</td>
<td style="text-align: right;">82.81</td>
<td style="text-align: right;">74.35</td>
<td style="text-align: right;">85.35</td>
<td style="text-align: right;">77.9</td>
<td style="text-align: right;">36.58</td>
<td style="text-align: right;">62.02</td>
<td style="text-align: right;">53.13</td>
<td style="text-align: right;"><strong>59.15</strong></td>
<td style="text-align: right;">84.08</td>
<td style="text-align: right;"><strong>68.28</strong></td>
</tr>
</tbody>
</table>
<p>The newer Mantis cooldown is better in almost every respect compared to Bison. COPA, PIQA and WSC see slight degradations, but, as expected, the coding evaluation HumanEval and math evals GSM8K and MATH
see marked improvements.</p>
<p>In terms of mean rank, Marin fares quite well among other open weights base models:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Mean Rank</th>
<th style="text-align: right;">Mean Reciprocal Rank</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Marin 32B (Bison)</strong></td>
<td style="text-align: right;">3.68</td>
<td style="text-align: right;">0.39</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Marin 32B (Mantis)</strong></td>
<td style="text-align: right;"><strong>3.05</strong></td>
<td style="text-align: right;">0.44</td>
</tr>
<tr>
<td style="text-align: left;"><strong>OLMo 2 32B Base</strong></td>
<td style="text-align: right;">3.89</td>
<td style="text-align: right;">0.34</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Qwen 2.5 32B Base</strong></td>
<td style="text-align: right;">3.16</td>
<td style="text-align: right;"><strong>0.54</strong></td>
</tr>
<tr>
<td style="text-align: left;"><strong>Gemma 3 27B PT</strong></td>
<td style="text-align: right;">3.37</td>
<td style="text-align: right;">0.39</td>
</tr>
<tr>
<td style="text-align: left;"><strong>NVIDIA Nemotron Nano 12B v2 Base</strong></td>
<td style="text-align: right;">3.68</td>
<td style="text-align: right;">0.38</td>
</tr>
</tbody>
</table>
<p>Overall, the Mantis 32B model does quite well! In terms of mean rank, it is the best of these models (lower is better).
That said, Qwen 2.5 32B Base has a higher mean reciprocal rank.</p>
<p>In terms of average accuracy, it manages to surpass OLMo 32B (+2.0, better on 14 of 19), the previous best open source base model,
and is more or less on par with Gemma 3 27B PT (+0.1, better on 9 of 19).
It is a bit behind Qwen 2.5 32B Base (-2.8, better on 8/19) and NVIDIA Nemotron Nano 12B v2 Base (-3.4, better on 10/19).
(Nemotron Nano has particularly strong math and code performance, likely due to its training mixture.)</p>
<h3 id="an-aside-on-terminology">An Aside on Terminology<a class="headerlink" href="#an-aside-on-terminology" title="Permanent link">&para;</a></h3>
<p>When we say "open weights," we mean that the model weights are publicly available for download without restrictive licenses.
These models do not necessarily have open training code or data. When we say "open source," we mean that the model weights,
training code, and training data are all publicly available without restrictive licenses.
All open source models are open weights, but not all open weights models are open source.
Qwen, Gemma 3, and Nemotron Nano are open weight, but not open source. Marin and OLMo are open source.
Note that this is a spectrum: to their credit, NVIDIA released significant details about <a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2#training-datasets">Nemotron Nano's training process and data</a>.</p>
<p>"Base" here means that these models have not undergone instruction tuning or RLHF, though increasingly
the line is blurred as Marin, OLMo, and Nemotron Nano include instruction, reasoning, or task data in their pretraining mixtures.
(We don't know the details of Qwen or Gemma's pretraining mixtures!)
We have not yet produced or evaluated instruction tuned or RLHF versions of Marin 32B. This is planned.</p>
<h3 id="caveats">Caveats<a class="headerlink" href="#caveats" title="Permanent link">&para;</a></h3>
<p>As ever, several caveats are in order:</p>
<ul>
<li>These results are from Eleuther's LM Eval Harness with default prompts; other prompt styles (e.g. OLMes) may yield different results.
  Eleuther tends to be a bit stricter than other frameworks, and it doesn't allow any prompt engineering. Your mileage may vary!</li>
<li>These results are for base models without any instruction tuning or RLHF.</li>
<li>Gemma 3 supports images, and the open weights models support more natural languages than English; these results are only for English text tasks.</li>
<li>These are 19 tasks out of many possible tasks; other tasks may yield different relative results.</li>
</ul>
<h2 id="limitations">Limitations<a class="headerlink" href="#limitations" title="Permanent link">&para;</a></h2>
<p>While we believe Marin 32B is a strong open source base model, it has some limitations:</p>
<ul>
<li>It is only a base model! While we believe it is a good base model, it lacks instruction tuning or RLHF that would make it more useful for end users.</li>
<li>It is still relatively large at 32B parameters, which may make it difficult to deploy for some users.</li>
<li>It has not undergone long context extension training, which may limit its performance on long context tasks.</li>
<li>As stated, our model is only trained on English text (as well as programming languages), limiting its performance on other languages.</li>
</ul>
<p>We welcome community contributions (<a href="https:/github.com/marin-community/marin/issues">Github</a>, <a href="https://discord.gg/J9CTk7pqcM">Discord</a>) to address these and other limitations!</p>
<h2 id="lessons-learned">Lessons Learned<a class="headerlink" href="#lessons-learned" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Pride goeth before a fall.</strong> Our initial hubris in thinking we could scale our recipe to 32B without QK‑Norm cost us time and compute. We should have tested QK‑Norm earlier.</li>
<li><strong>Midflight changes can work.</strong> The Tootsie Roll process of starting training early and making evidence‑driven changes mid‑flight worked well here, as it did in 8B. While we aspire to a clean run, we anticipate that our first release at each scale will often involve changes mid-run.</li>
<li><strong>Data shuffling quality matters more than we thought.</strong> The phase shift we observed during Bison cooldown highlighted the importance of good shuffling, as others (including OLMo) have noted. The Feistel shuffle we adopted in Mantis seems to have resolved this.</li>
<li><strong>Double check your configs.</strong> We should have caught the GSM8k contamination earlier. While we did eventually identify and fix it, this underscores the importance of verifying dataset contents and preprocessing steps, especially when using cached data. We now have checks in place to prevent similar issues in future runs.</li>
<li><strong>Instrument heavily.</strong> Our ability to diagnose and address issues mid-flight was greatly aided by extensive logging and monitoring. We recommend others invest in good instrumentation from the start.</li>
</ul>
<h2 id="acknowledgments">Acknowledgments<a class="headerlink" href="#acknowledgments" title="Permanent link">&para;</a></h2>
<p>We would like to acknowledge:</p>
<ul>
<li>Google, especially Google TPU Research Cloud for providing the compute;</li>
<li>The datasets and evals produced by AI2, Eleuther, Hugging Face, LLM360, NVIDIA, among others;</li>
<li>Advice from Elie Bakouch, Lucas Nestler, Omead Pooladzandi, Alec Radford, Luca Soldaini, and Evan Walters on loss spikes (as well as many people privately);</li>
<li>and our Discord community!</li>
</ul>
<h2 id="glossary">Glossary<a class="headerlink" href="#glossary" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>AdamC</strong> — An adjusted weight‑decay computation used during LR decay to reduce gradient growth.</li>
<li><strong>Cooldown</strong> — A training phase with gradually decreased LR, often paired with a higher‑quality, more structured data mix.</li>
<li><strong>EMA (Exponential Moving Average)</strong> — Weight averaging used for evaluation to reduce variance from hot parameters.</li>
<li><strong>Feistel shuffle</strong> — A deterministic, epoch‑wise re‑randomization scheme that avoids the autocorrelation of simple linear permutations.</li>
<li><strong>QK‑Norm</strong> — Normalization of query/key vectors in attention; improves headroom against spikes at large scales.</li>
<li><strong>Tootsie Roll process</strong> — Marin’s pragmatic strategy: start quickly, keep training, fold in changes mid‑flight as evidence accumulates.</li>
<li><strong>Z-loss</strong> — A small penalty on the logit norm to prevent <code>lm_head</code> explosions during deep cooldowns.</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../marin-8b-retro/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Marin 8B Retro">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Marin 8B Retro
              </div>
            </div>
          </a>
        
        
          
          <a href="../markdownified-datasets/" class="md-footer__link md-footer__link--next" aria-label="Next: Markdownified Datasets">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Markdownified Datasets
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/marin-community/marin" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://discord.gg/DhbGDq3jv4" target="_blank" rel="noopener" title="discord.gg" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M492.5 69.8c-.2-.3-.4-.6-.8-.7-38.1-17.5-78.4-30-119.7-37.1-.4-.1-.8 0-1.1.1s-.6.4-.8.8c-5.5 9.9-10.5 20.2-14.9 30.6-44.6-6.8-89.9-6.8-134.4 0-4.5-10.5-9.5-20.7-15.1-30.6-.2-.3-.5-.6-.8-.8s-.7-.2-1.1-.2C162.5 39 122.2 51.5 84.1 69c-.3.1-.6.4-.8.7C7.1 183.5-13.8 294.6-3.6 404.2c0 .3.1.5.2.8s.3.4.5.6c44.4 32.9 94 58 146.8 74.2.4.1.8.1 1.1 0s.7-.4.9-.7c11.3-15.4 21.4-31.8 30-48.8.1-.2.2-.5.2-.8s0-.5-.1-.8-.2-.5-.4-.6-.4-.3-.7-.4c-15.8-6.1-31.2-13.4-45.9-21.9-.3-.2-.5-.4-.7-.6s-.3-.6-.3-.9 0-.6.2-.9.3-.5.6-.7c3.1-2.3 6.2-4.7 9.1-7.1.3-.2.6-.4.9-.4s.7 0 1 .1c96.2 43.9 200.4 43.9 295.5 0 .3-.1.7-.2 1-.2s.7.2.9.4c2.9 2.4 6 4.9 9.1 7.2.2.2.4.4.6.7s.2.6.2.9-.1.6-.3.9-.4.5-.6.6c-14.7 8.6-30 15.9-45.9 21.8-.2.1-.5.2-.7.4s-.3.4-.4.7-.1.5-.1.8.1.5.2.8c8.8 17 18.8 33.3 30 48.8.2.3.6.6.9.7s.8.1 1.1 0c52.9-16.2 102.6-41.3 147.1-74.2.2-.2.4-.4.5-.6s.2-.5.2-.8c12.3-126.8-20.5-236.9-86.9-334.5zm-302 267.7c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.4 59.2-52.8 59.2m195.4 0c-29 0-52.8-26.6-52.8-59.2s23.4-59.2 52.8-59.2c29.7 0 53.3 26.8 52.8 59.2 0 32.7-23.2 59.2-52.8 59.2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["search.highlight", "search.share", "search.suggest", "content.code.annotation", "content.code.copy", "navigation.footer"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascript/readthedocs.js"></script>
      
    
  </body>
</html>