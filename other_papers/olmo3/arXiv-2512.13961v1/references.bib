%%%%%%%%%%%%%%% CONFERENCE / JOURNAL NAMES %%%%%%%%%%%%%%%
@string{iclr = "International Conference on Learning Representations \CNFX{ICLR}"}
@string{icml = "International Conference on Machine Learning \CNFX{ICML}"}
@string{jmlr = "Journal of Machine Learning Research \CNFX{JMLR}"}
@string{nips = "Advances in Neural Information Processing Systems \CNFX{NeurIPS}"}
@string{uai = "Conference on Uncertainty in Artificial Intelligence \CNFX{UAI}"}
@string{mlj = "Machine Learning Journal \CNFX{MLJ}"}
@string{FTML = "Foundations and Trends in Machine Learning" }
@string{colt = "Conference on Learning Theory \CNFX{COLT}"}
@string{clt = "Computational Learning Theory"}
@string{alt = "Algorithmic Learning Theory"}
@string{ecml = "European Conference on Machine Learning and Data Mining \CNFX{ECML-PKDD}"}
@string{naacl = "Conference of the North American Chapter of the Association for Computational Linguistics \CNFX{NAACL}"}
@string{tacl = "Transactions of the Association for Computational Linguistics \CNFX{TACL}"}
@string{tac = "Text Analysis Conference \CNFX{TAC}"}
@string{emnlp = "Conference on Empirical Methods in Natural Language Processing \CNFX{EMNLP}"}
@string{acl = "Annual Meeting of the Association for Computational Linguistics \CNFX{ACL}"}
@string{conll = "SIGNLL Conference on Natural Language Learning \CNFX{CoNLL}" }
@string{lrec = "International Conference on Language Resources and Evaluation \CNFX{LREC}"}
@string{semeval = "International Workshop on Semantic Evaluation \CNFX{SemEval}"}
@string{eacl = "Conference of the European Chapter of the Association for Computational Linguistics \CNFX{EACL}" }
@string{wmt = "Conference on Machine Translation \CNFX{WMT}"}
@string{interspeech = "Annual Conference of the International Speech Communication Association \CNFX{INTERSPEECH}"}
@string{ssem = "Joint Conference on Lexical and Computational Semantics \CNFX{*SEM}"}
@string{coling = "International Conference on Computational Linguistics \CNFX{COLING}" }
@string{aaai = "Conference on Artificial Intelligence \CNFX{AAAI}"}
@string{ijcai = "International Joint Conferences on Artificial Intelligence \CNFX{IJCAI}"}
@string{aimag = "AI Magazine"}
@string{akbc = "AKBC Workshop"}
@string{icdm = "IEEE International Conference on Data Mining \CNFX{ICDM}"}
@string{kdd = "ACM Conf. Knowl. Disc. and Data Mining \CNFX{KDD}"}
@string{www = "International Conference ib World Wide Web Conference \CNFX{WWW}"}
@string{chi = "Conference on Human Factors in Computing Systems \CNFX{CHI}"}
@string{corl = "Conference on Robot Learning \CNFX{IJCAI}"}
@string{cl = "Computational Linguistics \CNFX{CL}"}
@string{sigir = "Conference of the Association for Computing Machinery Special Interest Group in Information Retrieval \CNFX{SIGIR}"}
@string{acml = "Asian Conference on Machine Learning \CNFX{ACML}"}
@string{facct = "ACM Conference on Fairness, Accountability and Transparency \CNFX{FAccT}"}
@string{aies="AAAI/ACM Conference on AI, Ethics, and Society \CNFX{AIES}"}
@string{iccg = "International Conference on Computers and Games \CNFX{ICCG}"}
@string{pnas = "Proceedings of the National Academy of Sciences \CNFX{PNAS}"}
@string{sat="International Conference on Theory and Applications of Satisfiability Testing \CNFX{SAT}"}
@string{iccv="International Conference on Computer Vision \CNFX{ICCV}"}
@string{cvpr = "IEEE Conference on Computer Vision and Pattern Recognition \CNFX{CVPR}"}
@string{stoc="ACM symposium on Theory of computing\CNFX{STOC}"}
@string{algorithmica = "Algorithmica"}
@string{JACM = "Journal of Association for Computing Machinery"}
@string{ITA = "Information Theory and Applications Workshop \CNFX{ITA}" }
@string{JCSS = "Journal of Computer and System Sciences \CNFX{JCSS}" }
@string{MPROG = "Mathematical Programming"}
@string{SIOPT = "SIAM Journal on Optimization \CNFX{SIOPT}"}
@string{SICOMP = "SIAM Journal on Computing \CNFX{SICOMP}"}
@string{ORL = "Operations Ressearch Letters" }
@string{iit = "IEEE Transactions on Information Theory"}
@string{tip = "IEEE Transactions on Image Processing \CNFX{TIP}"}
@string{cacm = "Communications of the ACM"}
@string{science = "Science"}
@preamble{"\providecommand{\CNFX}[1]{{\em{\textrm{(#1)}}}}" }


# Preferences General ############################################################
@article{lambert2023entangled,
  title={Entangled preferences: The history and risks of reinforcement learning and human feedback},
  author={Lambert, Nathan and Gilbert, Thomas Krendl and Zick, Tom},
  journal={arXiv preprint arXiv:2310.13595},
  year={2023}
}

@article{wirth2017survey,
  title={A survey of preference-based reinforcement learning methods},
  author={Wirth, Christian and Akrour, Riad and Neumann, Gerhard and F{\"u}rnkranz, Johannes},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={136},
  pages={1--46},
  year={2017}
}
################################################################################################

# AI General ####################################################################
@book{russell2016artificial,
  title={Artificial intelligence: a modern approach},
  author={Russell, Stuart J and Norvig, Peter},
  year={2016},
  publisher={Pearson}
}

################################################################################################


# RL related lit

@inproceedings{kim2023aligning,
    title = "Aligning Large Language Models through Synthetic Feedback",
    author = "Kim, Sungdong  and
      Bae, Sanghwan  and
      Shin, Jamin  and
      Kang, Soyoung  and
      Kwak, Donghyun  and
      Yoo, Kang  and
      Seo, Minjoon",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.844/",
    doi = "10.18653/v1/2023.emnlp-main.844",
    pages = "13677--13700",
}

@inproceedings{knox2008tamer,
  title={Tamer: Training an agent manually via evaluative reinforcement},
  author={Knox, W Bradley and Stone, Peter},
  booktitle={2008 7th IEEE international conference on development and learning},
  pages={292--297},
  year={2008},
  organization={IEEE}
}
@inproceedings{macglashan2017interactive,
  title={Interactive learning from policy-dependent human feedback},
  author={MacGlashan, James and Ho, Mark K and Loftin, Robert and Peng, Bei and Wang, Guan and Roberts, David L and Taylor, Matthew E and Littman, Michael L},
  booktitle={International conference on machine learning},
  pages={2285--2294},
  year={2017},
  organization={PMLR}
}
@inproceedings{warnell2018deep,
  title={Deep tamer: Interactive agent shaping in high-dimensional state spaces},
  author={Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{yu2025dapo,
  title={Dapo: An open-source llm reinforcement learning system at scale},
  author={Yu, Qiying and Zhang, Zheng and Zhu, Ruofei and Yuan, Yufeng and Zuo, Xiaochen and Yue, Yu and Dai, Weinan and Fan, Tiantian and Liu, Gaohong and Liu, Lingjun and others},
  journal={arXiv preprint arXiv:2503.14476},
  year={2025}
}

@article{kaufmann2023survey,
  title={A survey of reinforcement learning from human feedback},
  author={Kaufmann, Timo and Weng, Paul and Bengs, Viktor and H{\"u}llermeier, Eyke},
  journal={arXiv preprint arXiv:2312.14925},
  year={2023}
}
@article{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S},
  journal={A Bradford Book},
  year={2018}
}
@inproceedings{ng2000algorithms,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, Andrew Y and Russell, Stuart and others},
  booktitle={Icml},
  volume={1},
  number={2},
  pages={2},
  year={2000}
}
# RLHF Methods ####################################################################
@article{BradleyTerry,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2334029},
 author = {Ralph Allan Bradley and Milton E. Terry},
 journal = {Biometrika},
 number = {3/4},
 pages = {324--345},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons},
 urldate = {2023-02-13},
 volume = {39},
 year = {1952}
}

@article{likert1932technique,
  title={A technique for the measurement of attitudes.},
  author={Likert, Rensis},
  journal={Archives of psychology},
  year={1932}
}

@article{gilks1992adaptive,
  title={Adaptive rejection sampling for Gibbs sampling},
  author={Gilks, Walter R and Wild, Pascal},
  journal={Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume={41},
  number={2},
  pages={337--348},
  year={1992},
  publisher={Wiley Online Library}
}
@article{ahmadian2024back,
  title={Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms},
  author={Ahmadian, Arash and Cremer, Chris and Gall{\'e}, Matthias and Fadaee, Marzieh and Kreutzer, Julia and {\"U}st{\"u}n, Ahmet and Hooker, Sara},
  journal={arXiv preprint arXiv:2402.14740},
  year={2024}
}
################################################################################################

# Reward Modeling More ####################################################################
@article{zhou2024rmb,
  title={RMB: Comprehensively Benchmarking Reward Models in LLM Alignment},
  author={Zhou, Enyu and Zheng, Guodong and Wang, Binghai and Xi, Zhiheng and Dou, Shihan and Bao, Rong and Shen, Wei and Xiong, Limao and Fan, Jessica and Mou, Yurong and others},
  journal={arXiv preprint arXiv:2410.09893},
  year={2024}
}
@inproceedings{zhu2023principled,
  title={Principled reinforcement learning with human feedback from pairwise or k-wise comparisons},
  author={Zhu, Banghua and Jordan, Michael and Jiao, Jiantao},
  booktitle={International Conference on Machine Learning},
  pages={43037--43067},
  year={2023},
  organization={PMLR}
}
@article{wang2024interpretable,
  title={Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts},
  author={Wang, Haoxiang and Xiong, Wei and Xie, Tengyang and Zhao, Han and Zhang, Tong},
  journal={arXiv preprint arXiv:2406.12845},
  year={2024}
}
@article{zhang2024generative,
  title={Generative verifiers: Reward modeling as next-token prediction},
  author={Zhang, Lunjun and Hosseini, Arian and Bansal, Hritik and Kazemi, Mehran and Kumar, Aviral and Agarwal, Rishabh},
  journal={arXiv preprint arXiv:2408.15240},
  year={2024}
}
@article{mahan2024generative,
  title={Generative Reward Models},
  author={Mahan, Dakota and Phung, Duy Van and Rafailov, Rafael and Blagden, Chase and Lile, Nathan and Castricato, Louis and Franken, Jan-Philipp and Finn, Chelsea and Albalak, Alon},
  year={2024},
  url={https://www.synthlabs.ai/pdf/Generative_Reward_Models.pdf}
}
@article{wang2024helpsteer2,
  title={HelpSteer2: Open-source dataset for training top-performing reward models},
  author={Wang, Zhilin and Dong, Yi and Delalleau, Olivier and Zeng, Jiaqi and Shen, Gerald and Egert, Daniel and Zhang, Jimmy J and Sreedhar, Makesh Narsimhan and Kuchaiev, Oleksii},
  journal={arXiv preprint arXiv:2406.08673},
  year={2024}
}


@article{hu2025open,
  title={Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model},
  author={Hu, Jingcheng and Zhang, Yinmin and Han, Qi and Jiang, Daxin and Zhang, Xiangyu and Shum, Heung-Yeung},
  journal={arXiv preprint arXiv:2503.24290},
  year={2025}
}



@article{su2025klear,
  title={Klear-reasoner: Advancing reasoning capability via gradient-preserving clipping policy optimization},
  author={Su, Zhenpeng and Pan, Leiyu and Bai, Xue and Liu, Dening and Dong, Guanting and Huang, Jiaming and Hu, Wenping and Zhang, Fuzheng and Gai, Kun and Zhou, Guorui},
  journal={arXiv preprint arXiv:2508.07629},
  year={2025}
}

@article{luo2025deepscaler,
  title={Deepscaler: Surpassing o1-preview with a 1.5 b model by scaling rl},
  author={Luo, Michael and Tan, Sijun and Wong, Justin and Shi, Xiaoxiang and Tang, William Y and Roongta, Manan and Cai, Colin and Luo, Jeffrey and Zhang, Tianjun and Li, Li Erran and others},
  journal={Notion Blog},
  year={2025}
}

@article{chen2025acereason,
  title={Acereason-nemotron: Advancing math and code reasoning through reinforcement learning},
  author={Chen, Yang and Yang, Zhuolin and Liu, Zihan and Lee, Chankyu and Xu, Peng and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint arXiv:2505.16400},
  year={2025}
}


@article{ankner2024critique,
  title={Critique-out-loud reward models},
  author={Ankner, Zachary and Paul, Mansheej and Cui, Brandon and Chang, Jonathan D and Ammanabrolu, Prithviraj},
  journal={arXiv preprint arXiv:2408.11791},
  year={2024}
}
@article{park2024offsetbias,
  title={Offsetbias: Leveraging debiased data for tuning evaluators},
  author={Park, Junsoo and Jwa, Seungyeon and Ren, Meiying and Kim, Daeyoung and Choi, Sanghyuk},
  journal={arXiv preprint arXiv:2407.06551},
  year={2024}
}
################################################################################################

# KL Refs ####################################################################
@article{jaques2020human,
  title={Human-centric dialog training via offline reinforcement learning},
  author={Jaques, Natasha and Shen, Judy Hanwen and Ghandeharioun, Asma and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang Shane and Picard, Rosalind},
  journal={arXiv preprint arXiv:2010.05848},
  year={2020}
}
@inproceedings{jaques2017sequence,
  title={Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control},
  author={Jaques, Natasha and Gu, Shixiang and Bahdanau, Dzmitry and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Turner, Richard E and Eck, Douglas},
  booktitle={International Conference on Machine Learning},
  pages={1645--1654},
  year={2017},
  organization={PMLR}
}
@inproceedings{havrilla-etal-2023-trlx,
    title = "trl{X}: A Framework for Large Scale Reinforcement Learning from Human Feedback",
    author = "Havrilla, Alexander  and
      Zhuravinskyi, Maksym  and
      Phung, Duy  and
      Tiwari, Aman  and
      Tow, Jonathan  and
      Biderman, Stella  and
      Anthony, Quentin  and
      Castricato, Louis",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.530",
    doi = "10.18653/v1/2023.emnlp-main.530",
    pages = "8578--8595",
}
@misc{vonwerra2022trl,
  author = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
  title = {TRL: Transformer Reinforcement Learning},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/trl}}
}

# RLHF Core ####################################################################
@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{ibarz2018reward,
  title={Reward learning from human preferences and demonstrations in atari},
  author={Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{leike2018scalable,
  title={Scalable agent alignment via reward modeling: a research direction},
  author={Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  journal={arXiv preprint arXiv:1811.07871},
  year={2018}
}
@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}
@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}
@article{wu2021recursively,
  title={Recursively summarizing books with human feedback},
  author={Wu, Jeff and Ouyang, Long and Ziegler, Daniel M and Stiennon, Nisan and Lowe, Ryan and Leike, Jan and Christiano, Paul},
  journal={arXiv preprint arXiv:2109.10862},
  year={2021}
}


@article{askell2021general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@article{askell2021general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}
@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}
@article{ganguli2022red,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}
@article{glaese2022improving,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author={Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
  journal={arXiv preprint arXiv:2209.14375},
  year={2022}
}
@article{menick2022teaching,
  title={Teaching language models to support answers with verified quotes},
  author={Menick, Jacob and Trebacz, Maja and Mikulik, Vladimir and Aslanides, John and Song, Francis and Chadwick, Martin and Glaese, Mia and Young, Susannah and Campbell-Gillingham, Lucy and Irving, Geoffrey and others},
  journal={arXiv preprint arXiv:2203.11147},
  year={2022}
}
@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@inproceedings{gao2023scaling,
  title={Scaling laws for reward model overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={10835--10866},
  year={2023},
  organization={PMLR}
}

@article{pyatkin2025generalizing,
  title={Generalizing Verifiable Instruction Following},
  author={Pyatkin, Valentina and Malik, Saumya and Graf, Victoria and Ivison, Hamish and Huang, Shengyi and Dasigi, Pradeep and Lambert, Nathan and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2507.02833},
  year={2025}
}

@article{geng2025delta,
  title={The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains},
  author={Geng, Scott and Ivison, Hamish and Li, Chun-Liang and Sap, Maarten and Li, Jerry and Krishna, Ranjay and Koh, Pang Wei},
  journal={arXiv preprint arXiv:2507.06187},
  year={2025}
}


@article{adler2024nemotron,
  title={Nemotron-4 340B Technical Report},
  author={Adler, Bo and Agarwal, Niket and Aithal, Ashwath and Anh, Dong H and Bhattacharya, Pallab and Brundyn, Annika and Casper, Jared and Catanzaro, Bryan and Clay, Sharon and Cohen, Jonathan and others},
  journal={arXiv preprint arXiv:2406.11704},
  year={2024}
}

@article{agarwal2025gpt,
  title={gpt-oss-120b \& gpt-oss-20b model card},
  author={Agarwal, Sandhini and Ahmad, Lama and Ai, Jason and Altman, Sam and Applebaum, Andy and Arbus, Edwin and Arora, Rahul K and Bai, Yu and Baker, Bowen and Bao, Haiming and others},
  journal={arXiv preprint arXiv:2508.10925},
  year={2025}
}

@article{d2025anchored,
  title={Anchored preference optimization and contrastive revisions: Addressing underspecification in alignment},
  author={D'Oosterlinck, Karel and Xu, Winnie and Develder, Chris and Demeester, Thomas and Singh, Amanpreet and Potts, Christopher and Kiela, Douwe and Mehri, Shikib},
  journal={Transactions of the Association for Computational Linguistics},
  volume={13},
  pages={442--460},
  year={2025},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@misc{dubey2024llama,
      title={The Llama 3 Herd of Models},
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathurx and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783},
}
@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
# RLHF More ########################################################################
@article{pang2024iterative,
  title={Iterative reasoning preference optimization},
  author={Pang, Richard Yuanzhe and Yuan, Weizhe and Cho, Kyunghyun and He, He and Sukhbaatar, Sainbayar and Weston, Jason},
  journal={arXiv preprint arXiv:2404.19733},
  year={2024}
}
@article{cohen2022dynamic,
  title={Dynamic planning in open-ended dialogue using reinforcement learning},
  author={Cohen, Deborah and Ryu, Moonkyung and Chow, Yinlam and Keller, Orgad and Greenberg, Ido and Hassidim, Avinatan and Fink, Michael and Matias, Yossi and Szpektor, Idan and Boutilier, Craig and others},
  journal={arXiv preprint arXiv:2208.02294},
  year={2022}
}
@article{ramamurthy2022reinforcement,
  title={Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization},
  author={Ramamurthy, Rajkumar and Ammanabrolu, Prithviraj and Brantley, Kiant{\'e} and Hessel, Jack and Sifa, Rafet and Bauckhage, Christian and Hajishirzi, Hannaneh and Choi, Yejin},
  journal={arXiv preprint arXiv:2210.01241},
  year={2022}
}

@article{gao2024rebel,
  title={Rebel: Reinforcement learning via regressing relative rewards},
  author={Gao, Zhaolin and Chang, Jonathan D and Zhan, Wenhao and Oertell, Owen and Swamy, Gokul and Brantley, Kiant{\'e} and Joachims, Thorsten and Bagnell, J Andrew and Lee, Jason D and Sun, Wen},
  journal={arXiv preprint arXiv:2404.16767},
  year={2024}
}
@article{casper2023open,
  title={Open problems and fundamental limitations of reinforcement learning from human feedback},
  author={Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others},
  journal={arXiv preprint arXiv:2307.15217},
  year={2023}
}
@article{kumar2024training,
  title={Training language models to self-correct via reinforcement learning},
  author={Kumar, Aviral and Zhuang, Vincent and Agarwal, Rishabh and Su, Yi and Co-Reyes, John D and Singh, Avi and Baumli, Kate and Iqbal, Shariq and Bishop, Colton and Roelofs, Rebecca and others},
  journal={arXiv preprint arXiv:2409.12917},
  year={2024}
}
@article{singh2023beyond,
  title={Beyond human data: Scaling self-training for problem-solving with language models},
  author={Singh, Avi and Co-Reyes, John D and Agarwal, Rishabh and Anand, Ankesh and Patil, Piyush and Liu, Peter J and Harrison, James and Lee, Jaehoon and Xu, Kelvin and Parisi, Aaron and others},
  journal={arXiv preprint arXiv:2312.06585},
  year={2023}
}
@misc{openai2024o1,
  title        = {Introducing OpenAI o1-preview},
  author       = {{OpenAI}},
  year         = {2024},
  month        = sep,
  url          = {https://openai.com/index/introducing-openai-o1-preview/},
  note         = {Accessed: 2024-10-18}
}


# LLM as a Judge ####################################################################
@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}
@inproceedings{kim2023prometheus,
  title={Prometheus: Inducing fine-grained evaluation capability in language models},
  author={Kim, Seungone and Shin, Jamin and Cho, Yejin and Jang, Joel and Longpre, Shayne and Lee, Hwaran and Yun, Sangdoo and Shin, Seongjin and Kim, Sungdong and Thorne, James and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@article{huang2024empirical,
  title={An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers},
  author={Huang, Hui and Qu, Yingqi and Liu, Jing and Yang, Muyun and Zhao, Tiejun},
  journal={arXiv preprint arXiv:2403.02839},
  year={2024}
}


@misc{mistral2024ministral,
  author = {{Mistral}},
  title = {{Un Ministral, des Ministraux: Introducing the world’s best edge models.}},
  year = {2024},
  howpublished = {\url{https://mistral.ai/news/ministraux/}},
  note = {Accessed: 2024-12-17}
}

@misc{mistral2024large2,
  author = {{Mistral}},
  title = {{Mistral Large 2: Large Enough}},
  year = {2024},
  howpublished = {\url{https://mistral.ai/news/mistral-large-2407/}},
  note = {Accessed: 2024-12-17}
}


@misc{cohere2024commandR,
  author = {{Cohere}},
  title = {{Command R: Retrieval-Augmented Generation at Production Scale}},
  year = {2024},
  howpublished = {\url{https://cohere.com/blog/command-r}},
  note = {Accessed: 2024-12-17}
}


@misc{cohere2024commandRplus,
  author = {{Cohere}},
  title = {{Introducing Command R+: A Scalable LLM Built for Business}},
  year = {2024},
  howpublished = {\url{https://cohere.com/blog/command-r-plus-microsoft-azure}},
  note = {Accessed: 2024-12-17}
}

@misc{cohere2024commandR7B,
  author = {{Cohere}},
  title = {{Introducing Command R7B: Fast and efficient generative AI}},
  year = {2024},
  howpublished = {\url{https://cohere.com/blog/command-r7b}},
  note = {Accessed: 2024-12-17}
}



Misc Blogs %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@misc{schulman2016klapprox,
  author = {Schulman, John},
  title = {Approximating KL-divergence},
  year = {2016},
  howpublished = {\url{http://joschu.net/blog/kl-approx.html}},
  note = {Accessed: 2024-10-01}
}
@misc{openai2022chatgpt,
  title = {ChatGPT: Optimizing Language Models for Dialogue},
  author = {{OpenAI}},
  year = {2022},
  howpublished = {\url{https://openai.com/blog/chatgpt/}},
  note = {Training a LM with RLHF for suitable use as an all-purpose chat bot.}
}

@misc{santacroce2023efficient,
      title={Efficient RLHF: Reducing the Memory Usage of PPO},
      author={Michael Santacroce and Yadong Lu and Han Yu and Yuanzhi Li and Yelong Shen},
      year={2023},
      eprint={2309.00754},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{sun2023exploring,
      title={Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of RLHF},
      author={Simeng Sun and Dhawal Gupta and Mohit Iyyer},
      year={2023},
      eprint={2309.09055},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mukherjee2023orca,
      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4},
      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},
      year={2023},
      eprint={2306.02707},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% DATSETS
@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{gpt4,
  title={{GPT-4} Technical Report},
  author={OpenAI},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.08774},
  url={https://api.semanticscholar.org/CorpusID:257532815}
}

@misc{gpt35,
  title={{GPT-3.5} Turbo},
  author={OpenAI},
  year={2023},
  url={https://platform.openai.com/docs/models/gp#gpt-3-5-turbo}
}


@inproceedings{tao2024scaling,
  title={Scaling laws with vocabulary: Larger models deserve larger vocabularies},
  author={Tao, Chaofan and Liu, Qian and Dou, Longxu and Muennighoff, Niklas and Wan, Zhongwei and Luo, Ping and Lin, Min and Wong, Ngai},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

@misc{
    title={}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}


https://platform.openai.com/docs/models/gp#gpt-3-5-turbo

@inproceedings{black2022gpt,
  title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
  booktitle={Proceedings of the ACL Workshop on Challenges \& Perspectives in Creating Large Language Models},
  url={https://arxiv.org/abs/2204.06745},
  year={2022}
}


@inproceedings{wei2021flan,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
year={2021}
}

@article{longpre2023flan,
  title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={arXiv preprint arXiv:2301.13688},
  year={2023}
}

@misc{no_robots,
  author = {Nazneen Rajani and Lewis Tunstall and Edward Beeching and Nathan Lambert and Alexander M. Rush and Thomas Wolf},
  title = {No Robots},
  year = {2023},
  publisher = {Hugging Face},
  journal = {Hugging Face repository},
  howpublished = {\url{https://huggingface.co/datasets/HuggingFaceH4/no_robots}}
}


@article{toshniwal2024openmathinstruct,
  title={OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data},
  author={Toshniwal, Shubham and Du, Wei and Moshkov, Ivan and Kisacanin, Branislav and Ayrapetyan, Alexan and Gitman, Igor},
  journal={arXiv preprint arXiv:2410.01560},
  year={2024}
}

@misc{numina_math_7b,
  author = {Edward Beeching and Shengyi Costa Huang and Albert Jiang and Jia Li and Benjamin Lipkin and Zihan Qina and Kashif Rasul and Ziju Shen and Roman Soletskyi and Lewis Tunstall},
  title = {NuminaMath 7B TIR},
  year = {2024},
  publisher = {Numina & Hugging Face},
  journal = {Hugging Face repository},
  howpublished = {\url{https://huggingface.co/AI-MO/NuminaMath-7B-TIR}}
}

@misc{luo2023wizardcoder,
      title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},
      author={Ziyang Luo and Can Xu and Pu Zhao and Qingfeng Sun and Xiubo Geng and Wenxiang Hu and Chongyang Tao and Jing Ma and Qingwei Lin and Daxin Jiang},
      year={2023},
}
@TECHREPORT{Chatterji2025-fs,
  title       = "How People Use {ChatGPT}",
  author      = "Chatterji, Aaron and Cunningham, Thomas and Deming, David and
                 Hitzig, Zoe and Ong, Christopher and Shan, Carl Yan and Wadman,
                 Kevin",
  publisher   = "National Bureau of Economic Research",
  institution = "National Bureau of Economic Research",
  address     = "Cambridge, MA",
  number      = "w34255",
  abstract    = "Founded in 1920, the NBER is a private, non-profit,
                 non-partisan organization dedicated to conducting economic
                 research and to disseminating research findings among
                 academics, public policy makers, and business professionals.",
  month       =  sep,
  year        =  2025,
  doi         = "10.3386/w34255"
}
@article{brahman2024art,
  title={The art of saying no: Contextual noncompliance in language models},
  author={Brahman, Faeze and Kumar, Sachin and Balachandran, Vidhisha and Dasigi, Pradeep and Pyatkin, Valentina and Ravichander, Abhilasha and Wiegreffe, Sarah and Dziri, Nouha and Chandu, Khyathi and Hessel, Jack and others},
  journal={arXiv preprint arXiv:2407.12043},
  year={2024}
}
@software{nvidia/Nemotron-Personas-USA,
  author = {Meyer, Yev and Corneil, Dane},
  title = {{Nemotron-Personas-USA}: Synthetic Personas Aligned to Real-World Distributions
},
  month = {June},
  year = {2025},
  url = {https://huggingface.co/datasets/nvidia/Nemotron-Personas-USA}
}

@article{zhao2024wildchat,
  title={Wildchat: 1m chatgpt interaction logs in the wild},
  author={Zhao, Wenting and Ren, Xiang and Hessel, Jack and Cardie, Claire and Choi, Yejin and Deng, Yuntian},
  journal={arXiv preprint arXiv:2405.01470},
  year={2024}
}
@misc{wildteaming2024,
      title={WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models},
      author={Liwei Jiang and Kavel Rao and Seungju Han and Allyson Ettinger and Faeze Brahman and Sachin Kumar and Niloofar Mireshghallah and Ximing Lu and Maarten Sap and Yejin Choi and Nouha Dziri},
      year={2024},
      eprint={2406.18510},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.18510},
}
@article{han2024wildguard,
  title={Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms},
  author={Han, Seungju and Rao, Kavel and Ettinger, Allyson and Jiang, Liwei and Lin, Bill Yuchen and Lambert, Nathan and Choi, Yejin and Dziri, Nouha},
  journal={arXiv preprint arXiv:2406.18495},
  year={2024}
}
@article{wadden2024sciriff,
  title={SciRIFF: A Resource to Enhance Language Model Instruction-Following over Scientific Literature},
  author={Wadden, David and Shi, Kejian and Morrison, Jacob and Naik, Aakanksha and Singh, Shruti and Barzilay, Nitzan and Lo, Kyle and Hope, Tom and Soldaini, Luca and Shen, Shannon Zejiang and others},
  journal={arXiv preprint arXiv:2406.07835},
  year={2024}
}

@article{chan2024scaling,
  title={Scaling synthetic data creation with 1,000,000,000 personas},
  author={Chan, Xin and Wang, Xiaoyang and Yu, Dian and Mi, Haitao and Yu, Dong},
  journal={arXiv preprint arXiv:2406.20094},
  year={2024}
}
@misc{muennighoff2024olmoeopenmixtureofexpertslanguage,
      title={OLMoE: Open Mixture-of-Experts Language Models},
      author={Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hannaneh Hajishirzi},
      year={2024},
      eprint={2409.02060},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.02060},
}

@article{cui2023ultrafeedback,
  title={{UltraFeedback}: Boosting Language Models with Scaled AI Feedback},
  author={Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and Zhu, Wei and Ni, Yuan and Xie, Guotong and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2310.01377},
  year={2023}
}
@misc{starling2023,
    title = {Starling-7B: Improving LLM Helpfulness & Harmlessness with RLAIF},
    author = {Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Jiao, Jiantao},
    month = {November},
    year = {2023}
}

@article{tunstall2023zephyr,
  title={Zephyr: Direct distillation of lm alignment},
  author={Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Cl{\'e}mentine and Habib, Nathan and others},
  journal={arXiv preprint arXiv:2310.16944},
  year={2023}
}

@inproceedings{
huang2024thenimplementationdetails,
title={The N+ Implementation Details of {RLHF} with {PPO}: A Case Study on {TL};{DR} Summarization},
author={Shengyi Huang and Michael Noukhovitch and Arian Hosseini and Kashif Rasul and Weixun Wang and Lewis Tunstall},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=kHO2ZTa8e3}
}

@misc{noukhovitch2024asynchronousrlhffasterefficient,
      title={Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models},
      author={Michael Noukhovitch and Shengyi Huang and Sophie Xhonneux and Arian Hosseini and Rishabh Agarwal and Aaron Courville},
      year={2024},
      eprint={2410.18252},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.18252},
}
@article{Muennighoff2024OLMoEOM,
  title={OLMoE: Open Mixture-of-Experts Language Models},
  author={Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Daniel Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hanna Hajishirzi},
  journal={ArXiv},
  year={2024},
  volume={abs/2409.02060},
  url={https://api.semanticscholar.org/CorpusID:272366674}
}
@article{Groeneveld2024OLMoAT,
  title={OLMo: Accelerating the Science of Language Models},
  author={Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and A. Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Daniel Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke S. Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hanna Hajishirzi},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.00838},
  url={https://api.semanticscholar.org/CorpusID:267365485}
}

@misc{vonwerra2022trl,
  author = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
  title = {TRL: Transformer Reinforcement Learning},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/trl}}
}


@article{gunter2024apple,
  title={Apple intelligence foundation language models},
  author={Gunter, Tom and Wang, Zirui and Wang, Chong and Pang, Ruoming and Narayanan, Andy and Zhang, Aonan and Zhang, Bowen and Chen, Chen and Chiu, Chung-Cheng and Qiu, David and others},
  journal={arXiv preprint arXiv:2407.21075},
  year={2024}
}

@article{ivison2023camels,
  title={Camels in a changing climate: Enhancing lm adaptation with tulu 2},
  author={Ivison, Hamish and Wang, Yizhong and Pyatkin, Valentina and Lambert, Nathan and Peters, Matthew and Dasigi, Pradeep and Jang, Joel and Wadden, David and Smith, Noah A and Beltagy, Iz and others},
  journal={arXiv preprint arXiv:2311.10702},
  year={2023}
}
@article{wang2023far,
  title={How far can camels go? exploring the state of instruction tuning on open resources},
  author={Wang, Yizhong and Ivison, Hamish and Dasigi, Pradeep and Hessel, Jack and Khot, Tushar and Chandu, Khyathi and Wadden, David and MacMillan, Kelsey and Smith, Noah A and Beltagy, Iz and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={74764--74786},
  year={2023}
}

@article{ivison2024unpacking,
  title={Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback},
  author={Ivison, Hamish and Wang, Yizhong and Liu, Jiacheng and Wu, Zeqiu and Pyatkin, Valentina and Lambert, Nathan and Smith, Noah A and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2406.09279},
  year={2024}
}


@misc{kazdan2024collapsethriveperilspromises,
      title={Collapse or Thrive? Perils and Promises of Synthetic Data in a Self-Generating World},
      author={Joshua Kazdan and Rylan Schaeffer and Apratim Dey and Matthias Gerstgrasser and Rafael Rafailov and David L. Donoho and Sanmi Koyejo},
      year={2024},
      eprint={2410.16713},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.16713},
}

@misc{zhou2023instructionfollowingevaluationlargelanguage,
      title={Instruction-Following Evaluation for Large Language Models},
      author={Jeffrey Zhou and Tianjian Lu and Swaroop Mishra and Siddhartha Brahma and Sujoy Basu and Yi Luan and Denny Zhou and Le Hou},
      year={2023},
      eprint={2311.07911},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.07911},
}

@article{Zhao2023PyTorchFSDP,
  title={PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel},
  author={Yanli Zhao and Andrew Gu and Rohan Varma and Liangchen Luo and Chien-chin Huang and Min Xu and Less Wright and Hamid Shojanazeri and Myle Ott and Sam Shleifer and Alban Desmaison and Can Balioglu and Bernard Nguyen and Geeta Chauhan and Yuchen Hao and Shen Li},
  journal={Proc. VLDB Endow.},
  year={2023},
  volume={16},
  pages={3848-3860},
  url={https://api.semanticscholar.org/CorpusID:258297871}
}

@misc{torchtitan,
  title={TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training},
  author={Wanchao Liang and Tianyu Liu and Less Wright and Will Constable and Andrew Gu and Chien-Chin Huang and Iris Zhang and Wei Feng and Howard Huang and Junjie Wang and Sanket Purandare and Gokul Nadathur and Stratos Idreos},
  year={2024},
  eprint={2410.06511},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://api.semanticscholar.org/CorpusID:273228883},
}

@article{Rajbhandari2019ZeRO,
  title={ZeRO: Memory optimizations Toward Training Trillion Parameter Models},
  author={Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He},
  journal={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  year={2019},
  pages={1-16},
  url={https://api.semanticscholar.org/CorpusID:203736482}
}

@misc{mosaicml2022composer,
  author = {{The Mosaic ML Team}},
  title = {composer},
  year = {2021},
  howpublished = {\url{https://github.com/mosaicml/composer/}},
}

@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

@inproceedings{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@misc{hsu2025ligerkernelefficienttriton,
      title={Liger Kernel: Efficient Triton Kernels for LLM Training},
      author={Pin-Lun Hsu and Yun Dai and Vignesh Kothapalli and Qingquan Song and Shao Tang and Siyu Zhu and Steven Shimizu and Shivam Sahni and Haowen Ning and Yanning Chen},
      year={2025},
      eprint={2410.10989},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.10989},
}

@article{Patterson2021CarbonEA,
  title={Carbon Emissions and Large Neural Network Training},
  author={David A. Patterson and Joseph Gonzalez and Quoc V. Le and Chen Liang and Llu{\'i}s-Miquel Mungu{\'i}a and Daniel Rothchild and David R. So and Maud Texier and Jeff Dean},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.10350},
  url={https://api.semanticscholar.org/CorpusID:233324338}
}
%%% Eval related citations (e.g. tasks)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{brownNeurips2020,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{biderman2024lessons,
      title={Lessons from the Trenches on Reproducible Evaluation of Language Models},
      author={Stella Biderman and Hailey Schoelkopf and Lintang Sutawika and Leo Gao and Jonathan Tow and Baber Abbasi and Alham Fikri Aji and Pawan Sasanka Ammanamanchi and Sidney Black and Jordan Clive and Anthony DiPofi and Julen Etxaniz and Benjamin Fattori and Jessica Zosa Forde and Charles Foster and Mimansa Jaiswal and Wilson Y. Lee and Haonan Li and Charles Lovering and Niklas Muennighoff and Ellie Pavlick and Jason Phang and Aviya Skowron and Samson Tan and Xiangru Tang and Kevin A. Wang and Genta Indra Winata and François Yvon and Andy Zou},
      year={2024},
      journal={arXiv:2405.14782},
      eprint={2405.14782},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  howpublished = {\url{https://zenodo.org/records/10256836}},
  url          = {https://zenodo.org/records/10256836}
}

@article{clark2018think,
      title={Think you have Solved Question Answering? {T}ry {ARC}, the {AI2} Reasoning Challenge},
      author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      year={2018},
      eprint={1803.05457},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      volume={arXiv:1803.05457},
      journal={CoRR}
}

@inproceedings{clark-etal-2019-boolq,
    title = "{B}ool{Q}: Exploring the Surprising Difficulty of Natural Yes/No Questions",
    author = "Clark, Christopher  and
      Lee, Kenton  and
      Chang, Ming-Wei  and
      Kwiatkowski, Tom  and
      Collins, Michael  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1300",
    doi = "10.18653/v1/N19-1300",
    pages = "2924--2936",
}

@inproceedings{talmor-etal-2019-commonsenseqa,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1421",
    doi = "10.18653/v1/N19-1421",
    pages = "4149--4158",
}

@inproceedings{zellers-etal-2019-hellaswag,
    title = "{H}ella{S}wag: Can a Machine Really Finish Your Sentence?",
    author = "Zellers, Rowan  and
      Holtzman, Ari  and
      Bisk, Yonatan  and
      Farhadi, Ali  and
      Choi, Yejin",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1472",
    doi = "10.18653/v1/P19-1472",
    pages = "4791--4800",
}


@article{hendryckstest2021,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}

@inproceedings{mihaylov-etal-2018-suit,
    title = "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
    author = "Mihaylov, Todor  and
      Clark, Peter  and
      Khot, Tushar  and
      Sabharwal, Ashish",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1260",
    doi = "10.18653/v1/D18-1260",
    pages = "2381--2391",
}

@misc{karpathy2024spikes,
	author = {Karpathy, Andrej},
	title = {{Cool! For the spike I'd try e.g. `-sl 7 -sg 7` to keep instability in check earlier in the training. (will skip update if loss/gradnorm > 7 sigma outlier is detected)}},
	howpublished = {X (formerly Twitter) \url{https://x.com/karpathy/status/1812917107379872145}},
	 day = {15},
    month = {July},
    year = {2024},
	note = {Accessed 2024-12-31},
}


@article{Bisk_Zellers_Le_bras_Gao_Choi_2020, title={{PIQA}: Reasoning about Physical Commonsense in Natural Language}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6239}, DOI={10.1609/aaai.v34i05.6239}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Bisk, Yonatan and Zellers, Rowan and Le bras, Ronan and Gao, Jianfeng and Choi, Yejin}, year={2020}, month={Apr.}, pages={7432-7439} }

@inproceedings{sap-etal-2019-social,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1454",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473",
}

@article{Sakaguchi_Le_Bras_Bhagavatula_Choi_2020, title={Wino{G}rande: An Adversarial Winograd Schema Challenge at Scale}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6399}, DOI={10.1609/aaai.v34i05.6399}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Sakaguchi, Keisuke and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin}, year={2020}, month={Apr.}, pages={8732-8740} }

@article{kwiatkowski-etal-2019-natural,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1026",
    doi = "10.1162/tacl_a_00276",
    pages = "452--466",
}

@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@inproceedings{dua-etal-2019-drop,
    title = "{DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
    author = "Dua, Dheeru  and
      Wang, Yizhong  and
      Dasigi, Pradeep  and
      Stanovsky, Gabriel  and
      Singh, Sameer  and
      Gardner, Matt",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1246",
    doi = "10.18653/v1/N19-1246",
    pages = "2368--2378",
}

@misc{mosaic-jeopardy,
  author = {MosaicML},
  year = {2024},
  title = {LLM Foundry - Jeopardy dataset},
  howpublished = {\url{https://github.com/mosaicml/llm-foundry/blob/main/scripts/eval/local_data/world_knowledge/jeopardy_all.jsonl}},
  note = {Accessed: 2024-11-10}
}

@article{reddy-etal-2019-coqa,
    title = "{C}o{QA}: A Conversational Question Answering Challenge",
    author = "Reddy, Siva  and
      Chen, Danqi  and
      Manning, Christopher D.",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1016",
    doi = "10.1162/tacl_a_00266",
    pages = "249--266",
}

@article{zhang2024automathtext,
      title={Autonomous Data Selection with Language Models for Mathematical Texts},
      author={Zhang, Yifan and Luo, Yifan and Yuan, Yang and Yao, Andrew Chi-Chih},
      journal={arXiv preprint arXiv:2402.07625},
      year={2024},
}

@article{husain2019codesearchnet,
  title={{CodeSearchNet} challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}

@article{yu2023metamath,
  title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}

@article{wang2023mathpile,
  title={Generative AI for Math: Part I -- MathPile: A Billion-Token-Scale Pretraining Corpus for Math},
  author={Wang, Zengzhi and Xia, Rui and Liu, Pengfei},
  journal={arXiv preprint arXiv:2312.17120},
  year={2023}
}

@inproceedings{
wang2024mathcoder,
title={MathCoder: Seamless Code Integration in {LLM}s for Enhanced Mathematical Reasoning},
author={Zimu Lu and Aojun Zhou and Zimu Lu and Sichun Luo and Weikang Shi and Renrui Zhang and Linqi Song and Mingjie Zhan and Hongsheng Li},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=z8TW0ttBPp}
}
@misc{lu2024mathcoder2bettermathreasoning,
      title={MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code},
      author={Zimu Lu and Aojun Zhou and Ke Wang and Houxing Ren and Weikang Shi and Junting Pan and Mingjie Zhan and Hongsheng Li},
      year={2024},
      eprint={2410.08196},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.08196},
}

@inproceedings{Tokpanov2024Zyda2A5,
  title={Zyda-2: a 5 Trillion Token High-Quality Dataset},
  author={Yury Tokpanov and Paolo Glorioso and Quentin Anthony and Beren Millidge},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:273963288}
}


@article{Shao2024ScalingRL,
  title={Scaling Retrieval-Based Language Models with a Trillion-Token Datastore},
  author={Rulin Shao and Jacqueline He and Akari Asai and Weijia Shi and Tim Dettmers and Sewon Min and Luke S. Zettlemoyer and Pang Wei Koh},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.12854},
  url={https://api.semanticscholar.org/CorpusID:271269939}
}

@misc{AMD-OLMo,
    title = {{AMD-OLMo}: A series of 1B language models trained from scratch by {AMD} on {AMD Instinct™ MI250} {GPUs}.},
    url = {https://huggingface.co/amd/AMD-OLMo},
    author = {Jiang Liu and Jialian Wu and Prakamya Mishra and Zicheng Liu and Sudhanshu Ranjan and Pratik Prabhanjan Brahma and Yusheng Su and Gowtham Ramesh and Peng Sun and Zhe Li and Dong Li and Lu Tian and Emad Barsoum},
    month = {October},
    year = {2024}
}

@article{Zhao2024DeconstructingWM,
  title={Deconstructing What Makes a Good Optimizer for Language Models},
  author={Rosie Zhao and Depen Morwani and David Brandfonbrener and Nikhil Vyas and Sham M. Kakade},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.07972},
  url={https://api.semanticscholar.org/CorpusID:271097803}
}
@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}
@inproceedings{Jin2024DemystifyingLM,
  title={Demystifying Language Model Forgetting with Low-rank Example Associations},
  author={Xisen Jin and Xiang Ren},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:270620654}
}
@article{feng2024maximize,
  title={Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase Pretraining},
  author={Feng, Steven and Prabhumoye, Shrimai and Kong, Kezhi and Su, Dan and Patwary, Mostofa and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2412.15285},
  year={2024}
}
@inproceedings{Shaib2024DetectionAM,
  title={Detection and Measurement of Syntactic Templates in Generated Text},
  author={Chantal Shaib and Yanai Elazar and Junyi Jessy Li and Byron C. Wallace},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:270869797}
}

@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}

@misc{liu2023tinygsmachieving80gsm8k,
      title={TinyGSM: achieving >80% on GSM8k with small language models},
      author={Bingbin Liu and Sebastien Bubeck and Ronen Eldan and Janardhan Kulkarni and Yuanzhi Li and Anh Nguyen and Rachel Ward and Yi Zhang},
      year={2023},
      eprint={2312.09241},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.09241},
}

@misc{akter2024mindmathinformedsynthetic,
      title={MIND: Math Informed syNthetic Dialogues for Pretraining LLMs},
      author={Syeda Nahida Akter and Shrimai Prabhumoye and John Kamalu and Sanjeev Satheesh and Eric Nyberg and Mostofa Patwary and Mohammad Shoeybi and Bryan Catanzaro},
      year={2024},
      eprint={2410.12881},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.12881},
}
@article{chang2024large,
  title={How Do Large Language Models Acquire Factual Knowledge During Pretraining?},
  author={Chang, Hoyeon and Park, Jinho and Ye, Seonghyeon and Yang, Sohee and Seo, Youngkyung and Chang, Du-Seong and Seo, Minjoon},
  journal={arXiv preprint arXiv:2406.11813},
  year={2024}
}

@article{zhang2024mapneo,
    title   = {MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series},
    author  = {Ge Zhang and Scott Qu and Jiaheng Liu and Chenchen Zhang and Chenghua Lin and Chou Leuang Yu and Danny Pan and Esther Cheng and Jie Liu and Qunshu Lin and Raven Yuan and Tuney Zheng and Wei Pang and Xinrun Du and Yiming Liang and Yinghao Ma and Yizhi Li and Ziyang Ma and Bill Lin and Emmanouil Benetos and Huan Yang and Junting Zhou and Kaijing Ma and Minghao Liu and Morry Niu and Noah Wang and Quehry Que and Ruibo Liu and Sine Liu and Shawn Guo and Soren Gao and Wangchunshu Zhou and Xinyue Zhang and Yizhi Zhou and Yubo Wang and Yuelin Bai and Yuhan Zhang and Yuxiang Zhang and Zenith Wang and Zhenzhu Yang and Zijian Zhao and Jiajun Zhang and Wanli Ouyang and Wenhao Huang and Wenhu Chen},
    year    = {2024},
    journal = {arXiv preprint arXiv: 2405.19327}
}

@misc{pandey2024gzippredictsdatadependentscaling,
      title={gzip Predicts Data-dependent Scaling Laws},
      author={Rohan Pandey},
      year={2024},
      eprint={2405.16684},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.16684},
}
@misc{jiang2022moreparameterfreetextclassification,
      title={Less is More: Parameter-Free Text Classification with Gzip},
      author={Zhiying Jiang and Matthew Y. R. Yang and Mikhail Tsirlin and Raphael Tang and Jimmy Lin},
      year={2022},
      eprint={2212.09410},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.09410},
}

@misc{dclm,
      title={DataComp-LM: In search of the next generation of training sets for language models},
      author={Jeffrey Li and Alex Fang and Georgios Smyrnis and Maor Ivgi and Matt Jordan and Samir Gadre and Hritik Bansal and Etash Guha and Sedrick Keh and Kushal Arora and Saurabh Garg and Rui Xin and Niklas Muennighoff and Reinhard Heckel and Jean Mercat and Mayee Chen and Suchin Gururangan and Mitchell Wortsman and Alon Albalak and Yonatan Bitton and Marianna Nezhurina and Amro Abbas and Cheng-Yu Hsieh and Dhruba Ghosh and Josh Gardner and Maciej Kilian and Hanlin Zhang and Rulin Shao and Sarah Pratt and Sunny Sanyal and Gabriel Ilharco and Giannis Daras and Kalyani Marathe and Aaron Gokaslan and Jieyu Zhang and Khyathi Chandu and Thao Nguyen and Igor Vasiljevic and Sham Kakade and Shuran Song and Sujay Sanghavi and Fartash Faghri and Sewoong Oh and Luke Zettlemoyer and Kyle Lo and Alaaeldin El-Nouby and Hadi Pouransari and Alexander Toshev and Stephanie Wang and Dirk Groeneveld and Luca Soldaini and Pang Wei Koh and Jenia Jitsev and Thomas Kollar and Alexandros G. Dimakis and Yair Carmon and Achal Dave and Ludwig Schmidt and Vaishaal Shankar},
      year={2024},
      eprint={2406.11794},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.11794},
}

@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{RMSNorm,
  title={Root Mean Square Layer Normalization},
  author={Biao Zhang and Rico Sennrich},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.07467},
  url={https://api.semanticscholar.org/CorpusID:113405151}
}

@article{Ba2016LayerNorm,
  title={Layer Normalization},
  author={Jimmy Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2016},
  volume={abs/1607.06450},
  url={https://api.semanticscholar.org/CorpusID:8236317}
}

@misc{yang2024spectral,
      title={A Spectral Condition for Feature Learning},
      author={Greg Yang and James B. Simon and Jeremy Bernstein},
      year={2024},
      eprint={2310.17813},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.17813},
}

@misc{cowsik2024geometric,
      title={Geometric Dynamics of Signal Propagation Predict Trainability of Transformers},
      author={Aditya Cowsik and Tamra Nebabu and Xiao-Liang Qi and Surya Ganguli},
      year={2024},
      eprint={2403.02579},
      archivePrefix={arXiv},
      primaryClass={cond-mat.dis-nn},
      url={https://arxiv.org/abs/2403.02579},
}

@misc{spikenomore,
      title={Spike No More: Stabilizing the Pre-training of Large Language Models},
      author={Sho Takase and Shun Kiyono and Sosuke Kobayashi and Jun Suzuki},
      year={2024},
      eprint={2312.16903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.16903},
}

@article{Su2021RoFormerET,
  title={RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author={Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.09864},
  url={https://api.semanticscholar.org/CorpusID:233307138}
}

@article{Hsieh2024RULERWT,
  title={RULER: What's the Real Context Size of Your Long-Context Language Models?},
  author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Boris Ginsburg},
  journal={COLM},
  year={2024},
  volume={abs/2404.06654},
  url={https://api.semanticscholar.org/CorpusID:269032933}
}

@inproceedings{
yen2025helmet,
title={{HELMET}: How to Evaluate Long-context Models Effectively and Thoroughly},
author={Howard Yen and Tianyu Gao and Minmin Hou and Ke Ding and Daniel Fleischer and Peter Izsak and Moshe Wasserblat and Danqi Chen},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=293V3bJbmE}
}

@misc{fang2025wrongperplexitylongcontextlanguage,
      title={What is Wrong with Perplexity for Long-context Language Modeling?},
      author={Lizhe Fang and Yifei Wang and Zhaoyang Liu and Chenheng Zhang and Stefanie Jegelka and Jinyang Gao and Bolin Ding and Yisen Wang},
      year={2025},
      eprint={2410.23771},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.23771},
}

@article{Shazeer2020GLUVI,
  title={GLU Variants Improve Transformer},
  author={Noam M. Shazeer},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.05202},
  url={https://api.semanticscholar.org/CorpusID:211096588}
}

@article{swintransformer,
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author={Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={9992-10002},
  url={https://api.semanticscholar.org/CorpusID:232352874}
}

@article{palm,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garc{\'i}a and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark D{\'i}az and Orhan Firat and Michele Catasta and Jason Wei and Kathleen S. Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.02311},
  url={https://api.semanticscholar.org/CorpusID:247951931}
}

@article{chameleon,
  title={Chameleon: Mixed-Modal Early-Fusion Foundation Models},
  author={{Chameleon Team}},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.09818},
  url={https://api.semanticscholar.org/CorpusID:269791516}
}
@article{lobacheva2023large,
  title={Large Learning Rates Improve Generalization: But How Large Are We Talking About?},
  author={Lobacheva, Ekaterina and Pockonechnyy, Eduard and Kodryan, Maxim and Vetrov, Dmitry},
  journal={arXiv preprint arXiv:2311.11303},
  year={2023}
}
@misc{cottier2024open,
  author       = {Ben Cottier and Josh You and Natalia Martemianova and David Owen},
  title        = {How Far Behind Are Open Models?},
  year         = 2024,
  month        = nov,
  url          = {https://epoch.ai/blog/open-models-report},
  note         = {Accessed: 2024-12-18}
}
@article{lewkowycz2020large,
  title={The large learning rate phase of deep learning: the catapult mechanism},
  author={Lewkowycz, Aitor and Bahri, Yasaman and Dyer, Ethan and Sohl-Dickstein, Jascha and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:2003.02218},
  year={2020}
}
@inproceedings{kim2025systematic,
  title={A systematic examination of preference learning through the lens of instruction-following},
  author={Kim, Joongwon and Goyal, Anirudh and Zhang, Aston and Xiong, Bo and Hou, Rui and Kambadur, Melanie and Mahajan, Dhruv and Hajishirzi, Hannaneh and Tan, Liang},
  booktitle={Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={11062--11082},
  year={2025}
}
@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}
@misc{mitch,
      title={Small-scale proxies for large-scale Transformer training instabilities},
      author={Mitchell Wortsman and Peter J. Liu and Lechao Xiao and Katie Everett and Alex Alemi and Ben Adlam and John D. Co-Reyes and Izzeddin Gur and Abhishek Kumar and Roman Novak and Jeffrey Pennington and Jascha Sohl-dickstein and Kelvin Xu and Jaehoon Lee and Justin Gilmer and Simon Kornblith},
      year={2023},
      eprint={2309.14322},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.14322},
}

@article{olmes,
  title={OLMES: A Standard for Language Model Evaluations},
  author={Yuling Gu and Oyvind Tafjord and Bailey Kuehl and Dany Haddad and Jesse Dodge and Hanna Hajishirzi},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.08446},
  url={https://api.semanticscholar.org/CorpusID:270391754}
}

@misc{openlm,
  author = {Gururangan, Suchin and Wortsman, Mitchell and Gadre, Samir Yitzhak and Dave, Achal and Kilian, Maciej and Shi, Weijia and Mercat, Jean and Smyrnis, Georgios and Ilharco, Gabriel and Jordan, Matt and Heckel, Reinhard and Dimakis, Alex and Farhadi, Ali and Shankar, Vaishaal and Schmidt, Ludwig},
  title = {{open\_lm}:  a minimal but performative language modeling (LM) repository},
  year = {2023},
  note = {GitHub repository},
  url = {https://github.com/mlfoundations/open_lm/}
}

@inproceedings{Zhang2019ImprovingDT,
  title={Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention},
  author={Biao Zhang and Ivan Titov and Rico Sennrich},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:201670412}
}

@inproceedings{Henry2020QueryKeyNF,
  title={Query-Key Normalization for Transformers},
  author={Alex Henry and Prudhvi Raj Dachapally and Shubham Vivek Pawar and Yuxuan Chen},
  booktitle={Findings},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:222272447}
}

@article{Dehghani2023ScalingVT,
  title={Scaling Vision Transformers to 22 Billion Parameters},
  author={Mostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Steiner and Mathilde Caron and Robert Geirhos and Ibrahim M. Alabdulmohsin and Rodolphe Jenatton and Lucas Beyer and Michael Tschannen and Anurag Arnab and Xiao Wang and Carlos Riquelme and Matthias Minderer and Joan Puigcerver and Utku Evci and Manoj Kumar and Sjoerd van Steenkiste and Gamaleldin F. Elsayed and Aravindh Mahendran and Fisher Yu and Avital Oliver and Fantine Huot and Jasmijn Bastings and Mark Collier and Alexey A. Gritsenko and Vighnesh Birodkar and Cristina Nader Vasconcelos and Yi Tay and Thomas Mensink and Alexander Kolesnikov and Filip Paveti'c and Dustin Tran and Thomas Kipf and Mario Luvci'c and Xiaohua Zhai and Daniel Keysers and Jeremiah Harmsen and Neil Houlsby},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.05442},
  url={https://api.semanticscholar.org/CorpusID:256808367}
}
@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}
@inproceedings{PyTorch2,
  author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, C. K. and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang, Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
  title = {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
  year = {2024},
  isbn = {9798400703850},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3620665.3640366},
  doi = {10.1145/3620665.3640366},
  abstract = {This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI's Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27\texttimes{} inference and 1.41\texttimes{} training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.},
  booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages = {929–947},
  numpages = {19},
  location = {La Jolla, CA, USA},
  series = {ASPLOS '24}
}

# Infrastructure #######################################

@article{wang2023rail,
  title={Rail-only: A Low-Cost High-Performance Network for Training LLMs with Trillion Parameters},
  author={Wang, Weiyang and Ghobadi, Manya and Shakeri, Kayvon and Zhang, Ying and Hasani, Naader},
  journal={arXiv preprint arXiv:2307.12169},
  year={2023}
}

########################################################


@misc{li2023starcoder,
      title={StarCoder: may the source be with you!},
      author={Raymond Li and Loubna Ben Allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia Li and Jenny Chim and others},
      year={2023},
      eprint={2305.06161},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{kocetkov2022stack3tbpermissively,
      title={The Stack: 3 TB of permissively licensed source code},
      author={Denis Kocetkov and Raymond Li and Loubna Ben Allal and Jia Li and Chenghao Mou and Carlos Muñoz Ferrandis and Yacine Jernite and Margaret Mitchell and Sean Hughes and Thomas Wolf and Dzmitry Bahdanau and Leandro von Werra and Harm de Vries},
      year={2022},
      eprint={2211.15533},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.15533},
}

@misc{peS2o,
    author = {Luca Soldaini and Kyle Lo},
    year = 2023,
    title = {{peS2o (Pretraining Efficiently on S2ORC) Dataset}},
    institution = {{Allen Institute for AI}},
    url = {https://github.com/allenai/pes2o}
}

@misc{soldaini2024dolma,
      title={Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research},
      author={Luca Soldaini and Rodney Kinney and Akshita Bhagia and Dustin Schwenk and David Atkinson and Russell Authur and Ben Bogin and Khyathi Chandu and Jennifer Dumas and Yanai Elazar and Valentin Hofmann and Ananya Harsh Jha and Sachin Kumar and Li Lucy and Xinxi Lyu and Nathan Lambert and Ian Magnusson and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Abhilasha Ravichander and Kyle Richardson and Zejiang Shen and Emma Strubell and Nishant Subramani and Oyvind Tafjord and Pete Walsh and Luke Zettlemoyer and Noah A. Smith and Hannaneh Hajishirzi and Iz Beltagy and Dirk Groeneveld and Jesse Dodge and Kyle Lo},
      year={2024},
      eprint={2402.00159},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{together2023redpajama,
  author={{Together AI}},
  title={{RedPajama}: An Open Source Recipe to Reproduce {LLaMA} training dataset},
  year={2023},
  url={https://github.com/togethercomputer/RedPajama-Data}
}

@misc{paster2023openwebmath,
      title={OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text},
      author={Keiran Paster and Marco Dos Santos and Zhangir Azerbayev and Jimmy Ba},
      year={2023},
      eprint={2310.06786},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{azerbayev2023llemma,
      title={Llemma: An Open Language Model For Mathematics},
      author={Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck},
      year={2023},
      eprint={2310.10631},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{swin2,
  title={Swin Transformer V2: Scaling Up Capacity and Resolution},
  author={Ze Liu and Han Hu and Yutong Lin and Zhuliang Yao and Zhenda Xie and Yixuan Wei and Jia Ning and Yue Cao and Zheng Zhang and Li Dong and Furu Wei and Baining Guo},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={11999-12009},
  url={https://api.semanticscholar.org/CorpusID:244346076}
}

@article{gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={{Gemma Team} and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{gemma2,
  title={Gemma 2: Improving open language models at a practical size},
  author={{Gemma Team} and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv e-prints},
  pages={arXiv--2408},
  year={2024}
}

@article{young2024yi,
  title={Yi: Open foundation models by 01. ai},
  author={Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and others},
  journal={arXiv preprint arXiv:2403.04652},
  year={2024}
}

@misc{grok_blog,
  author      = {X.AI},
  title       = {Announcing {Grok}},
  year        = {2023},
  month       = {11},
  day         = {3},
  url         = {https://x.ai/blog/grok},
  urldate     = {2024-11-20},
  organization = {X.AI},
  type        = {Blog post}
}

@misc{dbrx_blog,
 author      = {Databricks},
 title       = {Introducing {DBRX}: {A} {New} {State}-of-the-{Art} {Open} {LLM}},
 year        = {2024},
 month       = {3},
 day         = {27},
 url         = {https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm},
 urldate     = {2024-11-20},
 organization = {Databricks},
 type        = {Blog post}
}


@misc{openai2024midtraining,
 author      = {{OpenAI}},
 title       = {Introducing improvements to the fine-tuning {API} and expanding our custom models program},
 year        = {2024},
 month       = {4},
 day         = {4},
 url         = {https://openai.com/index/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program/},
 urldate     = {2024-04-04},
 organization = {{OpenAI}},
 type        = {Blog post}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}


@article{zamba2_model,
  title={The Zamba2 Suite: Technical Report},
  author={Glorioso, Paolo and Anthony, Quentin and Tokpanov, Yury and Golubeva, Anna and Shyam, Vasudev and Whittington, James and Pilault, Jonathan and Millidge, Beren},
  journal={arXiv preprint arXiv:2411.15242},
  year={2024}
}

@article{glorioso2024zamba,
  title={Zamba: A Compact 7B SSM Hybrid Model},
  author={Glorioso, Paolo and Anthony, Quentin and Tokpanov, Yury and Whittington, James and Pilault, Jonathan and Ibrahim, Adam and Millidge, Beren},
  journal={arXiv preprint arXiv:2405.16712},
  year={2024}
}

@article{liu2023llm360,
  title={Llm360: Towards fully transparent open-source llms},
  author={Liu, Zhengzhong and Qiao, Aurick and Neiswanger, Willie and Wang, Hongyi and Tan, Bowen and Tao, Tianhua and Li, Junbo and Wang, Yuqi and Sun, Suqi and Pangarkar, Omkar and others},
  journal={arXiv preprint arXiv:2312.06550},
  year={2023}
}
@article{liu2025llm360,
  title={LLM360 K2: Building a 65B 360-Open-Source Large Language Model from Scratch},
  author={Liu, Zhengzhong and Tan, Bowen and Wang, Hongyi and Neiswanger, Willie and Tao, Tianhua and Li, Haonan and Koto, Fajri and Wang, Yuqi and Sun, Suqi and Pangarkar, Omkar and others},
  journal={arXiv preprint arXiv:2501.07124},
  year={2025}
}
@misc{olmo_blog,
 author      = {{Ai2}},
 title       = {{OLMo-1.7 7B: A 24-point improvement on MMLU}},
 year        = {2024},
 month       = {4},
 day         = {17},
 url         = {https://allenai.org/blog/olmo-1-7-7b-a-24-point-improvement-on-mmlu-92b43f7d269d},
 urldate     = {2024-11-20},
 organization = {Allen Institute for Artificial Intelligence},
 type        = {Blog post}
}

@article{pile,
  author       = {Leo Gao and
                  Stella Biderman and
                  Sid Black and
                  Laurence Golding and
                  Travis Hoppe and
                  Charles Foster and
                  Jason Phang and
                  Horace He and
                  Anish Thite and
                  Noa Nabeshima and
                  Shawn Presser and
                  Connor Leahy},
  title        = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  journal      = {CoRR},
  volume       = {abs/2101.00027},
  year         = {2021},
  url          = {https://arxiv.org/abs/2101.00027},
  eprinttype    = {arXiv},
  eprint       = {2101.00027},
  timestamp    = {Thu, 14 Oct 2021 09:16:12 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2101-00027.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{modelsoups,
      title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
      author={Mitchell Wortsman and Gabriel Ilharco and Samir Yitzhak Gadre and Rebecca Roelofs and Raphael Gontijo-Lopes and Ari S. Morcos and Hongseok Namkoong and Ali Farhadi and Yair Carmon and Simon Kornblith and Ludwig Schmidt},
      year={2022},
      eprint={2203.05482},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.05482},
}


@inproceedings{lambert2024tulu3,
  title={Tulu 3: Pushing Frontiers in Open Language Model Post-Training},
  author={Nathan Lambert and Jacob Daniel Morrison and Valentina Pyatkin and Shengyi Huang and Hamish Ivison and Faeze Brahman and Lester James Validad Miranda and Alisa Liu and Nouha Dziri and Shane Lyu and Yuling Gu and Saumya Malik and Victoria Graf and Jena D. Hwang and Jiangjiang Yang and Ronan Le Bras and Oyvind Tafjord and Chris Wilhelm and Luca Soldaini and Noah A. Smith and Yizhong Wang and Pradeep Dasigi and Hanna Hajishirzi},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:274192505}
}

@article{teknium2024hermes,
  title={Hermes 3 technical report},
  author={Teknium, Ryan and Quesnelle, Jeffrey and Guang, Chen},
  journal={arXiv preprint arXiv:2408.11857},
  year={2024}
}

}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}
@misc{deepseekv2,
      title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
      author={DeepSeek-AI},
      year={2024},
      eprint={2405.04434},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{falcon40b,
  title={{Falcon-40B}: an open large language model with state-of-the-art performance},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year={2023}
}

@misc{allal2024SmolLM,
      title={SmolLM - blazingly fast and remarkably powerful},
      author={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch},
      year={2024},
      month={07}
}

@misc{allal2024SmolLM2,
      title={SmolLM2 - with great data, comes great performance},
      author={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Martín Blázquez and Lewis Tunstall and Agustín Piqueres and Andres Marafioti and Cyril Zakka and Leandro von Werra and Thomas Wolf},
      year={2024},
      month={11}
}
@misc{bakouch2025smollm3,
  title={{SmolLM3: smol, multilingual, long-context reasoner}},
  author={Bakouch, Elie and Ben Allal, Loubna and Lozhkov, Anton and Tazi, Nouamane and Tunstall, Lewis and Patiño, Carlos Miguel and Beeching, Edward and Roucher, Aymeric and Reedi, Aksel Joonas and Gallouédec, Quentin and Rasul, Kashif and Habib, Nathan and Fourrier, Clémentine and Kydlicek, Hynek and Penedo, Guilherme and Larcher, Hugo and Morlon, Mathieu and Srivastav, Vaibhav and Lochner, Joshua and Nguyen, Xuan-Son and Raffel, Colin and von Werra, Leandro and Wolf, Thomas},
  year={2025},
  howpublished={\url{https://huggingface.co/blog/smollm3}}
}


@article{merrick2024arctic,
  title={Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models},
  author={Merrick, Luke and Xu, Danmei and Nuti, Gaurav and Campos, Daniel},
  journal={arXiv preprint arXiv:2405.05374},
  year={2024}
}

@inproceedings{fan2019eli5,
  title={ELI5: Long Form Question Answering},
  author={Fan, Angela and Jernite, Yacine and Perez, Ethan and Grangier, David and Weston, Jason and Auli, Michael},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3558--3567},
  year={2019}
}


@inproceedings{penedo2024fineweb,
  title={{The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale}},
  author={Penedo, Guilherme and Kydl{\'\i}{\v{c}}ek, Hynek and Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin and Von Werra, Leandro and Wolf, Thomas and others},
  booktitle={{The Thirty-eight Conference on Neural Information Processing Systems; Datasets and Benchmarks Track}},
  year={2024}
}

@misc{qwen2.5,
      title={Qwen2.5 Technical Report},
      author={Qwen and : and An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      year={2024},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15115},
}



@misc{mistralnemo,
  author       = {{Mistral AI}},
  title        = {{Mistral introduces NeMO}},
  year         = {2024},
  url          = {https://mistral.ai/news/mistral-nemo/},
  note         = {Accessed: 2024-11-21}
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@misc{MosaicML2023Introducing,
    author    = {{MosaicML NLP Team}},
    title     = {Introducing MPT-30B: Raising the bar
for open-source foundation models},
    year      = {2023},
    url       = {www.mosaicml.com/blog/mpt-30b},
    note      = {Accessed: 2023-06-22},
    urldate   = {2023-06-22}
}
@article{team2024gemma,
  title={Gemma 2: Improving open language models at a practical size},
  author={{Gemma Team} and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@misc{scalingvisiontransformers22,
      title={Scaling Vision Transformers to 22 Billion Parameters},
      author={Mostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Steiner and Mathilde Caron and Robert Geirhos and Ibrahim Alabdulmohsin and Rodolphe Jenatton and Lucas Beyer and Michael Tschannen and Anurag Arnab and Xiao Wang and Carlos Riquelme and Matthias Minderer and Joan Puigcerver and Utku Evci and Manoj Kumar and Sjoerd van Steenkiste and Gamaleldin F. Elsayed and Aravindh Mahendran and Fisher Yu and Avital Oliver and Fantine Huot and Jasmijn Bastings and Mark Patrick Collier and Alexey Gritsenko and Vighnesh Birodkar and Cristina Vasconcelos and Yi Tay and Thomas Mensink and Alexander Kolesnikov and Filip Pavetić and Dustin Tran and Thomas Kipf and Mario Lučić and Xiaohua Zhai and Daniel Keysers and Jeremiah Harmsen and Neil Houlsby},
      year={2023},
      eprint={2302.05442},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2302.05442},
}

@article{olmo-co2,
    title = {Holistically Evaluating the Environmental Impact of Creating Language Models},
    author = {Jacob Morrison and Clara Na and Jared Fernandez and Tim Dettmers and Emma Strubell and Jesse Dodge},
  journal={Upcoming},
  year={2025}
}

@misc{li2023makingaithirstyuncovering,
      title={Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models},
      author={Pengfei Li and Jianyi Yang and Mohammad A. Islam and Shaolei Ren},
      year={2023},
      eprint={2304.03271},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.03271},
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{Abdin2024Phi4TR,
  title={Phi-4 technical report},
  author={Marah Abdin and Jyoti Aneja and Harkirat Singh Behl and S{\'e}bastien Bubeck and Ronen Eldan and Suriya Gunasekar and Michael Harrison and Russell J. Hewett and Mojan Javaheripi and Piero Kauffmann and James R. Lee and Yin Tat Lee and Yuanzhi Li and Weishung Liu and Caio C'esar Teodoro Mendes and Anh Nguyen and Eric Price and Gustavo de Rosa and Olli Saarikivi and Adil Salim and Shital Shah and Xin Wang and Rachel Ward and Yue Wu and Dingli Yu and Cyril Zhang and Yi Zhang},
  journal={arXiv preprint arXiv:2412.08905},
  year={2024}
}


@misc{NuExtract15,
  author       = {Numind},
  title        = {NuExtract-1.5},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/numind/NuExtract-1.5}},
  note         = {Accessed: 2024-11-24}
}


@inproceedings{gururangan2020dontStopPretraining,
  title={Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8342--8360},
  year={2020}
}

@misc{falcon2,
  author       = {{TII}},
  title        = {Meet Falcon 2: {TII} Releases New {AI} Model Series, Outperforming Meta’s New Llama 3},
  year         = {2024},
  howpublished = {\url{https://falconllm.tii.ae/falcon-2.html}},
  note         = {Accessed: 2024-12-17}
}

@misc{beaker2022,
  author       = {Michal Guerquin},
  title        = {Introducing {Ai2}’s Beaker},
  year         = {2022},
  howpublished = {Ai2 Blog, \url{https://web.archive.org/web/20241231204439/https://medium.com/ai2-blog/beaker-ed617d5f4593}},
  note         = {Accessed: 2024-12-31}
}


@misc{pytorch2024cuda,
  author       = {{PyTorch} team},
  title        = {{CUDA} semantics},
  year         = {2024},
  howpublished = {\url{https://web.archive.org/web/20241118063610/https://pytorch.org/docs/main/notes/cuda.html}},
  note         = {Accessed: 2024-11-18}
}


@misc{falcon3,
  author       = {{TII}},
  title        = {Falcon 3: Making Advanced {AI} Accessible and Available to Everyone, Everywhere},
  year         = {2024},
  howpublished = {\url{https://falconllm.tii.ae/falcon3/index.html}},
  note         = {Accessed: 2024-12-17}
}



%% unseen evals
@article{wang2024mmlu,
  title={Mmlu-pro: A more robust and challenging multi-task language understanding benchmark},
  author={Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and others},
  journal={arXiv preprint arXiv:2406.01574},
  year={2024}
}

@article{yue2025limit-of-rlvr,
  title={Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?},
  author={Yue, Yang and Chen, Zhiqi and Lu, Rui and Zhao, Andrew and Wang, Zhaokai and Yue, Yang and Song, Shiji and Huang, Gao},
  journal={arXiv preprint arXiv:2504.13837},
  year={2025}
}

@misc{cheng2025revisiting,
  title         = {Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective},
  author        = {Zhoujun Cheng and Shibo Hao and Tianyang Liu and Fan Zhou and Yutao Xie and Feng Yao and Yuexin Bian and Yonghao Zhuang and Nilabjo Dey and Yuheng Zha and Yi Gu and Kun Zhou and Yuqi Wang and Yuan Li and Richard Fan and Jianshu She and Chengqian Gao and Abulhair Saparov and Haonan Li and Taylor W. Killian and Mikhail Yurochkin and Zhengzhong Liu and Eric P. Xing and Zhiting Hu},
  journal       = {arXiv preprint arXiv:2506.14965},
  year          = {2025},
  doi           = {10.48550/arXiv.2506.14965},
  url           = {https://arxiv.org/abs/2506.14965}
}

@inproceedings{zhong-etal-2024-agieval,
    title = "{AGIE}val: A Human-Centric Benchmark for Evaluating Foundation Models",
    author = "Zhong, Wanjun  and
      Cui, Ruixiang  and
      Guo, Yiduo  and
      Liang, Yaobo  and
      Lu, Shuai  and
      Wang, Yanlin  and
      Saied, Amin  and
      Chen, Weizhu  and
      Duan, Nan",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.149",
    doi = "10.18653/v1/2024.findings-naacl.149",
    pages = "2299--2314",
}
@article{Sun2025OMEGACL,
  title={OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization},
  author={Yiyou Sun and Shawn Hu and Georgia Zhou and Ken Zheng and Hanna Hajishirzi and Nouha Dziri and Dawn Xiaodong Song},
  journal={ArXiv},
  year={2025},
  volume={abs/2506.18880},
  url={https://api.semanticscholar.org/CorpusID:280000246}
}

@misc{suzgun2022challengingbigbenchtaskschainofthought,
      title={Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
      author={Mirac Suzgun and Nathan Scales and Nathanael Schärli and Sebastian Gehrmann and Yi Tay and Hyung Won Chung and Aakanksha Chowdhery and Quoc V. Le and Ed H. Chi and Denny Zhou and Jason Wei},
      year={2022},
      eprint={2210.09261},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.09261},
}

@misc{aggarwal2025l1controllinglongreasoning,
  title={L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning},
  author={Pranjal Aggarwal and Sean Welleck},
  year={2025},
  eprint={2503.04697},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2503.04697},
}

@inproceedings{liu2025understanding,
  title={Understanding r1-zero-like training: A critical perspective},
  author={Liu, Zichen and Chen, Changyu and Li, Wenjun and Qi, Penghui and Pang, Tianyu and Du, Chao and Lee, Wee Sun and Lin, Min},
  booktitle={Conference on Language Modeling (COLM)},
  year={2025}
}
@book{rlhf2024,
  author = {Nathan Lambert},
  title = {Reinforcement Learning from Human Feedback},
  year = {2025},
  publisher = {Online},
  url = {https://rlhfbook.com}
}
@misc{yao2025offpolicy,
  title = {Your Efficient RL Framework Secretly Brings You Off-Policy RL Training},
  url = {https://fengyao.notion.site/off-policy-rl},
  author = {Yao, Feng and Liu, Liyuan and Zhang, Dinghuai and Dong, Chengyu and Shang, Jingbo and Gao, Jianfeng},
  journal = {Feng Yao's Notion},
  year = {2025},
  month = aug,
}

@inproceedings{joshi-etal-2017-triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1147",
    doi = "10.18653/v1/P17-1147",
    pages = "1601--1611",
}

@misc{cobbe2021trainingverifierssolvemath,
      title={Training Verifiers to Solve Math Word Problems},
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.14168},
}

@misc{blakeney2024doesdatasparkjoy,
      title={Does your data spark joy? Performance gains from domain upsampling at the end of training},
      author={Cody Blakeney and Mansheej Paul and Brett W. Larsen and Sean Owen and Jonathan Frankle},
      year={2024},
      eprint={2406.03476},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.03476},
}

@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903},
}

@misc{ibrahim2024simplescalablestrategiescontinually,
      title={Simple and Scalable Strategies to Continually Pre-train Large Language Models},
      author={Adam Ibrahim and Benjamin Thérien and Kshitij Gupta and Mats L. Richter and Quentin Anthony and Timothée Lesort and Eugene Belilovsky and Irina Rish},
      year={2024},
      eprint={2403.08763},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.08763},
}

@misc{wriestimatingwater,
      title={Guidance for Calculating Water Use Embedded in Purchased Electricity},
      author={Paul Reig and Tianyi Luo and Eric Christensen and Julie Sinistore},
      year={2020},
      archivePrefix={WRI},
      url={https://www.wri.org/research/guidance-calculating-water-use-embedded-purchased-electricity},
}

@misc{luccioni2022estimatingcarbonfootprintbloom,
      title={Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model},
      author={Alexandra Sasha Luccioni and Sylvain Viguier and Anne-Laure Ligozat},
      year={2022},
      eprint={2211.02001},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.02001},
}

@misc{dodge2022measuringcarbonintensityai,
      title={Measuring the Carbon Intensity of AI in Cloud Instances},
      author={Jesse Dodge and Taylor Prewitt and Remi Tachet Des Combes and Erika Odmark and Roy Schwartz and Emma Strubell and Alexandra Sasha Luccioni and Noah A. Smith and Nicole DeCario and Will Buchanan},
      year={2022},
      eprint={2206.05229},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.05229},
}

@misc{patterson2021carbonemissionslargeneural,
      title={Carbon Emissions and Large Neural Network Training},
      author={David Patterson and Joseph Gonzalez and Quoc Le and Chen Liang and Lluis-Miquel Munguia and Daniel Rothchild and David So and Maud Texier and Jeff Dean},
      year={2021},
      eprint={2104.10350},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2104.10350},
}


@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{mccandlish2018empirical,
  title={An empirical model of large-batch training},
  author={McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Team, OpenAI Dota},
  journal={arXiv preprint arXiv:1812.06162},
  year={2018}
}
@misc{bhagia2024establishingtaskscalinglaws,
      title={Establishing Task Scaling Laws via Compute-Efficient Model Ladders},
      author={Akshita Bhagia and Jiacheng Liu and Alexander Wettig and David Heineman and Oyvind Tafjord and Ananya Harsh Jha and Luca Soldaini and Noah A. Smith and Dirk Groeneveld and Pang Wei Koh and Jesse Dodge and Hannaneh Hajishirzi},
      year={2024},
      eprint={2412.04403},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.04403},
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@article{vyas2024soap,
  title={Soap: Improving and stabilizing shampoo using adam},
  author={Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham},
  journal={arXiv preprint arXiv:2409.11321},
  year={2024}
}
@article{land2024fishing,
  title={Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models},
  author={Land, Sander and Bartolo, Max},
  journal={arXiv preprint arXiv:2405.05417},
  year={2024}
}
@article{Antoniades2024GeneralizationVM,
  title={Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data},
  author={Antonis Antoniades and Xinyi Wang and Yanai Elazar and Alfonso Amayuelas and Alon Albalak and Kexun Zhang and William Yang Wang},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.14985},
  url={https://api.semanticscholar.org/CorpusID:271328219}
}

@misc{granite3,
  title={Granite 3.0 Language Models},
  url={https://github.com/ibm-granite/granite-3.0-language-models/},
  author={Granite Team, IBM},
  month={October},
  year={2024}
}

@misc{souleBergmann2025granite33,
  author       = {Soule, Kate and Bergmann, Dave},
  title        = {{IBM Granite 3.3: Speech recognition, refined reasoning, and RAG LoRAs}},
  year         = {2025},
  month        = apr,
  day          = {16},
  organization = {IBM},
  url          = {https://www.ibm.com/new/announcements/ibm-granite-3-3-speech-recognition-refined-reasoning-rag-loras},
  urldate      = {2025-12-12},
  note         = {Blog post}
}



@misc{drtulu,
      title={{DR Tulu}: Reinforcement Learning with Evolving Rubrics for Deep Research}, 
      author={Rulin Shao and Akari Asai and Shannon Zejiang Shen and Hamish Ivison and Varsha Kishore and Jingming Zhuo and Xinran Zhao and Molly Park and Samuel G. Finlayson and David Sontag and Tyler Murray and Sewon Min and Pradeep Dasigi and Luca Soldaini and Faeze Brahman and Wen-tau Yih and Tongshuang Wu and Luke Zettlemoyer and Yoon Kim and Hannaneh Hajishirzi and Pang Wei Koh},
      year={2025},
      eprint={2511.19399},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2511.19399}, 
}

@misc{vaswani2025rnj1,
  author       = {Vaswani, Ashish},
  title        = {Announcing Rnj-1: Building Instruments of Intelligence},
  year         = {2025},
  month        = dec,
  day          = {5},
  organization = {Essential AI Labs},
  url          = {https://essential.ai/research/rnj-1},
  urldate      = {2025-12-12},
  note         = {Blog post}
}

@misc{olmo20242olmo2furious,
      title={2 OLMo 2 Furious},
      author={Team OLMo and Pete Walsh and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Shane Arora and Akshita Bhagia and Yuling Gu and Shengyi Huang and Matt Jordan and Nathan Lambert and Dustin Schwenk and Oyvind Tafjord and Taira Anderson and David Atkinson and Faeze Brahman and Christopher Clark and Pradeep Dasigi and Nouha Dziri and Michal Guerquin and Hamish Ivison and Pang Wei Koh and Jiacheng Liu and Saumya Malik and William Merrill and Lester James V. Miranda and Jacob Morrison and Tyler Murray and Crystal Nam and Valentina Pyatkin and Aman Rangapur and Michael Schmitz and Sam Skjonsberg and David Wadden and Christopher Wilhelm and Michael Wilson and Luke Zettlemoyer and Ali Farhadi and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      eprint={2501.00656},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.00656},
}


@misc{swissai2025apertus,
  title={{Apertus: Democratizing Open and Compliant LLMs for Global Language Environments}},
  author={{Apertus Team}},
  year={2025},
  howpublished={\url{https://huggingface.co/swiss-ai/Apertus-70B-2509}}
}
@article{nano2025efficient,
  title={Efficient Hybrid Mamba-Transformer Reasoning Model},
  author={Nano, NVIDIA Nemotron},
  journal={arXiv preprint arXiv:2508.14444},
  year={2025}
}

@article{guha2025openthoughts,
  title   = {OpenThoughts: Data Recipes for Reasoning Models},
  author  = {Etash Guha and Ryan Marten and Sedrick Keh and Negin Raoof and Georgios Smyrnis and Hritik Bansal and Marianna Nezhurina and Jean Mercat and Trung Vu and Zayne Sprague and Ashima Suvarna and Benjamin Feuer and Liangyu Chen and Zaid Khan and Eric Frankel and Sachin Grover and Caroline Choi and Niklas Muennighoff and Shiye Su and Wanjia Zhao and John Yang and Shreyas Pimpalgaonkar and Kartik Sharma and Charlie Cheng-Jie Ji and Yichuan Deng and Sarah Pratt and Vivek Ramanujan and Jon Saad-Falcon and Jeffrey Li and Achal Dave and Alon Albalak and Kushal Arora and Blake Wulfe and Chinmay Hegde and Greg Durrett and Sewoong Oh and Mohit Bansal and Saadia Gabriel and Aditya Grover and Kai-Wei Chang and Vaishaal Shankar and Aaron Gokaslan and Mike A. Merrill and Tatsunori Hashimoto and Yejin Choi and Jenia Jitsev and Reinhard Heckel and Maheswaran Sathiamoorthy and Alexandros G. Dimakis and Ludwig Schmidt},
  journal = {arXiv preprint arXiv:2506.04178},
  year    = {2025},
  url     = {https://arxiv.org/abs/2506.04178}
}




@misc{nvidia2025nemotron_personas,
  title        = {Nemotron-Personas: An Open Dataset of Synthetic Personas},
  author       = {{NVIDIA AI}},
  howpublished = {\url{https://huggingface.co/datasets/nvidia/Nemotron-Personas}},
  year         = {2025},
  note         = {Dataset, CC BY 4.0}
}


@article{jiang2024wildteaming,
  title   = {WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models},
  author  = {Liwei Jiang and Kavel Rao and Seungju Han and Allyson Ettinger and Faeze Brahman and Sachin Kumar and Niloofar Mireshghallah and Ximing Lu and Maarten Sap and Yejin Choi and Nouha Dziri},
  journal = {arXiv preprint arXiv:2406.18510},
  year    = {2024},
  url     = {https://arxiv.org/abs/2406.18510}
}

@misc{primeintellect2025synthetic2,
  title        = {SYNTHETIC-2},
  author       = {{PrimeIntellect}},
  howpublished = {\url{https://huggingface.co/datasets/PrimeIntellect/SYNTHETIC-2}},
  year         = {2025},
  note         = {Dataset}
}

@article{singh2024aya,
  title   = {Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning},
  author  = {Shivalika Singh and Freddie Vargus and Daniel Dsouza and B{\"o}rje F. Karlsson and Abinaya Mahendiran and Wei-Yin Ko and Herumb Shandilya and Jay Patel and Deividas Mataciunas and Laura O'Mahony and Mike Zhang and Ramith Hettiarachchi and Joseph Wilson and Marina Machado and Luisa Souza Moura and Dominik Krzemi{\'n}ski and Hakimeh Fadaei and Irem Erg{\"u}n and Ifeoma Okoh and Aisha Alaagib and Oshan Mudannayake and Zaid Alyafeai and Vu Minh Chien and Sebastian Ruder and Surya Guthikonda and Emad A. Alghamdi and Sebastian Gehrmann and Niklas Muennighoff and Max Bartolo and Julia Kreutzer and Ahmet {\"U}st{\"u}n and Marzieh Fadaee and Sara Hooker},
  journal = {arXiv preprint arXiv:2402.06619},
  year    = {2024},
  url     = {https://arxiv.org/abs/2402.06619}
}

@article{zha2023tablegpt,
  title   = {TableGPT: Towards Unifying Tables, Natural Language and Commands into One GPT},
  author  = {Liangyu Zha and Junlin Zhou and Liyao Li and Rui Wang and Qingyi Huang and Saisai Yang and Jing Yuan and Changbao Su and Xiang Li and Aofeng Su and Tao Zhang and Chen Zhou and Kaizhe Shou and Miao Wang and Wufang Zhu and Guoshan Lu and Chao Ye and Yali Ye and Wentao Ye and Yiming Zhang and Xinglong Deng and Jie Xu and Haobo Wang and Gang Chen and Junbo Zhao},
  journal = {arXiv preprint arXiv:2307.08674},
  year    = {2023},
  url     = {https://arxiv.org/abs/2307.08674}
}

@misc{nvidia2025nemotron_post_training_dataset,
  title        = {Nemotron-Post-Training-Dataset-v1},
  author       = {{NVIDIA AI}},
  howpublished = {\url{https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1}},
  year         = {2025},
  note         = {Dataset}
}

@article{ahmad2025opencodereasoning,
  title   = {OpenCodeReasoning: Advancing Data Distillation for Competitive Coding},
  author  = {Wasi Uddin Ahmad and Sean Narenthiran and Somshubra Majumdar and Aleksander Ficek and Siddhartha Jain and Jocelyn Huang and Vahid Noroozi and Boris Ginsburg},
  journal = {arXiv preprint arXiv:2504.01943},
  year    = {2025},
  url     = {https://arxiv.org/abs/2504.01943}
}

@article{zeng2025acecoder,
  title   = {AceCoder: Acing Coder RL via Automated Test-Case Synthesis},
  author  = {Haoran Zeng and Jinyu Yang and Yizhe Zhang and Bowen Yu and Shuo Wang and Zhiyuan Liu and Maosong Sun and Tianyu Liu},
  journal = {arXiv preprint arXiv:2502.01718},
  year    = {2025},
  url     = {https://arxiv.org/abs/2502.01718}
}

@misc{thealgorithms_python,
  title        = {The Algorithms -- Python},
  author       = {{The Algorithms}},
  howpublished = {\url{https://github.com/TheAlgorithms/Python}},
  year         = {2025},
  note         = {GitHub repository, MIT License}
}
@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}
@misc{qwen_qwq_32b_2025,
  title        = {QwQ-32B: Embracing the Power of Reinforcement Learning},
  author       = {{Qwen Team}},
  year         = {2025},
  month        = mar,
  howpublished = {\url{https://qwenlm.github.io/blog/qwq-32b/}},
  note         = {Model release blog}
}

@misc{ding2024fewertruncationsimprovelanguage,
      title={Fewer Truncations Improve Language Modeling},
      author={Hantian Ding and Zijian Wang and Giovanni Paolini and Varun Kumar and Anoop Deoras and Dan Roth and Stefano Soatto},
      year={2024},
      eprint={2404.10830},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.10830},
}

@inproceedings{scalingllama3,
author = {Chu, Weiwei and Xie, Xinfeng and Yu, Jiecao and Wang, Jie and Phanishayee, Amar and Tang, Chunqiang and Hao, Yuchen and Huang, Jianyu and Ozdal, Mustafa and Wang, Jun and Goswami, Vedanuj and Goyal, Naman and Kadian, Abhishek and Gu, Andrew and Cai, Chris and Tian, Feng and Wang, Xiaodong and Si, Min and Balaji, Pavan and Chu, Ching-Hsiang and Park, Jongsoo},
title = {Scaling Llama 3 Training with Efficient Parallelism Strategies},
year = {2025},
isbn = {9798400712616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3695053.3731410},
doi = {10.1145/3695053.3731410},
abstract = {Llama is a widely used open-source large language model. This paper presents the design and implementation of the parallelism techniques used in Llama 3 pre-training. To achieve efficient training on tens of thousands of GPUs, Llama 3 employs a combination of four-dimensional parallelism: fully sharded data parallelism, tensor parallelism, pipeline parallelism, and context parallelism. Beyond achieving efficiency through parallelism and model co-design, we also address other equally critical aspects. First, we enhance flexibility—for example, through novel pipeline parallelism that supports evolving batch sizes and heterogeneous model architectures, and innovative context parallelism that enables model innovations such as document-mask attention. Second, we prioritize practicality—for example, by enabling the diagnosis of performance and numerical issues at scale. Finally, drawing on our experience with large-scale training, we provide recommendations for future hardware design.},
booktitle = {Proceedings of the 52nd Annual International Symposium on Computer Architecture},
pages = {1703–1716},
numpages = {14},
keywords = {Large Language Model, Training, Parallelism, Distributed System},
location = {
},
series = {ISCA '25}
}

@misc{peng2023yarnefficientcontextwindow,
      title={YaRN: Efficient Context Window Extension of Large Language Models},
      author={Bowen Peng and Jeffrey Quesnelle and Honglu Fan and Enrico Shippole},
      year={2023},
      eprint={2309.00071},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.00071},
}

@misc{chen2023extendingcontextwindowlarge,
      title={Extending Context Window of Large Language Models via Positional Interpolation},
      author={Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},
      year={2023},
      eprint={2306.15595},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.15595},
}

@misc{xiong2023effectivelongcontextscalingfoundation,
      title={Effective Long-Context Scaling of Foundation Models},
      author={Wenhan Xiong and Jingyu Liu and Igor Molybog and Hejia Zhang and Prajjwal Bhargava and Rui Hou and Louis Martin and Rashi Rungta and Karthik Abinav Sankararaman and Barlas Oguz and Madian Khabsa and Han Fang and Yashar Mehdad and Sharan Narang and Kshitiz Malik and Angela Fan and Shruti Bhosale and Sergey Edunov and Mike Lewis and Sinong Wang and Hao Ma},
      year={2023},
      eprint={2309.16039},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.16039},
}

@misc{rozière2024codellamaopenfoundation,
      title={Code Llama: Open Foundation Models for Code},
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2024},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.12950},
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@misc{zhao2023pytorchfsdpexperiencesscaling,
      title={PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel},
      author={Yanli Zhao and Andrew Gu and Rohan Varma and Liang Luo and Chien-Chin Huang and Min Xu and Less Wright and Hamid Shojanazeri and Myle Ott and Sam Shleifer and Alban Desmaison and Can Balioglu and Pritam Damania and Bernard Nguyen and Geeta Chauhan and Yuchen Hao and Ajit Mathews and Shen Li},
      year={2023},
      eprint={2304.11277},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2304.11277},
}

@article{skarlinski2024language,
  title={Language agents achieve superhuman synthesis of scientific knowledge},
  author={Skarlinski, Michael D and Cox, Sam and Laurent, Jon M and Braza, James D and Hinks, Michaela and Hammerling, Michael J and Ponnapati, Manvitha and Rodriques, Samuel G and White, Andrew D},
  journal={arXiv preprint arXiv:2409.13740},
  year={2024}
}


@article{gandhi2025cognitive,
  title={Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars},
  author={Gandhi, Kanishk and Chakravarthy, Ayush and Singh, Anikait and Lile, Nathan and Goodman, Noah D},
  journal={arXiv preprint arXiv:2503.01307},
  year={2025}
}


@misc{deepscaler2025, title={DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL}, author={Michael Luo and Sijun Tan and Justin Wong and Xiaoxiang Shi and William Y. Tang and Manan Roongta and Colin Cai and Jeffrey Luo and Li Erran Li and Raluca Ada Popa and Ion Stoica}, howpublished={\url{https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2}}, note={Notion Blog}, year={2025} }


@article{moshkov2025aimo2,
  title   = {{AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset}},
  author  = {Ivan Moshkov and Darragh Hanley and Ivan Sorokin and Shubham Toshniwal and Christof Henkel and Benedikt Schifferer and Wei Du and Igor Gitman},
  year    = {2025},
  journal = {arXiv preprint arXiv:2504.16891}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}
​

@article{tacoli,
  title={TACO: Topics in Algorithmic COde generation dataset},
  author={Rongao Li and Jie Fu and Bo-Wen Zhang and Tao Huang and Zhihong Sun and Chen Lyu and Guang Liu and Zhi Jin and Ge Li},
  journal={arXiv preprint arXiv:2312.14852},
  year={2023}
}

@article{hendrycksapps2021,
  title={Measuring Coding Challenge Competence With APPS},
  author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}



@misc{toy2024metacognitionneedusingintrospection,
      title={Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior},
      author={Jason Toy and Josh MacAdam and Phil Tabor},
      year={2024},
      eprint={2401.10910},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC},
      url={https://arxiv.org/abs/2401.10910},
}

@article{bfa322bf36e54a4ca19f9a73bee6184b,
title = "Rational use of cognitive resources in human planning",
abstract = "Making good decisions requires thinking ahead, but the huge number of actions and outcomes one could consider makes exhaustive planning infeasible for computationally constrained agents, such as humans. How people are nevertheless able to solve novel problems when their actions have long-reaching consequences is thus a long-standing question in cognitive science. To address this question, we propose a model of resource-constrained planning that allows us to derive optimal planning strategies. We find that previously proposed heuristics such as best-first search are near optimal under some circumstances but not others. In a mouse-tracking paradigm, we show that people adapt their planning strategies accordingly, planning in a manner that is broadly consistent with the optimal model but not with any single heuristic model. We also find systematic deviations from the optimal model that might result from additional cognitive constraints that are yet to be uncovered.",
author = "Frederick Callaway and \{van Opheusden\}, Bas and Sayan Gul and Priyam Das and Krueger, \{Paul M.\} and Griffiths, \{Thomas L.\} and Falk Lieder",
note = "Publisher Copyright: {\textcopyright} 2022, The Author(s), under exclusive licence to Springer Nature Limited.",
year = "2022",
month = aug,
doi = "10.1038/s41562-022-01332-8",
language = "English (US)",
volume = "6",
pages = "1112--1125",
journal = "Nature Human Behaviour",
issn = "2397-3374",
publisher = "Nature Publishing Group",
number = "8",
}

 @article{Fleming2017-FLESOD, author = {Stephen Fleming and Nathaniel Daw}, doi = {10.1037/rev0000045}, journal = {Psychological Review}, number = {1}, pages = {91--114}, title = {Self-Evaluation of Decision-Making: A General Bayesian Framework for Metacognitive Computation}, volume = {124}, year = {2017} }

@article{ACKERMAN2017607,
title = {Meta-Reasoning: Monitoring and Control of Thinking and Reasoning},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {8},
pages = {607-617},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317301055},
author = {Rakefet Ackerman and Valerie A. Thompson},
keywords = {reasoning, problem solving, metacognition, effort regulation, monitoring and control},
abstract = {Meta-Reasoning refers to the processes that monitor the progress of our reasoning and problem-solving activities and regulate the time and effort devoted to them. Monitoring processes are usually experienced as feelings of certainty or uncertainty about how well a process has, or will, unfold. These feelings are based on heuristic cues, which are not necessarily reliable. Nevertheless, we rely on these feelings of (un)certainty to regulate our mental effort. Most metacognitive research has focused on memorization and knowledge retrieval, with little attention paid to more complex processes, such as reasoning and problem solving. In that context, we recently developed a Meta-Reasoning framework, used here to review existing findings, consider their consequences, and frame questions for future research.}
}

@article{GRIFFITHS201924,
title = {Doing more with less: meta-reasoning and meta-learning in humans and machines},
journal = {Current Opinion in Behavioral Sciences},
volume = {29},
pages = {24-30},
year = {2019},
note = {Artificial Intelligence},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618302122},
author = {Thomas L Griffiths and Frederick Callaway and Michael B Chang and Erin Grant and Paul M Krueger and Falk Lieder},
abstract = {Artificial intelligence systems use an increasing amount of computation and data to solve very specific problems. By contrast, human minds solve a wide range of problems using a fixed amount of computation and limited experience. We identify two abilities that we see as crucial to this kind of general intelligence: meta-reasoning (deciding how to allocate computational resources) and meta-learning (modeling the learning environment to make better use of limited data). We summarize the relevant AI literature and relate the resulting ideas to recent work in psychology.}
}

@misc{bercovich2025llamanemotronefficientreasoningmodels,
      title={Llama-Nemotron: Efficient Reasoning Models},
      author={Akhiad Bercovich and Itay Levy and Izik Golan and Mohammad Dabbah and Ran El-Yaniv and Omri Puny and Ido Galil and Zach Moshe and Tomer Ronen and Najeeb Nabwani and Ido Shahaf and Oren Tropp and Ehud Karpas and Ran Zilberstein and Jiaqi Zeng and Soumye Singhal and Alexander Bukharin and Yian Zhang and Tugrul Konuk and Gerald Shen and Ameya Sunil Mahabaleshwarkar and Bilal Kartal and Yoshi Suhara and Olivier Delalleau and Zijia Chen and Zhilin Wang and David Mosallanezhad and Adi Renduchintala and Haifeng Qian and Dima Rekesh and Fei Jia and Somshubra Majumdar and Vahid Noroozi and Wasi Uddin Ahmad and Sean Narenthiran and Aleksander Ficek and Mehrzad Samadi and Jocelyn Huang and Siddhartha Jain and Igor Gitman and Ivan Moshkov and Wei Du and Shubham Toshniwal and George Armstrong and Branislav Kisacanin and Matvei Novikov and Daria Gitman and Evelina Bakhturina and Jane Polak Scowcroft and John Kamalu and Dan Su and Kezhi Kong and Markus Kliegl and Rabeeh Karimi and Ying Lin and Sanjeev Satheesh and Jupinder Parmar and Pritam Gundecha and Brandon Norick and Joseph Jennings and Shrimai Prabhumoye and Syeda Nahida Akter and Mostofa Patwary and Abhinav Khattar and Deepak Narayanan and Roger Waleffe and Jimmy Zhang and Bor-Yiing Su and Guyue Huang and Terry Kong and Parth Chadha and Sahil Jain and Christine Harvey and Elad Segal and Jining Huang and Sergey Kashirsky and Robert McQueen and Izzy Putterman and George Lam and Arun Venkatesan and Sherry Wu and Vinh Nguyen and Manoj Kilaru and Andrew Wang and Anna Warno and Abhilash Somasamudramath and Sandip Bhaskar and Maka Dong and Nave Assaf and Shahar Mor and Omer Ullman Argov and Scot Junkin and Oleksandr Romanenko and Pedro Larroy and Monika Katariya and Marco Rovinelli and Viji Balas and Nicholas Edelman and Anahita Bhiwandiwalla and Muthu Subramaniam and Smita Ithape and Karthik Ramamoorthy and Yuting Wu and Suguna Varshini Velury and Omri Almog and Joyjit Daw and Denys Fridman and Erick Galinkin and Michael Evans and Katherine Luna and Leon Derczynski and Nikki Pope and Eileen Long and Seth Schneider and Guillermo Siman and Tomasz Grzegorzek and Pablo Ribalta and Monika Katariya and Joey Conway and Trisha Saar and Ann Guan and Krzysztof Pawelec and Shyamala Prayaga and Oleksii Kuchaiev and Boris Ginsburg and Oluwatobi Olabiyi and Kari Briski and Jonathan Cohen and Bryan Catanzaro and Jonah Alben and Yonatan Geifman and Eric Chung and Chris Alexiuk},
      year={2025},
      eprint={2505.00949},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.00949},
}

@misc{guha2025openthoughtsdatarecipesreasoning,
  title={OpenThoughts: Data Recipes for Reasoning Models},
  author={Etash Guha and Ryan Marten and Sedrick Keh and Negin Raoof and Georgios Smyrnis and Hritik Bansal and Marianna Nezhurina and Jean Mercat and Trung Vu and Zayne Sprague and Ashima Suvarna and Benjamin Feuer and Liangyu Chen and Zaid Khan and Eric Frankel and Sachin Grover and Caroline Choi and Niklas Muennighoff and Shiye Su and Wanjia Zhao and John Yang and Shreyas Pimpalgaonkar and Kartik Sharma and Charlie Cheng-Jie Ji and Yichuan Deng and Sarah Pratt and Vivek Ramanujan and Jon Saad-Falcon and Jeffrey Li and Achal Dave and Alon Albalak and Kushal Arora and Blake Wulfe and Chinmay Hegde and Greg Durrett and Sewoong Oh and Mohit Bansal and Saadia Gabriel and Aditya Grover and Kai-Wei Chang and Vaishaal Shankar and Aaron Gokaslan and Mike A. Merrill and Tatsunori Hashimoto and Yejin Choi and Jenia Jitsev and Reinhard Heckel and Maheswaran Sathiamoorthy and Alexandros G. Dimakis and Ludwig Schmidt},
  year={2025},
  eprint={2506.04178},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2506.04178},
}

@article{horgan2018distributed,
  title={Distributed prioritized experience replay},
  author={Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and Van Hasselt, Hado and Silver, David},
  journal={arXiv preprint arXiv:1803.00933},
  year={2018}
}

@article{silver2017alphazero,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}

@inproceedings{hessel2021muesli,
  title={Muesli: Combining improvements in policy optimization},
  author={Hessel, Matteo and Danihelka, Ivo and Viola, Fabio and Guez, Arthur and Schmitt, Simon and Sifre, Laurent and Weber, Theophane and Silver, David and Van Hasselt, Hado},
  booktitle={International conference on machine learning},
  pages={4214--4226},
  year={2021},
  organization={PMLR}
}

@inproceedings{deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={3505--3506},
  year={2020}
}

@inproceedings{vllm,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@misc{muennighoff2025s1simpletesttimescaling,
      title={s1: Simple test-time scaling},
      author={Niklas Muennighoff and Zitong Yang and Weijia Shi and Xiang Lisa Li and Li Fei-Fei and Hannaneh Hajishirzi and Luke Zettlemoyer and Percy Liang and Emmanuel Candès and Tatsunori Hashimoto},
      year={2025},
      eprint={2501.19393},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.19393},
}

@article{pipelinerl,
  title={PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio},
  author={Pich{\'e}, Alexandre and Kamaloo, Ehsan and Pardinas, Rafael and Bahdanau, Dzmitry},
  journal={arXiv preprint arXiv:2509.19128},
  year={2025}
}

@book{sutton-and-barto,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G and others},
  volume={1},
  number={1},
  year={1998},
  publisher={MIT press Cambridge}
}

@article{deadly-triad,
  title={Deep reinforcement learning and the deadly triad},
  author={Van Hasselt, Hado and Doron, Yotam and Strub, Florian and Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph},
  journal={arXiv preprint arXiv:1812.02648},
  year={2018}
}

@article{Haupt2018,
  title        = {Hierarchical thinking: a cognitive tool for guiding coherent decision making in design problem solving},
  author       = {Haupt, Grietjie},
  journal      = {International Journal of Technology and Design Education},
  year         = {2018},
  volume       = {28},
  number       = {1},
  pages        = {207--237},
  doi          = {10.1007/s10798-016-9381-0},
  url          = {https://doi.org/10.1007/s10798-016-9381-0},
  issn         = {1573-1804},
  abstract     = {This paper builds on two concepts, the first of which is the extended information processing model of expert design cognition. This proposes twelve internal psychological characteristics interacting with the external world of expert designers during the early phases of the design process. Here, I explore one of the characteristics, hierarchical abstraction, and adapt it into an alternative ontological model of decision making. The model serves as an in-depth descriptor of how designers from different domains transform their mental states using judgment and decision making through hierarchical abstraction. The second concept entails an expansion of the idea of synergistic vertical transformation as a framework for mapping expert designers’ design process. Here, I focus on hierarchical decision making as multi-directional, and inter-relating the internal and external world of designers. In doing so, I provide a coding tool for researchers interested in exploring designers’ complex decision making processes. Concurrently, the model serves as decision making tool in design and technology education classrooms. As such, the paper focuses on the ontology of conceptual structures that support the early phases of the design process. This was based on empirical research.}
}


@inproceedings{Olieslagers2024,
  author       = {Olieslagers, J. and Bnaya, Z. and Li, Y. and Ma, W.},
  title        = {Backward reasoning through AND/OR trees to solve problems},
  booktitle    = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  year         = {2024},
  volume       = {46},
  url          = {https://escholarship.org/uc/item/9h4863xm},
  publisher    = {Cognitive Science Society},
  note         = {Retrieved from \url{https://escholarship.org/uc/item/9h4863xm}}
}

@article{Markovits2015,
  author       = {Markovits, Henry and Thompson, Valerie A. and Brisson, Janie},
  title        = {Metacognition and abstract reasoning},
  journal      = {Memory \& Cognition},
  year         = {2015},
  volume       = {43},
  number       = {4},
  pages        = {681--693},
  doi          = {10.3758/s13421-014-0488-9},
  url          = {https://doi.org/10.3758/s13421-014-0488-9},
  issn         = {1532-5946},
}


%%% Benchmark Contamination %%%%
@article{Magar2022DataCF,
  title={Data Contamination: From Memorization to Exploitation},
  author={Inbal Magar and Roy Schwartz},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.08242},
  url={https://api.semanticscholar.org/CorpusID:247475929}
}

@article{Bordt2024HowMC,
  title={How much can we forget about Data Contamination?},
  author={Sebastian Bordt and Suraj Srinivas and Valentyn Boreiko and Ulrike von Luxburg},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.03249},
  url={https://api.semanticscholar.org/CorpusID:273163321}
}

@misc{Hall2025marin,
	author = {Hall, David and Chou, Christopher and Garg, Abhinav and Ravi, Nikil and Liu, Nelson and Shandilya, Herumb and Ahmed, Ahmed and Liang, Percy and Kuditipudi, Rohith and {J38} and Lee, Tony and Power, Russell and Salahi, Kamyar and Held, William and Wang, Jason and {chiheem} and Niklaus, Joel and Mai, Yifan and {dependabot[bot]} and Zhou, Ivan and Li, Kevin Xiang and Yang, Sherry and Karamcheti, Sidd and Williams, Ryan and Zhou, Cathy and Ramaswami, Ashwin and {whenwen} and Kotha, Suhas and Miguel, Gary and Xu, Calvin},
	year = {2025},
	month = {nov 14},
	title = {marin-community/marin},
	url = {https://github.com/marin-community/marin},
	howpublished = {https://github.com/marin-community/marin},
}


@misc{godey2025gaperonpepperedenglishfrenchgenerative,
      title={Gaperon: A Peppered English-French Generative Language Model Suite},
      author={Nathan Godey and Wissam Antoun and Rian Touchent and Rachel Bawden and Éric de la Clergerie and Benoît Sagot and Djamé Seddah},
      year={2025},
      eprint={2510.25771},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2510.25771},
}


%%% eval %%%%
@misc{heineman2025signalnoiseframeworkreducing,
  title = {Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation},
  author = {David Heineman and Valentin Hofmann and Ian Magnusson and Yuling Gu and Noah A. Smith and Hannaneh Hajishirzi and Kyle Lo and Jesse Dodge},
  year = 2025,
  url = {https://arxiv.org/abs/2508.13144},
  eprint = {2508.13144},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL}
}
@misc{magnusson2025datadecidepredictbestpretraining,
  title = {DataDecide: How to Predict Best Pretraining Data with Small Experiments},
  author = {Ian Magnusson and Nguyen Tai and Ben Bogin and David Heineman and Jena D. Hwang and Luca Soldaini and Akshita Bhagia and Jiacheng Liu and Dirk Groeneveld and Oyvind Tafjord and Noah A. Smith and Pang Wei Koh and Jesse Dodge},
  year = 2025,
  url = {https://arxiv.org/abs/2504.11393},
  eprint = {2504.11393},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
@article{he2025nondeterminism,
  title = {Defeating Nondeterminism in LLM Inference},
  author = {Horace He and Thinking Machines Lab},
  year = 2025,
  journal = {Thinking Machines Lab: Connectionism},
  doi = {10.64434/tml.20250910},
  note = {https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/}
}
@article{su2025expanding,
  title = {Expanding RL with Verifiable Rewards Across Diverse Domains},
  author = {Su, Yi and Yu, Dian and Song, Linfeng and Li, Juntao and Mi, Haitao and Tu, Zhaopeng and Zhang, Min and Yu, Dong},
  year = 2025,
  journal = {arXiv preprint arXiv:2503.23829}
}
@inproceedings{pmlr-v174-pal22a,
  title = {MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering},
  author = {Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  year = 2022,
  month = {07--08 Apr},
  booktitle = {Proceedings of the Conference on Health, Inference, and Learning},
  publisher = {PMLR},
  series = {Proceedings of Machine Learning Research},
  volume = 174,
  pages = {248--260},
  url = {https://proceedings.mlr.press/v174/pal22a.html},
  editor = {Flores, Gerardo and Chen, George H and Pollard, Tom and Ho, Joyce C and Naumann, Tristan},
  pdf = {https://proceedings.mlr.press/v174/pal22a/pal22a.pdf},
  abstract = {This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS &amp; NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects &amp; topics. A detailed explanation of the solution, along with the above information, is provided in this study.}
}
@article{li2024crowdsourced,
  title={From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline},
  author={Li, Tianle and Chiang, Wei-Lin and Frick, Evan and Dunlap, Lisa and Wu, Tianhao and Zhu, Banghua and Gonzalez, Joseph E and Stoica, Ion},
  journal={arXiv preprint arXiv:2406.11939},
  year={2024}
}
@article{jin2021disease,
  title = {What disease does this patient have? a large-scale open domain question answering dataset from medical exams},
  author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  year = 2021,
  journal = {Applied Sciences},
  publisher = {MDPI},
  volume = 11,
  number = 14,
  pages = 6421
}
@inproceedings{SciQ,
  title = {Crowdsourcing Multiple Choice Science Questions},
  author = {Johannes Welbl, Nelson F. Liu, Matt Gardner},
  year = 2017,
  journal = {arXiv:1707.06209v1}
}
@article{paperno2016lambada,
  title = {The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author = {Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  year = 2016,
  journal = {arXiv preprint arXiv:1606.06031}
}
@misc{gsm-symbolic,
  title = {GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models},
  author = {Iman Mirzadeh and Keivan Alizadeh and Hooman Shahrokhi and Oncel Tuzel and Samy Bengio and Mehrdad Farajtabar},
  year = 2024,
  url = {https://arxiv.org/abs/2410.05229}
}
@article{lewkowycz2022solving,
  title = {Solving quantitative reasoning problems with language models},
  author = {Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  year = 2022,
  journal = {Advances in neural information processing systems},
  volume = 35,
  pages = {3843--3857}
}
@article{lightman2023lets,
  title = {Let's Verify Step by Step},
  author = {Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  year = 2023,
  journal = {arXiv preprint arXiv:2305.20050}
}
@inproceedings{welbl-etal-2017-crowdsourcing,
  title = "Crowdsourcing Multiple Choice Science Questions",
  author = "Welbl, Johannes  and
      Liu, Nelson F.  and
      Gardner, Matt",
  year = 2017,
  month = sep,
  booktitle = "Proceedings of the 3rd Workshop on Noisy User-generated Text",
  publisher = "Association for Computational Linguistics",
  address = "Copenhagen, Denmark",
  pages = "94--106",
  doi = "10.18653/v1/W17-4413",
  url = "https://aclanthology.org/W17-4413/",
  editor = "Derczynski, Leon  and
      Xu, Wei  and
      Ritter, Alan  and
      Baldwin, Tim"
}
@article{chen2021codex,
  title = {Evaluating Large Language Models Trained on Code},
  author = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  year = 2021,
  eprint = {2107.03374},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG}
}
@article{cobbe2021gsm8k,
  title = {Training Verifiers to Solve Math Word Problems},
  author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  year = 2021,
  journal = {arXiv preprint arXiv:2110.14168}
}
@article{cassano2022multipl,
  title = {Multipl-e: A scalable and extensible approach to benchmarking neural code generation},
  author = {Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and others},
  year = 2022,
  journal = {arXiv preprint arXiv:2208.08227}
}
@article{zhuo2024bigcodebench,
  title = {BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions},
  author = {Zhuo, Terry Yue and Vu, Minh Chien and Chim, Jenny and Hu, Han and Yu, Wenhao and Widyasari, Ratnadira and Yusuf, Imam Nur Bani and Zhan, Haolan and He, Junda and Paul, Indraneil and others},
  year = 2024,
  journal = {arXiv preprint arXiv:2406.15877}
}
@article{Lai2022DS1000,
  title = {DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation},
  author = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-Tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  year = 2022,
  journal = {ArXiv},
  volume = {abs/2211.11501}
}
@article{guo2024deepseek,
  title = {DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence},
  author = {Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Yu and Li, YK and others},
  year = 2024,
  journal = {arXiv preprint arXiv:2401.14196}
}
@inproceedings{matton-etal-2024-leakage,
  title = "On Leakage of Code Generation Evaluation Datasets",
  author = "Matton, Alexandre  and
      Sherborne, Tom  and
      Aumiller, Dennis  and
      Tommasone, Elena  and
      Alizadeh, Milad  and
      He, Jingyi  and
      Ma, Raymond  and
      Voisin, Maxime  and
      Gilsenan-McMahon, Ellen  and
      Gall{\'e}, Matthias",
  year = 2024,
  month = nov,
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
  publisher = "Association for Computational Linguistics",
  address = "Miami, Florida, USA",
  pages = "13215--13223",
  doi = "10.18653/v1/2024.findings-emnlp.772",
  url = "https://aclanthology.org/2024.findings-emnlp.772/",
  editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"
}
@article{saxton2019analysing,
  title = {Analysing mathematical reasoning abilities of neural models},
  author = {Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
  year = 2019,
  journal = {arXiv preprint arXiv:1904.01557}
}
@article{suzgun2022challenging,
  title = {Challenging big-bench tasks and whether chain-of-thought can solve them},
  author = {Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  year = 2022,
  journal = {arXiv preprint arXiv:2210.09261}
}
@article{bavarian2022efficient,
  title = {Efficient training of language models to fill in the middle},
  author = {Bavarian, Mohammad and Jun, Heewoo and Tezak, Nikolas and Schulman, John and McLeavey, Christine and Tworek, Jerry and Chen, Mark},
  year = 2022,
  journal = {arXiv preprint arXiv:2207.14255}
}
@inproceedings{evalplus,
  title = {Is Your Code Generated by Chat{GPT} Really Correct? Rigorous Evaluation of Large Language Models for Code Generation},
  author = {Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  year = 2023,
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  url = {https://openreview.net/forum?id=1qvx610Cu7}
}
@article{jain2024livecodebench,
  title = {LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code},
  author = {Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
  year = 2024,
  journal = {arXiv preprint arXiv:2403.07974}
}
@article{lin2025zebralogic,
  title = {Zebralogic: On the scaling limits of llms for logical reasoning},
  author = {Lin, Bill Yuchen and Bras, Ronan Le and Richardson, Kyle and Sabharwal, Ashish and Poovendran, Radha and Clark, Peter and Choi, Yejin},
  year = 2025,
  journal = {arXiv preprint arXiv:2502.01100}
}
@article{zhong2023agieval,
  title = {Agieval: A human-centric benchmark for evaluating foundation models},
  author = {Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  year = 2023,
  journal = {arXiv preprint arXiv:2304.06364}
}
@inproceedings{rein2024gpqa,
  title = {{GPQA}: A Graduate-Level Google-Proof Q\&A Benchmark},
  author = {David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
  year = 2024,
  booktitle = {First Conference on Language Modeling},
  url = {https://openreview.net/forum?id=Ti67584b98}
}
@article{mallen2023llm_memorization,
  title = {When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories},
  author = {Mallen, Alex and Asai,Akari and  Zhong, Victor and Das, Rajarshi and Hajishirzi, Hannaneh and Khashabi, Daniel},
  year = {2022},
  journal = {arXiv preprint}
}
@article{wei2024measuring,
  title = {Measuring short-form factuality in large language models},
  author = {Wei, Jason and Karina, Nguyen and Chung, Hyung Won and Jiao, Yunxin Joy and Papay, Spencer and Glaese, Amelia and Schulman, John and Fedus, William},
  year = 2024,
  journal = {arXiv preprint arXiv:2411.04368}
}
@article{dubois2024length,
  title = {Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators},
  author = {Dubois, Yann and Galambosi, Bal{\'a}zs and Liang, Percy and Hashimoto, Tatsunori B},
  year = 2024,
  journal = {arXiv preprint arXiv:2404.04475}
}
@misc{alpaca_eval,
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
  year = 2023,
  month = 5,
  journal = {GitHub repository},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}
@inproceedings{patil2025bfcl,
  title = {The Berkeley Function Calling Leaderboard (BFCL): From Tool Use to Agentic Evaluation of Large Language Models},
  author = {Patil, Shishir G. and Mao, Huanzhi and Cheng-Jie Ji, Charlie and Yan, Fanjia and Suresh, Vishnu and Stoica, Ion and E. Gonzalez, Joseph},
  year = 2025,
  booktitle = {Forty-second International Conference on Machine Learning}
}
@article{huang2024compression,
  title = {Compression represents intelligence linearly},
  author = {Huang, Yuzhen and Zhang, Jinghan and Shan, Zifei and He, Junxian},
  year = 2024,
  journal = {arXiv preprint arXiv:2404.09937}
}
@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}
@article{ward1963hierarchical,
  title={Hierarchical grouping to optimize an objective function},
  author={Ward Jr, Joe H},
  journal={Journal of the American statistical association},
  volume={58},
  number={301},
  pages={236--244},
  year={1963},
  publisher={Taylor \& Francis}
}
@article{gu2024cruxeval,
  title={Cruxeval: A benchmark for code reasoning, understanding and execution},
  author={Gu, Alex and Rozi{\`e}re, Baptiste and Leather, Hugh and Solar-Lezama, Armando and Synnaeve, Gabriel and Wang, Sida I},
  journal={arXiv preprint arXiv:2401.03065},
  year={2024}
}
@article{schaeffer2023emergent,
  title={Are emergent abilities of large language models a mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={55565--55581},
  year={2023}
}
@article{vendrow2025large,
  title={Do large language model benchmarks test reliability?},
  author={Vendrow, Joshua and Vendrow, Edward and Beery, Sara and Madry, Aleksander},
  journal={arXiv preprint arXiv:2502.03461},
  year={2025}
}
@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}
@article{austin2021program,
  title={Program Synthesis with Large Language Models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}
@article{qwen3,
  title={Qwen3 technical report},
  author={Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and others},
  journal={arXiv preprint arXiv:2505.09388},
  year={2025}
}
@article{dasigi2021dataset,
  title={A dataset of information-seeking questions and answers anchored in research papers},
  author={Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A and Gardner, Matt},
  journal={arXiv preprint arXiv:2105.03011},
  year={2021}
}
@article{laurent2024lab,
  title={Lab-bench: Measuring capabilities of language models for biology research},
  author={Laurent, Jon M and Janizek, Joseph D and Ruzo, Michael and Hinks, Michaela M and Hammerling, Michael J and Narayanan, Siddharth and Ponnapati, Manvitha and White, Andrew D and Rodriques, Samuel G},
  journal={arXiv preprint arXiv:2407.10362},
  year={2024}
}
@article{ruan2024observational,
  title={Observational scaling laws and the predictability of langauge model performance},
  author={Ruan, Yangjun and Maddison, Chris J and Hashimoto, Tatsunori B},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={15841--15892},
  year={2024}
}

# LC


@misc{ainslie2023gqatraininggeneralizedmultiquery,
      title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints}, 
      author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebrón and Sumit Sanghai},
      year={2023},
      eprint={2305.13245},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13245}, 
}


@MISC{goddard2025extendingAFM,
  title        = "Extending {AFM}-4.{5B} to {64K} Context Length",
  author       = "Goddard, Charles",
  booktitle    = "Arcee AI",
  abstract     = "From 4k to 64k context through aggressive experimentation,
                  model merging, distillation, and a concerning amount of soup.",
  month        =  jun,
  year         =  2025,
  howpublished = "\url{https://www.arcee.ai/blog/extending-afm-4-5b-to-64k-context-length}",
  note         = "Accessed: 2025-11-10",
  language     = "en"
}


@misc{glm45,
      title={{GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models}},
      author={{GLM-4.5 Team} and Aohan Zeng and Xin Lv and Qinkai Zheng and Zhenyu Hou and Bin Chen and Chengxing Xie and Cunxiang Wang and Da Yin and Hao Zeng and Jiajie Zhang and Kedong Wang and Lucen Zhong and Mingdao Liu and Rui Lu and Shulin Cao and Xiaohan Zhang and Xuancheng Huang and Yao Wei and Yean Cheng and Yifan An and Yilin Niu and Yuanhao Wen and Yushi Bai and Zhengxiao Du and Zihan Wang and Zilin Zhu and Bohan Zhang and Bosi Wen and Bowen Wu and Bowen Xu and Can Huang and Casey Zhao and Changpeng Cai and Chao Yu and Chen Li and Chendi Ge and Chenghua Huang and Chenhui Zhang and Chenxi Xu and Chenzheng Zhu and Chuang Li and Congfeng Yin and Daoyan Lin and Dayong Yang and Dazhi Jiang and Ding Ai and Erle Zhu and Fei Wang and Gengzheng Pan and Guo Wang and Hailong Sun and Haitao Li and Haiyang Li and Haiyi Hu and Hanyu Zhang and Hao Peng and Hao Tai and Haoke Zhang and Haoran Wang and Haoyu Yang and He Liu and He Zhao and Hongwei Liu and Hongxi Yan and Huan Liu and Huilong Chen and Ji Li and Jiajing Zhao and Jiamin Ren and Jian Jiao and Jiani Zhao and Jianyang Yan and Jiaqi Wang and Jiayi Gui and Jiayue Zhao and Jie Liu and Jijie Li and Jing Li and Jing Lu and Jingsen Wang and Jingwei Yuan and Jingxuan Li and Jingzhao Du and Jinhua Du and Jinxin Liu and Junkai Zhi and Junli Gao and Ke Wang and Lekang Yang and Liang Xu and Lin Fan and Lindong Wu and Lintao Ding and Lu Wang and Man Zhang and Minghao Li and Minghuan Xu and Mingming Zhao and Mingshu Zhai and Pengfan Du and Qian Dong and Shangde Lei and Shangqing Tu and Shangtong Yang and Shaoyou Lu and Shijie Li and Shuang Li and Shuang-Li and Shuxun Yang and Sibo Yi and Tianshu Yu and Wei Tian and Weihan Wang and Wenbo Yu and Weng Lam Tam and Wenjie Liang and Wentao Liu and Xiao Wang and Xiaohan Jia and Xiaotao Gu and Xiaoying Ling and Xin Wang and Xing Fan and Xingru Pan and Xinyuan Zhang and Xinze Zhang and Xiuqing Fu and Xunkai Zhang and Yabo Xu and Yandong Wu and Yida Lu and Yidong Wang and Yilin Zhou and Yiming Pan and Ying Zhang and Yingli Wang and Yingru Li and Yinpei Su and Yipeng Geng and Yitong Zhu and Yongkun Yang and Yuhang Li and Yuhao Wu and Yujiang Li and Yunan Liu and Yunqing Wang and Yuntao Li and Yuxuan Zhang and Zezhen Liu and Zhen Yang and Zhengda Zhou and Zhongpei Qiao and Zhuoer Feng and Zhuorui Liu and Zichen Zhang and Zihan Wang and Zijun Yao and Zikang Wang and Ziqiang Liu and Ziwei Chai and Zixuan Li and Zuodong Zhao and Wenguang Chen and Jidong Zhai and Bin Xu and Minlie Huang and Hongning Wang and Juanzi Li and Yuxiao Dong and Jie Tang},
      year={2025},
      eprint={2508.06471},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2508.06471},
}


@MISC{deepseekV31,
  title        = "{DeepSeek}-{V3.1} Release",
  author       = {{DeepSeek-AI}},
  booktitle    = "DeepSeek",
  abstract     = "Introducing DeepSeek-V3.1: our first step toward the agent
                  era!",
  howpublished = "\url{https://api-docs.deepseek.com/news/news250821}",
  note         = "Accessed: 2025-11-10",
  language     = "en",
  year = "2025"
}


@misc{hsieh2024rulerwhatsrealcontext,
      title="RULER: What's the Real Context Size of Your Long-Context Language Models?",
      author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Yang Zhang and Boris Ginsburg},
      year={2024},
      eprint={2404.06654},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.06654},
}



@misc{kimiK2,
      title={Kimi K2: Open Agentic Intelligence},
      author={{Kimi Team} and Yifan Bai and Yiping Bao and Guanduo Chen and Jiahao Chen and Ningxin Chen and Ruijue Chen and Yanru Chen and Yuankun Chen and Yutian Chen and Zhuofu Chen and Jialei Cui and Hao Ding and Mengnan Dong and Angang Du and Chenzhuang Du and Dikang Du and Yulun Du and Yu Fan and Yichen Feng and Kelin Fu and Bofei Gao and Hongcheng Gao and Peizhong Gao and Tong Gao and Xinran Gu and Longyu Guan and Haiqing Guo and Jianhang Guo and Hao Hu and Xiaoru Hao and Tianhong He and Weiran He and Wenyang He and Chao Hong and Yangyang Hu and Zhenxing Hu and Weixiao Huang and Zhiqi Huang and Zihao Huang and Tao Jiang and Zhejun Jiang and Xinyi Jin and Yongsheng Kang and Guokun Lai and Cheng Li and Fang Li and Haoyang Li and Ming Li and Wentao Li and Yanhao Li and Yiwei Li and Zhaowei Li and Zheming Li and Hongzhan Lin and Xiaohan Lin and Zongyu Lin and Chengyin Liu and Chenyu Liu and Hongzhang Liu and Jingyuan Liu and Junqi Liu and Liang Liu and Shaowei Liu and T. Y. Liu and Tianwei Liu and Weizhou Liu and Yangyang Liu and Yibo Liu and Yiping Liu and Yue Liu and Zhengying Liu and Enzhe Lu and Lijun Lu and Shengling Ma and Xinyu Ma and Yingwei Ma and Shaoguang Mao and Jie Mei and Xin Men and Yibo Miao and Siyuan Pan and Yebo Peng and Ruoyu Qin and Bowen Qu and Zeyu Shang and Lidong Shi and Shengyuan Shi and Feifan Song and Jianlin Su and Zhengyuan Su and Xinjie Sun and Flood Sung and Heyi Tang and Jiawen Tao and Qifeng Teng and Chensi Wang and Dinglu Wang and Feng Wang and Haiming Wang and Jianzhou Wang and Jiaxing Wang and Jinhong Wang and Shengjie Wang and Shuyi Wang and Yao Wang and Yejie Wang and Yiqin Wang and Yuxin Wang and Yuzhi Wang and Zhaoji Wang and Zhengtao Wang and Zhexu Wang and Chu Wei and Qianqian Wei and Wenhao Wu and Xingzhe Wu and Yuxin Wu and Chenjun Xiao and Xiaotong Xie and Weimin Xiong and Boyu Xu and Jing Xu and Jinjing Xu and L. H. Xu and Lin Xu and Suting Xu and Weixin Xu and Xinran Xu and Yangchuan Xu and Ziyao Xu and Junjie Yan and Yuzi Yan and Xiaofei Yang and Ying Yang and Zhen Yang and Zhilin Yang and Zonghan Yang and Haotian Yao and Xingcheng Yao and Wenjie Ye and Zhuorui Ye and Bohong Yin and Longhui Yu and Enming Yuan and Hongbang Yuan and Mengjie Yuan and Haobing Zhan and Dehao Zhang and Hao Zhang and Wanlu Zhang and Xiaobin Zhang and Yangkun Zhang and Yizhi Zhang and Yongting Zhang and Yu Zhang and Yutao Zhang and Yutong Zhang and Zheng Zhang and Haotian Zhao and Yikai Zhao and Huabin Zheng and Shaojie Zheng and Jianren Zhou and Xinyu Zhou and Zaida Zhou and Zhen Zhu and Weiyu Zhuang and Xinxing Zu},
      year={2025},
      eprint={2507.20534},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2507.20534},
}


@misc{nvidia2025nvidianemotronnano2,
      title={{NVIDIA Nemotron Nano 2}: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model},
      author={NVIDIA and and Aarti Basant and Abhijit Khairnar and Abhijit Paithankar and Abhinav Khattar and Adithya Renduchintala and Aditya Malte and Akhiad Bercovich and Akshay Hazare and Alejandra Rico and Aleksander Ficek and Alex Kondratenko and Alex Shaposhnikov and Alexander Bukharin and Ali Taghibakhshi and Amelia Barton and Ameya Sunil Mahabaleshwarkar and Amy Shen and Andrew Tao and Ann Guan and Anna Shors and Anubhav Mandarwal and Arham Mehta and Arun Venkatesan and Ashton Sharabiani and Ashwath Aithal and Ashwin Poojary and Ayush Dattagupta and Balaram Buddharaju and Banghua Zhu and Barnaby Simkin and Bilal Kartal and Bita Darvish Rouhani and Bobby Chen and Boris Ginsburg and Brandon Norick and Brian Yu and Bryan Catanzaro and Charles Wang and Charlie Truong and Chetan Mungekar and Chintan Patel and Chris Alexiuk and Christian Munley and Christopher Parisien and Dan Su and Daniel Afrimi and Daniel Korzekwa and Daniel Rohrer and Daria Gitman and David Mosallanezhad and Deepak Narayanan and Dima Rekesh and Dina Yared and Dmytro Pykhtar and Dong Ahn and Duncan Riach and Eileen Long and Elliott Ning and Eric Chung and Erick Galinkin and Evelina Bakhturina and Gargi Prasad and Gerald Shen and Haifeng Qian and Haim Elisha and Harsh Sharma and Hayley Ross and Helen Ngo and Herman Sahota and Hexin Wang and Hoo Chang Shin and Hua Huang and Iain Cunningham and Igor Gitman and Ivan Moshkov and Jaehun Jung and Jan Kautz and Jane Polak Scowcroft and Jared Casper and Jian Zhang and Jiaqi Zeng and Jimmy Zhang and Jinze Xue and Jocelyn Huang and Joey Conway and John Kamalu and Jonathan Cohen and Joseph Jennings and Julien Veron Vialard and Junkeun Yi and Jupinder Parmar and Kari Briski and Katherine Cheung and Katherine Luna and Keith Wyss and Keshav Santhanam and Kezhi Kong and Krzysztof Pawelec and Kumar Anik and Kunlun Li and Kushan Ahmadian and Lawrence McAfee and Laya Sleiman and Leon Derczynski and Luis Vega and Maer Rodrigues de Melo and Makesh Narsimhan Sreedhar and Marcin Chochowski and Mark Cai and Markus Kliegl and Marta Stepniewska-Dziubinska and Matvei Novikov and Mehrzad Samadi and Meredith Price and Meriem Boubdir and Michael Boone and Michael Evans and Michal Bien and Michal Zawalski and Miguel Martinez and Mike Chrzanowski and Mohammad Shoeybi and Mostofa Patwary and Namit Dhameja and Nave Assaf and Negar Habibi and Nidhi Bhatia and Nikki Pope and Nima Tajbakhsh and Nirmal Kumar Juluru and Oleg Rybakov and Oleksii Hrinchuk and Oleksii Kuchaiev and Oluwatobi Olabiyi and Pablo Ribalta and Padmavathy Subramanian and Parth Chadha and Pavlo Molchanov and Peter Dykas and Peter Jin and Piotr Bialecki and Piotr Januszewski and Pradeep Thalasta and Prashant Gaikwad and Prasoon Varshney and Pritam Gundecha and Przemek Tredak and Rabeeh Karimi Mahabadi and Rajen Patel and Ran El-Yaniv and Ranjit Rajan and Ria Cheruvu and Rima Shahbazyan and Ritika Borkar and Ritu Gala and Roger Waleffe and Ruoxi Zhang and Russell J. Hewett and Ryan Prenger and Sahil Jain and Samuel Kriman and Sanjeev Satheesh and Saori Kaji and Sarah Yurick and Saurav Muralidharan and Sean Narenthiran and Seonmyeong Bak and Sepehr Sameni and Seungju Han and Shanmugam Ramasamy and Shaona Ghosh and Sharath Turuvekere Sreenivas and Shelby Thomas and Shizhe Diao and Shreya Gopal and Shrimai Prabhumoye and Shubham Toshniwal and Shuoyang Ding and Siddharth Singh and Siddhartha Jain and Somshubra Majumdar and Soumye Singhal and Stefania Alborghetti and Syeda Nahida Akter and Terry Kong and Tim Moon and Tomasz Hliwiak and Tomer Asida and Tony Wang and Tugrul Konuk and Twinkle Vashishth and Tyler Poon and Udi Karpas and Vahid Noroozi and Venkat Srinivasan and Vijay Korthikanti and Vikram Fugro and Vineeth Kalluru and Vitaly Kurin and Vitaly Lavrukhin and Wasi Uddin Ahmad and Wei Du and Wonmin Byeon and Ximing Lu and Xin Dong and Yashaswi Karnati and Yejin Choi and Yian Zhang and Ying Lin and Yonggan Fu and Yoshi Suhara and Zhen Dong and Zhiyu Li and Zhongbo Zhu and Zijia Chen},
      year={2025},
      eprint={2508.14444},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2508.14444},
}

@inproceedings{zhao2024interdoc,
   title={Analysing The Impact of Sequence Composition on Language Model Pre-Training},
   url={http://dx.doi.org/10.18653/v1/2024.acl-long.427},
   DOI={10.18653/v1/2024.acl-long.427},
   booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
   publisher={Association for Computational Linguistics},
   author={Zhao, Yu and Qu, Yuanbin and Staniszewski, Konrad and Tworkowski, Szymon and Liu, Wei and Miłoś, Piotr and Wu, Yuxiang and Minervini, Pasquale},
   year={2024},
   pages={7897–7912} }



@misc{deepseekv3,
      title={DeepSeek-V3 Technical Report},
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},
      year={2025},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437},
}

@misc{wu2025longattn,
      title={{LongAttn}: Selecting Long-context Training Data via Token-level Attention},
      author={Longyun Wu and Dawei Zhu and Guangxiang Zhao and Zhuocheng Yu and Junfeng Ran and Xiangyu Wong and Lin Sun and Sujian Li},
      year={2025},
      eprint={2502.16860},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.16860},
}


@article{liu2025prorl,
  author    = {Mingjie Liu and Shizhe Diao and Ximing Lu and Jian Hu and Xin Dong and Yejin Choi and Jan Kautz and Yi Dong},
  title={ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models},
  journal   = {arXiv preprint},
  year      = {2025},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url={https://arxiv.org/abs/2505.24864},
}
@article{wu2025reasoning,
  title={Reasoning or memorization? unreliable results of reinforcement learning due to data contamination},
  author={Wu, Mingqi and Zhang, Zhihao and Dong, Qiaole and Xi, Zhiheng and Zhao, Jun and Jin, Senjie and Fan, Xiaoran and Zhou, Yuhao and Lv, Huijie and Zhang, Ming and others},
  journal={arXiv preprint arXiv:2507.10532},
  year={2025}
}
@misc{shao2025spuriousrewardsrethinkingtraining,
      title={Spurious Rewards: Rethinking Training Signals in RLVR},
      author={Rulin Shao and Shuyue Stella Li and Rui Xin and Scott Geng and Yiping Wang and Sewoong Oh and Simon Shaolei Du and Nathan Lambert and Sewon Min and Ranjay Krishna and Yulia Tsvetkov and Hannaneh Hajishirzi and Pang Wei Koh and Luke Zettlemoyer},
      year={2025},
      eprint={2506.10947},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.10947},
}

@inproceedings{prolong,
  title={How to Train Long-Context Language Models (Effectively)},
  author={Gao, Tianyu and Wettig, Alexander and Yen, Howard and Chen, Danqi},
  booktitle={ACL},
  year={2025}
}


@article{zeng2025rlve,
  title={RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments},
  author={Zeng, Zhiyuan and Ivison, Hamish and Wang, Yiping and Yuan, Lifan and Li, Shuyue Stella and Ye, Zhuorui and Li, Siting and He, Jacqueline and Zhou, Runlong and Chen, Tong and Zhao, Chenyang and Tsvetkov, Yulia and Du, Simon Shaolei and Jaques, Natasha and Peng, Hao and Koh, Pang Wei and Hajishirzi, Hannaneh},
  journal={arXiv preprint 2511.07317},
  year={2025}
}

@article{poznanski2025olmocr,
  title={{olmOCR: Unlocking trillions of tokens in pdfs with vision language models}},
  author={Poznanski, Jake and Rangapur, Aman and Borchardt, Jon and Dunkelberger, Jason and Huff, Regan and Lin, Daniel and Wilhelm, Christopher and Lo, Kyle and Soldaini, Luca},
  journal={arXiv preprint arXiv:2502.18443},
  year={2025}
}


@misc{poznanski2025olmocr2unittest,
      title={{olmOCR 2: Unit Test Rewards for Document OCR}},
      author={Jake Poznanski and Luca Soldaini and Kyle Lo},
      year={2025},
      eprint={2510.19817},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2510.19817},
}

@misc{weborganizer,
      title={Organize the Web: Constructing Domains Enhances Pre-Training Data Curation},
      author={Alexander Wettig and Kyle Lo and Sewon Min and Hannaneh Hajishirzi and Danqi Chen and Luca Soldaini},
      year={2025},
      eprint={2502.10341},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.10341},
}

@article{Joyce2009,
  author  = {Joyce, James M.},
  title   = {Causal Reasoning and Backtracking},
  journal = {Philosophical Studies},
  year    = {2009},
  volume  = {147},
  number  = {1},
  pages   = {139--154},
  doi     = {10.1007/s11098-009-9454-y},
  publisher = {Springer}
}
@article{fujii2025rewriting,
  title={Rewriting pre-training data boosts llm performance in math and code},
  author={Fujii, Kazuki and Tajima, Yukito and Mizuki, Sakae and Shimada, Hinari and Shiotani, Taihei and Saito, Koshiro and Ohi, Masanari and Kawamura, Masaki and Nakamura, Taishi and Okamoto, Takumi and others},
  journal={arXiv preprint arXiv:2505.02881},
  year={2025}
}

@article{wang2025octothinker,
  title={Octothinker: Mid-training incentivizes reinforcement learning scaling},
  author={Wang, Zengzhi and Zhou, Fan and Li, Xuefeng and Liu, Pengfei},
  journal={arXiv preprint arXiv:2506.20512},
  year={2025}
}

@article{zhou2025megamath,
  title={Megamath: Pushing the limits of open math corpora},
  author={Zhou, Fan and Wang, Zengzhi and Ranjan, Nikhil and Cheng, Zhoujun and Tang, Liping and He, Guowei and Liu, Zhengzhong and Xing, Eric P},
  journal={arXiv preprint arXiv:2504.02807},
  year={2025}
}

@article{lozhkov2024starcoder,
  title={Starcoder 2 and the stack v2: The next generation},
  author={Lozhkov, Anton and Li, Raymond and Allal, Loubna Ben and Cassano, Federico and Lamy-Poirier, Joel and Tazi, Nouamane and Tang, Ao and Pykhtar, Dmytro and Liu, Jiawei and Wei, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.19173},
  year={2024}
}

@article{kargupta2025cognitive,
  title={Cognitive Foundations for Reasoning and Their Manifestation in LLMs},
  author={Kargupta, Priyanka and Li, Shuyue Stella and Wang, Haocheng and Lee, Jinu and Chen, Shan and Ahia, Orevaoghene and Light, Dean and Griffiths, Thomas L. and Kleiman-Weiner, Max and Han, Jiawei and Celikyilmaz, Asli and Tsvetkov, Yulia},
  journal={arXiv},
  year={2025}
}

@misc{CommonCrawl,
  title = {{Common Crawl Dataset}},
  author = {{Common Crawl Foundation}},
  howpublished = {\url{https://commoncrawl.org/}},
  note = {Accessed: December 31, 2024}
}

@misc{team2025gemma3,
    title={Gemma 3 Technical Report},
    author={{Gemma 3 Team}},
    year={2025},
    eprint={2503.19786},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2503.19786}
}


@misc{mimo,
      title={{MiMo}: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining},
      author={LLM-Core Xiaomi and : and Bingquan Xia and Bowen Shen and Cici and Dawei Zhu and Di Zhang and Gang Wang and Hailin Zhang and Huaqiu Liu and Jiebao Xiao and Jinhao Dong and Liang Zhao and Peidian Li and Peng Wang and Shihua Yu and Shimao Chen and Weikun Wang and Wenhan Ma and Xiangwei Deng and Yi Huang and Yifan Song and Zihan Jiang and Bowen Ye and Can Cai and Chenhong He and Dong Zhang and Duo Zhang and Guoan Wang and Hao Tian and Haochen Zhao and Heng Qu and Hongshen Xu and Jun Shi and Kainan Bao and Kai Fang and Kang Zhou and Kangyang Zhou and Lei Li and Menghang Zhu and Nuo Chen and Qiantong Wang and Shaohui Liu and Shicheng Li and Shuhao Gu and Shuhuai Ren and Shuo Liu and Sirui Deng and Weiji Zhuang and Weiwei Lv and Wenyu Yang and Xin Zhang and Xing Yong and Xing Zhang and Xingchen Song and Xinzhe Xu and Xu Wang and Yihan Yan and Yu Tu and Yuanyuan Tian and Yudong Wang and Yue Yu and Zhenru Lin and Zhichao Song and Zihao Yue},
      year={2025},
      eprint={2505.07608},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.07608},
}

@misc{pham2025clipper,
      title={CLIPPER: Compression enables long-context synthetic data generation},
      author={Chau Minh Pham and Yapei Chang and Mohit Iyyer},
      year={2025},
      eprint={2502.14854},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.14854},
}


@misc{qwen1M,
      title={{Qwen2.5-1M Technical Report}},
      author={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},
      year={2025},
      eprint={2501.15383},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.15383},
}


@inproceedings{longbench1,
    title = "{L}ong{B}ench: A Bilingual, Multitask Benchmark for Long Context Understanding",
    author = "Bai, Yushi  and
      Lv, Xin  and
      Zhang, Jiajie  and
      Lyu, Hongchang  and
      Tang, Jiankai  and
      Huang, Zhidian  and
      Du, Zhengxiao  and
      Liu, Xiao  and
      Zeng, Aohan  and
      Hou, Lei  and
      Dong, Yuxiao  and
      Tang, Jie  and
      Li, Juanzi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.172/",
    doi = "10.18653/v1/2024.acl-long.172",
    pages = "3119--3137",
    abstract = "Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs' long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability."
}

@inproceedings{longbench2,
    title = "{L}ong{B}ench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks",
    author = "Bai, Yushi  and
      Tu, Shangqing  and
      Zhang, Jiajie  and
      Peng, Hao  and
      Wang, Xiaozhi  and
      Lv, Xin  and
      Cao, Shulin  and
      Xu, Jiazheng  and
      Hou, Lei  and
      Dong, Yuxiao  and
      Tang, Jie  and
      Li, Juanzi",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.183/",
    doi = "10.18653/v1/2025.acl-long.183",
    pages = "3639--3664",
    ISBN = "979-8-89176-251-0",
    abstract = "This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7{\%} accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1{\%} accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7{\%}, surpassing the human baseline by 4{\%}. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2."
}

@misc{su2025nemotroncctransformingcommoncrawl,
      title={Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset},
      author={Dan Su and Kezhi Kong and Ying Lin and Joseph Jennings and Brandon Norick and Markus Kliegl and Mostofa Patwary and Mohammad Shoeybi and Bryan Catanzaro},
      year={2025},
      eprint={2412.02595},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.02595},
}

@misc{nelson2024needlehaystackmemorybased,
      title={Needle in the Haystack for Memory Based Large Language Models},
      author={Elliot Nelson and Georgios Kollias and Payel Das and Subhajit Chaudhury and Soham Dan},
      year={2024},
      eprint={2407.01437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01437},
}

@misc{allal2025smollm2smolgoesbig,
      title={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model},
      author={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Martín Blázquez and Guilherme Penedo and Lewis Tunstall and Andrés Marafioti and Hynek Kydlíček and Agustín Piqueres Lajarín and Vaibhav Srivastav and Joshua Lochner and Caleb Fahlgren and Xuan-Son Nguyen and Clémentine Fourrier and Ben Burtenshaw and Hugo Larcher and Haojun Zhao and Cyril Zakka and Mathieu Morlon and Colin Raffel and Leandro von Werra and Thomas Wolf},
      year={2025},
      eprint={2502.02737},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.02737},
}

@article{liu2024regmix,
  title={Regmix: Data mixture as regression for language model pre-training},
  author={Liu, Qian and Zheng, Xiaosen and Muennighoff, Niklas and Zeng, Guangtao and Dou, Longxu and Pang, Tianyu and Jiang, Jing and Lin, Min},
  journal={arXiv preprint arXiv:2407.01492},
  year={2024}
}

@misc{ye2025datamixinglawsoptimizing,
      title={Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance},
      author={Jiasheng Ye and Peiju Liu and Tianxiang Sun and Jun Zhan and Yunhua Zhou and Xipeng Qiu},
      year={2025},
      eprint={2403.16952},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.16952},
}
@misc{diao2025climbclusteringbasediterativedata,
      title={CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training},
      author={Shizhe Diao and Yu Yang and Yonggan Fu and Xin Dong and Dan Su and Markus Kliegl and Zijia Chen and Peter Belcak and Yoshi Suhara and Hongxu Yin and Mostofa Patwary and Yingyan and Lin and Jan Kautz and Pavlo Molchanov},
      year={2025},
      eprint={2504.13161},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.13161},
}

@software{NemotronPostTrainingDatasetV1,
      author = {Nathawani, Dhruv and Gitman, Igor and Majumdar, Somshubra and Bakhturina, Evelina and Sunil Mahabaleshwarkar, Ameya and and Zhang, Jian and Polak Scowcroft, Jane},
      title = {{Nemotron-Post-Training-Dataset-v1}},
      version = {1.0},
      publisher = {{NVIDIA}},
      year = {2025}, month = July,
      url = {https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1}
}


@inproceedings{wu-etal-2025-webwalker,
    title = "{W}eb{W}alker: Benchmarking {LLM}s in Web Traversal",
    author = "Wu, Jialong  and
      Yin, Wenbiao  and
      Jiang, Yong  and
      Wang, Zhenglin  and
      Xi, Zekun  and
      Fang, Runnan  and
      Zhang, Linhai  and
      He, Yulan  and
      Zhou, Deyu  and
      Xie, Pengjun  and
      Huang, Fei",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.508/",
    doi = "10.18653/v1/2025.acl-long.508",
    pages = "10290--10305",
    ISBN = "979-8-89176-251-0",
    abstract = "Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address this, we introduce WebWalkerQA, a benchmark designed to assess the ability of LLMs to perform web traversal. It evaluates the capacity of LLMs to traverse a website{'}s subpages to extract high-quality data systematically. We propose WebWalker, which is a multi-agent framework that mimics human-like web navigation through an explore-critic paradigm. Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through this horizontal and vertical integration in real-world scenarios."
}

@inproceedings{Yang2018HotpotQAAD,
  title={HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering},
  author={Zhilin Yang and Peng Qi and Saizheng Zhang and Yoshua Bengio and William W. Cohen and Ruslan Salakhutdinov and Christopher D. Manning},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:52822214}
}

@article{Shi2025TaskCraftAG,
  title={TaskCraft: Automated Generation of Agentic Tasks},
  author={Dingfeng Shi and Jingyi Cao and Qianben Chen and Weichen Sun and Weizhen Li and Hongxuan Lu and Fangchen Dong and Tianrui Qin and King Zhu and Minghao Liu and Jian Yang and Ge Zhang and Jiaheng Liu and Changwang Zhang and Jun Wang and Yuchen Eleanor Jiang and Wangchunshu Zhou},
  journal={ArXiv},
  year={2025},
  volume={abs/2506.10055},
  url={https://api.semanticscholar.org/CorpusID:279318561}
}

@article{Asai2024OpenScholarSS,
  title={OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs},
  author={Akari Asai and Jacqueline He and Rulin Shao and Weijia Shi and Amanpreet Singh and Joseph Chee Chang and Kyle Lo and Luca Soldaini and Sergey Feldman and Mike D'Arcy and David Wadden and Matt Latzke and Minyang Tian and Pan Ji and Shengyan Liu and Hao Tong and Bohao Wu and Yanyu Xiong and Luke S. Zettlemoyer and Graham Neubig and Dan Weld and Doug Downey and Wen-tau Yih and Pang Wei Koh and Hanna Hajishirzi},
  journal={ArXiv},
  year={2024},
  volume={abs/2411.14199},
  url={https://api.semanticscholar.org/CorpusID:274166189}
}

@article{Miroyan2025SearchAA,
  title={Search Arena: Analyzing Search-Augmented LLMs},
  author={Mihran Miroyan and Tsung-Han Wu and Logan King and Tianle Li and Jiayi Pan and Xinyan Hu and Wei-Lin Chiang and Anastasios Nikolas Angelopoulos and Trevor Darrell and Narges Norouzi and Joseph Gonzalez},
  journal={ArXiv},
  year={2025},
  volume={abs/2506.05334},
  url={https://api.semanticscholar.org/CorpusID:279243096}
}

@article{Liu2024APIGenAP,
  title={APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets},
  author={Zuxin Liu and Thai Hoang and Jianguo Zhang and Ming Zhu and Tian Lan and Shirley Kokane and Juntao Tan and Weiran Yao and Zhiwei Liu and Yihao Feng and Rithesh Murthy and Liangwei Yang and Silvio Savarese and Juan Carlos Niebles and Huan Wang and Shelby Heinecke and Caiming Xiong},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.18518},
  url={https://api.semanticscholar.org/CorpusID:270738094}
}

@article{Liu2024ToolACEWT,
  title={ToolACE: Winning the Points of LLM Function Calling},
  author={Weiwen Liu and Xu Huang and Xingshan Zeng and Xinlong Hao and Shuai Yu and Dexun Li and Shuai Wang and Weinan Gan and Zhengying Liu and Yuanqing Yu and Zezhong Wang and Yuxian Wang and Wu Ning and Yutai Hou and Bin Wang and Chuhan Wu and Xinzhi Wang and Yong Liu and Yasheng Wang and Duyu Tang and Dandan Tu and Lifeng Shang and Xin Jiang and Ruiming Tang and Defu Lian and Qun Liu and Enhong Chen},
  journal={ArXiv},
  year={2024},
  volume={abs/2409.00920},
  url={https://api.semanticscholar.org/CorpusID:272368347}
}


@misc{olmix,
      title={Olmix: Efficient Mixture Recomputation for Evolving LM Datasets},
      author={Mayee F. Chen and Tyler Murray and  David Heineman and Matt Jordan and Hannaneh Hajishirzi and Christopher R\'e and Luca Soldaini and Kyle Lo},
      year={2026},
}



@misc{bertsch2026cracks, 
    title={Cracks in the Foundation: Architectural Choices Impact Long Context Extension}, 
    author={Amanda Bertsch and Luca Soldaini and Matt Gormley and Graham Neubig and Hanna Hajishirzi and Kyle Lo and Dirk Groeneveld}, 
    year={2026}, 
}


@inproceedings{goddard-etal-2024-arcees,
    title = "Arcee{'}s {M}erge{K}it: A Toolkit for Merging Large Language Models",
    author = "Goddard, Charles  and
      Siriwardhana, Shamane  and
      Ehghaghi, Malikeh  and
      Meyers, Luke  and
      Karpukhin, Vladimir  and
      Benedict, Brian  and
      McQuade, Mark  and
      Solawetz, Jacob",
    editor = "Dernoncourt, Franck  and
      Preo{\c{t}}iuc-Pietro, Daniel  and
      Shimorina, Anastasia",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-industry.36",
    doi = "10.18653/v1/2024.emnlp-industry.36",
    pages = "477--485",
    abstract = "The rapid growth of open-source language models provides the opportunity to merge model checkpoints, combining their parameters to improve performance and versatility. Advances in transfer learning have led to numerous task-specific models, which model merging can integrate into powerful multitask models without additional training. MergeKit is an open-source library designed to support this process with an efficient and extensible framework suitable for any hardware. It has facilitated the merging of thousands of models, contributing to some of the world{'}s most powerful open-source model checkpoints. The library is accessible at: https://github.com/arcee-ai/mergekit.",
}

@misc{morrison2024mergelearnefficientlyadding,
      title={Merge to Learn: Efficiently Adding Skills to Language Models with Model Merging},
      author={Jacob Morrison and Noah A. Smith and Hannaneh Hajishirzi and Pang Wei Koh and Jesse Dodge and Pradeep Dasigi},
      year={2024},
      eprint={2410.12937},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.12937},
}

@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International conference on machine learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR}
}

@article{kopf2024openassistant,
  title={Openassistant conversations-democratizing large language model alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi Rui and Stevens, Keith and Barhoum, Abdullah and Nguyen, Duc and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{marjanović2025deepseekr1thoughtologyletsthink,
      title={DeepSeek-R1 Thoughtology: Let's think about LLM Reasoning},
      author={Sara Vera Marjanović and Arkil Patel and Vaibhav Adlakha and Milad Aghajohari and Parishad BehnamGhader and Mehar Bhatia and Aditi Khandelwal and Austin Kraft and Benno Krojer and Xing Han Lù and Nicholas Meade and Dongchan Shin and Amirhossein Kazemnejad and Gaurav Kamath and Marius Mosbach and Karolina Stańczak and Siva Reddy},
      year={2025},
      eprint={2504.07128},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.07128},
}

@article{modelmerginginpretraining,
  title={Model Merging in Pre-training of Large Language Models},
  author={Yunshui Li and Yiyuan Ma and Shen Yan and Chaoyi Zhang and Jing Liu and Jianqiao Lu and Ziwen Xu and Mengzhao Chen and Minrui Wang and Shiyi Zhan and Jin Ma and Xunhao Lai and Deyi Liu and Yao Luo and Xingyan Bin and Hongbin Ren and Mingji Han and Wenhao Hao and Bairen Yi and Lingjun Liu and Bole Ma and Xiaoying Jia and Xun Zhou and Siyuan Qiao and Liang Xiang and Yonghui Wu},
  journal={ArXiv},
  year={2025},
  volume={abs/2505.12082},
  url={https://api.semanticscholar.org/CorpusID:278739754}
}
@inproceedings{mindermann2022prioritized,
  title={Prioritized training on points that are learnable, worth learning, and not yet learnt},
  author={Mindermann, S{\"o}ren and Brauner, Jan M and Razzak, Muhammed T and Sharma, Mrinank and Kirsch, Andreas and Xu, Winnie and H{\"o}ltgen, Benedikt and Gomez, Aidan N and Morisot, Adrien and Farquhar, Sebastian and others},
  booktitle={International Conference on Machine Learning},
  pages={15630--15649},
  year={2022},
  organization={PMLR}
}
@inproceedings{ding2023enhancing,
  title={Enhancing chat language models by scaling high-quality instructional conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={3029--3051},
  year={2023}
}

@misc{Polaris2025,
    title = {POLARIS: A Post-Training Recipe for Scaling Reinforcement Learning on Advanced Reasoning Models},
    url = {https://hkunlp.github.io/blog/2025/Polaris},
    author = {An, Chenxin and Xie, Zhihui and Li, Xiaonan and Li, Lei and Zhang, Jun and Gong, Shansan and Zhong, Ming and Xu, Jingjing and Qiu, Xipeng and Wang, Mingxuan and Kong, Lingpeng},
    year = {2025}
}

@misc{penedo2023refinedwebdatasetfalconllm,
      title={The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only},
      author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
      year={2023},
      eprint={2306.01116},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.01116},
}

@misc{kudugunta2023madlad400multilingualdocumentlevellarge,
      title={MADLAD-400: A Multilingual And Document-Level Large Audited Dataset},
      author={Sneha Kudugunta and Isaac Caswell and Biao Zhang and Xavier Garcia and Christopher A. Choquette-Choo and Katherine Lee and Derrick Xin and Aditya Kusupati and Romi Stella and Ankur Bapna and Orhan Firat},
      year={2023},
      eprint={2309.04662},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.04662},
}

@misc{lee2022deduplicatingtrainingdatamakes,
      title={Deduplicating Training Data Makes Language Models Better},
      author={Katherine Lee and Daphne Ippolito and Andrew Nystrom and Chiyuan Zhang and Douglas Eck and Chris Callison-Burch and Nicholas Carlini},
      year={2022},
      eprint={2107.06499},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2107.06499},
}


@misc{azar2023generaltheoreticalparadigmunderstand,
      title={A General Theoretical Paradigm to Understand Learning from Human Preferences},
      author={Mohammad Gheshlaghi Azar and Mark Rowland and Bilal Piot and Daniel Guo and Daniele Calandriello and Michal Valko and Rémi Munos},
      year={2023},
      eprint={2310.12036},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.12036},
}

@misc{fang2025datasetsdocumentsrepetitionspracticalities,
      title={Datasets, Documents, and Repetitions: The Practicalities of Unequal Data Quality},
      author={Alex Fang and Hadi Pouransari and Matt Jordan and Alexander Toshev and Vaishaal Shankar and Ludwig Schmidt and Tom Gunter},
      year={2025},
      eprint={2503.07879},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.07879},
}

@misc{muennighoff2025scalingdataconstrainedlanguagemodels,
      title={Scaling Data-Constrained Language Models},
      author={Niklas Muennighoff and Alexander M. Rush and Boaz Barak and Teven Le Scao and Aleksandra Piktus and Nouamane Tazi and Sampo Pyysalo and Thomas Wolf and Colin Raffel},
      year={2025},
      eprint={2305.16264},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.16264},
}

@misc{OpenHermes,
  title={OpenHermes 2.5: An Open Dataset of Synthetic Data for Generalist LLM Assistants},
  author={Teknium},
  year={2023},
  publisher={HuggingFace},
  url={https://huggingface.co/datasets/teknium/OpenHermes-2.5}
}

@misc{magnusson2024palomabenchmarkevaluatinglanguage,
      title={Paloma: A Benchmark for Evaluating Language Model Fit},
      author={Ian Magnusson and Akshita Bhagia and Valentin Hofmann and Luca Soldaini and Ananya Harsh Jha and Oyvind Tafjord and Dustin Schwenk and Evan Pete Walsh and Yanai Elazar and Kyle Lo and Dirk Groeneveld and Iz Beltagy and Hannaneh Hajishirzi and Noah A. Smith and Kyle Richardson and Jesse Dodge},
      year={2024},
      eprint={2312.10523},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10523},
}

@InProceedings{bevendorff2018,
  address =             {Berlin Heidelberg New York},
  author =              {Janek Bevendorff and Benno Stein and Matthias Hagen and Martin Potthast},
  booktitle =           {Advances in Information Retrieval. 40th European Conference on IR Research (ECIR 2018)},
  editor =              {Leif Azzopardi and Allan Hanbury and Gabriella Pasi and Benjamin Piwowarski},
  ids =                 {potthast:2018c,stein:2018c},
  month =               mar,
  publisher =           {Springer},
  series =              {Lecture Notes in Computer Science},
  site =                {Grenoble, France},
  title =               {{Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl}},
  year =                2018
}

@misc{Wikiextractor2015,
  author = {Giusepppe Attardi},
  title = {WikiExtractor},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/attardi/wikiextractor}}
}

@article{bragg2025astabench,
  title={AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite},
  author={Bragg, Jonathan and D'Arcy, Mike and Balepur, Nishant and Bareket, Dan and Dalvi, Bhavana and Feldman, Sergey and Haddad, Dany and Hwang, Jena D and Jansen, Peter and Kishore, Varsha and others},
  journal={arXiv preprint arXiv:2510.21652},
  year={2025}
}


%%%% fully-open models %%%%

@misc{martins2024eurollmmultilinguallanguagemodels,
      title={EuroLLM: Multilingual Language Models for Europe},
      author={Pedro Henrique Martins and Patrick Fernandes and João Alves and Nuno M. Guerreiro and Ricardo Rei and Duarte M. Alves and José Pombal and Amin Farajian and Manuel Faysse and Mateusz Klimaszewski and Pierre Colombo and Barry Haddow and José G. C. de Souza and Alexandra Birch and André F. T. Martins},
      year={2024},
      eprint={2409.16235},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.16235},
}

@misc{ng2025sealionsoutheastasianlanguages,
      title={SEA-LION: Southeast Asian Languages in One Network},
      author={Raymond Ng and Thanh Ngan Nguyen and Yuli Huang and Ngee Chia Tai and Wai Yi Leong and Wei Qi Leong and Xianbin Yong and Jian Gang Ngui and Yosephine Susanto and Nicholas Cheng and Hamsawardhini Rengarajan and Peerat Limkonchotiwat and Adithya Venkatadri Hulagadri and Kok Wai Teng and Yeo Yeow Tong and Bryan Siow and Wei Yi Teo and Wayne Lau and Choon Meng Tan and Brandon Ong and Zhi Hao Ong and Jann Railey Montalan and Adwin Chan and Sajeban Antonyrex and Ren Lee and Esther Choa and David Ong Tat-Wee and Bing Jie Darius Liu and William Chandra Tjhi and Erik Cambria and Leslie Teo},
      year={2025},
      eprint={2504.05747},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.05747},
}
@misc{black2022gptneox20bopensourceautoregressivelanguage,
      title={GPT-NeoX-20B: An Open-Source Autoregressive Language Model},
      author={Sid Black and Stella Biderman and Eric Hallahan and Quentin Anthony and Leo Gao and Laurence Golding and Horace He and Connor Leahy and Kyle McDonell and Jason Phang and Michael Pieler and USVSN Sai Prashanth and Shivanshu Purohit and Laria Reynolds and Jonathan Tow and Ben Wang and Samuel Weinbach},
      year={2022},
      eprint={2204.06745},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.06745},
}

@misc{k2team2025k2v2360openreasoningenhancedllm,
      title={K2-V2: A 360-Open, Reasoning-Enhanced LLM}, 
      author={K2 Team and Zhengzhong Liu and Liping Tang and Linghao Jin and Haonan Li and Nikhil Ranjan and Desai Fan and Shaurya Rohatgi and Richard Fan and Omkar Pangarkar and Huijuan Wang and Zhoujun Cheng and Suqi Sun and Seungwook Han and Bowen Tan and Gurpreet Gosal and Xudong Han and Varad Pimpalkhute and Shibo Hao and Ming Shan Hee and Joel Hestness and Haolong Jia and Liqun Ma and Aaryamonvikram Singh and Daria Soboleva and Natalia Vassilieva and Renxi Wang and Yingquan Wu and Yuekai Sun and Taylor Killian and Alexander Moreno and John Maggs and Hector Ren and Guowei He and Hongyi Wang and Xuezhe Ma and Yuqi Wang and Mikhail Yurochkin and Eric P. Xing},
      year={2025},
      eprint={2512.06201},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2512.06201}, 
}