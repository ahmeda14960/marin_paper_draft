\begin{table}[tbp]
\centering
\small
\begin{tabular}{@{}l >{\columncolor{ai2pink!18}}c cccccccccc@{}}
\toprule
& \multicolumn{11}{c}{\textbf{Subset of \olmothreethinking Benchmarks}} \\
\cmidrule(lr){2-12}
\textbf{Name} & \cellcolor{ai2pink!18}\textbf{Avg.} & \textbf{MMLU} & \textbf{BBH} & \textbf{GPQA} & \textbf{Zebra} & \textbf{AGI} & \textbf{AIME25} & \textbf{AIME24} & \textbf{CHE} & \textbf{LCB} & \textbf{IFEval} \\
\midrule
SFT & 70.1 & 74.9 & 84.1 & 45.8 & 57.9 & 77.2 & 57.6 & 69.6 & 88.2 & 67.8 & 77.9 \\
SFT + DPO & 72.7 & 74.8 & 83.7 & 48.6 & 60.6 & 79.1  & 62.7 & {\bf 74.6} & {\bf 91.4} & {\bf 75.1} & 75.9 \\
SFT + RLVR & 71.9 & 77.4 & 83.2 & 42.7 & 63.1 & 78.5 & 62.4 & 70.0 & 87.9 & 70.7 & {\bf 82.8} \\
SFT + DPO + RLVR & {\bf 74.1} & {\bf 77.9} & {\bf 86.8} & {\bf 50.2} & {\bf 62.9} & {\bf 80.1} & {\bf 64.2} & 73.2 & 89.9 & 73.4 & 82.3 \\
\bottomrule
\end{tabular}
\caption{\textbf{Delta learning provides a stronger initialization for subsequent RLVR than SFT alone}. We show the effect of conducting RLVR for 1000 steps after DPO and SFT on our 7B model on a subset of our evaluation suite. Note that here evaluations are from one run only. Preference tuning with delta learning first followed by RLVR, yields the best overall performance. For RLVR, we use data offline-filtered by the corresponding starting point (SFT only or SFT + DPO).}
\label{tab:dpo_rl_sequence}
\end{table}

