\begin{table}[t]
\centering
\small
\begin{tabular}{@{}l >{\columncolor{ai2pink!18}}c ccccccccc@{}}
\toprule
& \multicolumn{10}{c}{\textbf{Subset of \olmothreeinstruct Benchmarks}} \\
\cmidrule(lr){2-11}
\textbf{Name} & \cellcolor{ai2pink!18}\textbf{Avg.} & \textbf{MMLU} & \textbf{BBH} & \textbf{GPQA} & \textbf{AGI} & \textbf{MATH} & \textbf{CHE} & \textbf{LCB} & \textbf{IFEval} & \textbf{AE2} \\
\midrule
Dev. 7B SFT ckpt & 51.9 & 67.6 & 47.7 & 30.2 & 62.0 & 65.5 & 69.3 & 17.9 & 83.2 & 23.8 \\
\olmotoo preference data & 55.5 & 69.4 & 55.6 & 33.7 & 63.6 & 71.3 & 73.7 & 12.7 & {\bf 84.5} & 35.2 \\
\midrule
Updated GPT UltraF pipeline & 55.4 & 67.6 & 51.2 & 31.5 & 61.8 & 72.2 & 71.5 & 14.7 & 80.8 & 47.5 \\
+ Sample weak models & 56.3 & 68.4 & 50.4 & 33.9 & 63.8 & 71.6 & 74.3 & 18.2 & 81.9 & 44.4 \\
+ Min score rejected & 57.4 & 68.5 & 53.6 & 34.4 & 64.2 & 72.6 & {\bf 75.2} & 19.1 & 82.3 & 47.0 \\
Delta learning only & 57.6 & 68.7 & 49.5 & {\bf 35.5} & {\bf 64.6} & 79.1 & 73.9 & {\bf 22.0} & 78.6 & 46.1 \\
\midrule
Delta learning + GPT & {\bf 60.4} & {\bf 69.4} & {\bf 66.9} & 34.6 & 64.3 & {\bf 80.0} & 74.1 & 21.1 & 83.0 & {\bf 49.8} \\
\bottomrule
\end{tabular}
\caption{\textbf{Comparing sources of preference signals}. Preference pairs created with the delta-learning heuristic (chosen = large model response, rejected = smaller model response) and pairs created with our delta-aware LLM-judge pipeline yield a different spread of gains, suggesting that they provide different preference signals. These signals are complementary; combining them both yields the largest average gain. Our final \olmothreeinstruct preference data greatly outperforms our previous \olmotoo preference data.}
\label{tab:dpo_different_signals}
\end{table}




