


\begin{table}[h]
\centering
\begin{minipage}{0.48\textwidth}
    \footnotesize
    \centering
    \setlength\tabcolsep{3pt}
    \begin{tabular}{>{\raggedright}p{0.35\linewidth}p{0.55\linewidth}}
        \toprule
        \textbf{Hyperparameter} & \textbf{RLVR value}  \\
        \midrule
        \rowcolor{ai2offwhite} Learning rate & $3 \cdot 10^{-7}$ for 13B; $4 \cdot 10^{-7}$ for 7B \\
        \rowcolor{ai2offwhite} Effective batch size & 248 for 13B; 224 for 7B \\
        \rowcolor{ai2offwhite} 
        KL penalty coef.  ($\beta$) &
        {0.1 for first and final 13B; 0.03 for second 13B; 0.05 for 7B} \\
        \rowcolor{ai2offwhite} Max total episodes & 200,000 for 13B; 100,000 for 7B \\
        Discount factor $\gamma$ & 1.0 \\
        General advantage estimation $\lambda$ & 0.95 \\
        Mini-batches $N_\text{mb}$ & 1 \\
        PPO update iterations $K$ & 4 \\
        \bottomrule
    \end{tabular}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \footnotesize
    \centering
    \setlength\tabcolsep{3pt}
    \begin{tabular}{>{\raggedright}p{0.60\linewidth}p{0.3\linewidth}}
        \toprule
        \textbf{Hyperparameter} & \textbf{RLVR value}  \\
        \midrule
        PPO's clipping coefficient $\varepsilon$  & 0.2 \\
        Value function coefficient $c_1$ & 0.1 \\
        Gradient norm threshold & 1.0 \\
        Learning rate schedule & \textit{linear} \\
        Generation temperature & 1.0 \\
        Max token length & 2,048 \\
        Max prompt token length & 2,048 \\
        {Penalty reward for no EOS token} & $-10.0$ \\
        Response length & 2,048 \\
        Warm up ratio ($\omega$) & 0.0 \\
        \bottomrule
    \end{tabular}
\end{minipage}
\sethlcolor{ai2offwhite} %
\caption{The hyperparameters of PPO used for optimizing against the verifiable reward function with RLVR.
Hyparameters with different settings for the 7B and 13B parameter models are \hl{highlighted}.}
\label{tab:hypers_rlvr}
\end{table}


