\begin{table}[h]
\centering
\begin{small}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lcccc}
\toprule
\textbf{Mid-training mix} & \textbf{OLMES} \sans{(MCF)} & \textbf{OLMES-Gen} & \textbf{MMLU} \sans{(MCF)} & \textbf{GSM*} \\
\midrule
\rowcolor{ai2offwhite}\textit{n/a (pretrain checkpoint)} & 69.6 & 63.2 & 59.8 & 28.5 \\
\MeBaseline & 74.0 & 64.5 & 61.8 & 27.0 \\
\rowcolor{ai2offwhite}\MeDclmSeven & 73.5 & 64.1 & 61.9 & 24.5 \\
\MeFinewebThree & 73.5 & 63.0 & 62.4 & 30.5 \\
\rowcolor{ai2offwhite}\MeFinewebTwo & 75.2 & 63.8 & 63.1 & 28.5 \\
\MeFinewebTwoIns & 74.2 & 64.1 & 63.0 & 46.0 \\
\rowcolor{ai2offwhite}\MeFinewebTwoMath & {\bf75.7} & 69.7 & 62.3 & {\bf52.0} \\
\MeFinewebTwoMathIns & {\bf75.7} & {\bf70.2} & {\bf63.1} & 46.5 \\[.2em]
\bottomrule
\end{tabular}
\end{small}
\caption{
Comparison of mid-training mixes introduced in Table~\ref{table:hq_mixes}. 
Each row corresponds to a 50 billion token training run following learning rate schedule described in Section~\S\ref{sec:learning_rate_deep_dive} (except first row).
Weights are initialized from a \olmotoo checkpoint pretrained for 4T tokens. 
We compare each run on a mix of OLMES core tasks (multiple choice format; see Table~\ref{tab:evals_overview}), OLMES generative tasks (Table~\ref{tab:evals_overview}), MMLU (multiple choice format; \citealp{hendryckstest2021}), and a random sample of 200 GSM8K~\citep{cobbe2021trainingverifierssolvemath} questions we use as development set (GSM*; Section~\S\ref{app:eval-base}).
Results on the {\bf{final mid-training mix}}  are in Table~\ref{tab:evals-pre-post-anneal}.
}
\label{table:hq_mixes_results}
\end{table}
