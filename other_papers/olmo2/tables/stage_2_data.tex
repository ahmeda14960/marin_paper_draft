
\begin{table}[h]
\centering
\small
\renewcommand{\arraystretch}{1}
\begin{tabular}{l l r r r r}
\toprule
\textbf{Source} & 
\textbf{Type} & 
\textbf{Tokens} & 
\textbf{Words} & 
\textbf{Bytes} & 
\textbf{Docs} \\
\midrule
    \rowcolor{ai2midwhite}
    \multicolumn{6}{c}{\textbf{\textit{Mid-Training \ding{70} Dolmino High Quality Subset}}} 
\\
    \rowcolor{ai2offwhite}
    \begin{tabular}{@{}l@{}}
        \rowcolor{ai2offwhite}
        {DCLM-Baseline} 
        \\[-.6em]
        {
            \hspace{0.8em}
            \scriptsize
            \color{neutralFive}
            FastText top 7\%
        }
        \\[-.6em]
        {
            \hspace{0.8em}
            \scriptsize
            \color{neutralFive}
            $\text{FineWeb}\geqslant2$
        }
    \end{tabular} & 
    High quality web & 
    752B &  
    670B & 
    4.56T & 
    606M 
\\
    \rowcolor{ai2offwhite}
    \begin{tabular}{@{}l@{}}
        \rowcolor{ai2offwhite}
        {FLAN} 
        \\[-.6em]
        {
            \hspace{0.8em}
            \scriptsize
            \color{neutralFive}
            from Dolma 1.7
        }
        \\[-.6em]
        {
            \hspace{0.8em}
            \scriptsize
            \color{neutralFive}
            decontaminated
        }
    \end{tabular} & 
    Instruction data & 
    17.0B & 
    14.4B & 
    98.2B & 
    57.3M 
\\
    \rowcolor{ai2offwhite}
    \begin{tabular}{@{}l@{}}
        \rowcolor{ai2offwhite}
        {peS2o} 
        \\[-.6em]
        {
            \hspace{0.8em}
            \scriptsize
            \color{neutralFive}
            from Dolma 1.7
        }
    \end{tabular} & 
    Academic papers & 
    58.6B & 
    51.1B & 
    413B & 
    38.8M 
\\
    \rowcolor{ai2offwhite}
    \begin{tabular}{@{}l@{}}
        \rowcolor{ai2offwhite}
        {Wikipedia \& Wikibooks} 
        \\[-.6em]
        {
            \hspace{0.8em}
            \scriptsize
            \color{neutralFive}
            from Dolma 1.7
        }
    \end{tabular} &
    Encyclopedic & 
    3.7B & 
    3.16B & 
    16.2B & 
    6.17M 
\\
    \rowcolor{ai2offwhite}
    \begin{tabular}{@{}l@{}}
        \rowcolor{ai2offwhite}
        {Stack Exchange} 
        \\[-.6em]
        {
            \hspace{0.8em}
            \scriptsize
            \color{neutralFive}
            09/30/2024 dump
        }
        \\[-.6em]
        {
            \hspace{0.8em}
            \scriptsize
            \color{neutralFive}
            curated Q\&A data
        }
    \end{tabular} &
    Q\&A & 
    1.26B & 
    1.14B & 
    7.72B & 
    2.48M 
\\
    \rowcolor{ai2offwhite}
    \textbf{High quality total} & 
    & 
    \textbf{832.6B} & 
    \textbf{739.8B} & 
    \textbf{5.09T} & 
    \textbf{710.8M} 
\\
    \rowcolor{ai2midpink}
    \multicolumn{6}{c}{
        \textbf{
            \textit{
                Mid-training \ding{70} Dolmino Math Mix
            }
        }
    } 
\\
    \rowcolor{ai2lightpink}
    TuluMath & 
    Synthetic math & 
    230M & 
    222M & 
    1.03B & 
    220K 
\\
    \rowcolor{ai2lightpink}
    Dolmino SynthMath & 
    Synthetic math & 
    28.7M & 
    35.1M & 
    163M & 
    725K 
\\
    \rowcolor{ai2lightpink}
    TinyGSM-MIND & 
    Synthetic math & 
    6.48B & 
    5.68B & 
    25.52B & 
    17M 
\\
    \rowcolor{ai2lightpink}
    \begin{tabular}{@{}l@{}}
        \rowcolor{ai2lightpink}
        {MathCoder2} 
        \\[-.4em]
        {Synth Books} 
        \\[-.6em]
        {
            \hspace{0.8em}
            \scriptsize
            \color{neutralFive}
            Ajibawa-2023
        }
        \\[-.6em]
        {
            \hspace{0.8em}
            \scriptsize
            \color{neutralFive}
            M-A-P Matrix
        }
    \end{tabular} &
    Synthetic Math & 
    3.87B & 
    3.71B & 
    18.4B & 
    2.83M 
\\
    \rowcolor{ai2lightpink}
    \begin{tabular}{@{}l@{}}
        \rowcolor{ai2lightpink}
        {Metamath} 
        \\[-.6em]
        {
            \hspace{0.8em}
            \scriptsize
            \color{neutralFive}
            OWM-filtered
        }
    \end{tabular} &
    Math & 
    84.2M & 
    76.6M & 
    741M & 
    383K 
\\
    \rowcolor{ai2lightpink}
    \begin{tabular}{@{}l@{}}
        \rowcolor{ai2lightpink}
        {CodeSearchNet} 
        \\[-.6em]
        {
            \hspace{0.8em}
            \scriptsize
            \color{neutralFive}
            OWM-filtered
        }
    \end{tabular} &
    Code & 
    1.78M & 
    1.41M & 
    29.8M & 
    7.27K 
\\
    \rowcolor{ai2lightpink}
    \begin{tabular}{@{}l@{}}
        \rowcolor{ai2lightpink}
        {GSM8K} 
        \\[-.6em]
        {
            \hspace{0.8em}
            \scriptsize
            \color{neutralFive}
            Train split
        }
    \end{tabular} &
    Math & 
    2.74M & 
    3.00M & 
    25.3M & 
    17.6K 
\\
    \rowcolor{ai2lightpink}
    \textbf{Math total} & 
    & 
    \textbf{10.7B} & 
    \textbf{9.73B} & 
    \textbf{45.9B} & 
    \textbf{21.37M} 
\\
\bottomrule
\end{tabular}
\caption{
\textbf{Composition of the mid-training data (Dolmino)}.  
From this set, we create samples of 50B, 100B and 300B tokens to mid-train \olmotoo on. 
See Section~\S\ref{sec:diveAnnealing} for details regarding individual source details, and Table \ref{tab:dolmino-composition-pretty} for the specific composition of each annealing mixture.
}
\label{table:stage-2-data}
\end{table}
