


\begin{figure}[h]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\footnotesize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.08}
\begin{tabular}{cccc}
\toprule
Epochs & L.R. & Loss & Avg. Perf. \\
\midrule
2 & 1e-5 & sum & \bf{49.97} \\
3 & 4e-6 & sum & 49.76 \\
2 & 1e-5 & sum & 49.74 \\
2 & 1e-5 & sum & 49.59 \\
3 & 4e-6 & mean & 48.25 \\
2 & 2e-6 & mean & 48.18 \\
\bottomrule
\end{tabular}
\vspace{3pt}
\captionof{table}{Hyperparameter configurations tried for the 7B SFT checkpoint, all on the same dataset used in the final model. SFT models are trained with an effective batch size of 128, a linear learning rate schedule and a warmup up ratio of 0.3.}
\label{tab:sft_hypers}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{figures/dpo_hypers.png}
\captionof{figure}{The average score for DPO checkpoints trained on a development SFT checkpoint on different learning rates. Avg does not include Safety.}
\label{fig:dpo_lrs_plot}
\end{minipage}
\end{figure}
