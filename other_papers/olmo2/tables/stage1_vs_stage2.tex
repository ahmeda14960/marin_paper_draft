\begin{table}[ht]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{small}
\begin{tabular}{lc|cccccc|cccc}

\toprule
    \multicolumn{2}{l}{} &
    \multicolumn{6}{c}{\textbf{\texttt{Dev Benchmarks}}} & 
    \multicolumn{4}{c}{\textbf{\texttt{Held-out Evals}}} 
\\
    {\textbf{\fontsize{8}{8}\selectfont{Checkpoint}}} &
    {\textbf{\fontsize{8}{8}\selectfont~Avg}} &
    {\textbf{\fontsize{8}{8}\selectfont~MMLU}} & 
    {$\textbf{\fontsize{8}{8}\selectfont~ARC}_\textbf{\fontsize{6}{6}\selectfont~C}$} & 
    {\textbf{\fontsize{8}{8}\selectfont~HSwag}} & 
    {\textbf{\fontsize{8}{8}\selectfont~WinoG}} & 
    {\textbf{\fontsize{8}{8}\selectfont~NQ}} & 
    {\textbf{\fontsize{8}{8}\selectfont~DROP}} & 
    {\textbf{\fontsize{8}{8}\selectfont~AGIEval}} & 
    {\textbf{\fontsize{8}{8}\selectfont~GSM8K}} & 
    {$\textbf{\fontsize{8}{8}\selectfont~MMLU}_\textbf{\fontsize{6}{6}\selectfont~PRO}$} &
    {\textbf{\fontsize{8}{8}\selectfont~TQA}}
\\
\midrule
\rowcolor{midgrey}\multicolumn{12}{c}{\textbf{\olmotoo 1B}} \\
\rowcolor{lightgrey}{{Pretraining}} & 31.9 & 26.9 & 26.1 & 67.5 & 67.8 & 16.1 & 25.1 & 24.5 & 3.3 & 11.1 & 50.1 \\
\rowcolor{lightgrey}{{Pretraining \& mid-training}\hspace{.5em}} & 43.7 & 44.3 & 51.3 & 69.5 & 66.5 & 20.8 & 34.0 & 36.3 & 43.8 & 16.1 & 54.7 \\%[.8em]
\rowcolor{midgrey}\multicolumn{12}{c}{\textbf{\olmotoo 7B}} \\
\rowcolor{lightgrey}{{Pretraining}} & 53.0 & 59.8 & 72.6 & 81.3 & 75.8 & 29.0 & 40.7 & 44.6 & 24.1 & 27.4 & 74.6 \\
\rowcolor{lightgrey}{{Pretraining \& mid-training}\hspace{.5em}} & 62.9 & 63.7 & 79.8 & 83.8 & 77.2 & 36.9 & 60.8 & 50.4 & 67.5 & 31.0 & 78.0 \\%[.8em]
\rowcolor{ai2midwhite}\multicolumn{12}{c}{\textbf{\olmotoo 13B}} \\
\rowcolor{ai2offwhite}{{Pretraining}} & 58.9 & 63.4 & 80.2 & 84.8 & 79.4 & 34.6 & 49.6 & 48.2 & 37.3 & 31.2 & 80.3 \\
\rowcolor{ai2offwhite}{{Pretraining \& mid-training}\hspace{.5em}} & 68.3 & 67.5 & 83.5 & 86.4 & 81.5 & 46.7 & 70.7 & 54.2 & 75.1 & 35.1 & 81.9 \\
\rowcolor{ai2midpink}\multicolumn{12}{c}{\textbf{\olmotoo 32B}} \\
\rowcolor{ai2lightpink}{{Pretraining}} & 66.3 & 72.9 & 88.7 & 84.2 & 82.4 & 40.6 & 57.2 & 56.8 & 56.2 & 38.5 & 85.4 \\
\rowcolor{ai2lightpink}{{Pretraining \& mid-training}\hspace{.5em}} & 73.3 & 74.9 & 90.4 & 89.7 & 83.0 & 50.2 & 74.3 & 61.0 & 78.8 & 43.3 & 88.0 \\
\bottomrule
\end{tabular}
\end{small}
\vspace{2mm}
\caption{
Evaluations comparing \olmotoo 1B, 7B, 13B and 32B at the end of pretraining and mid-training stages (setup mirrors Table~\ref{tab:evals_overview}). 
Pretrain checkpoints have been trained on 4 trillion (1B, 7B), 5 trillion (13B) and 7 trillion (32B) tokens respectively. 
For 7B, we obtain the final mid-train checkpoints by averaging three training runs on 50B \textsc{Dolmino} tokens;
for 13B and 32B, we use three runs on 100B tokens and one run on 300B tokens.
For 1B, the final checkpoint is the result of training on 50B \textsc{Dolmino} tokens \textit{without} averaging.}
\label{tab:evals-pre-post-anneal}
\end{center}
\end{table}

